{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations with IBM\n",
    "\n",
    "In this notebook, you will be putting your recommendation skills to use on real data from the IBM Watson Studio platform. \n",
    "\n",
    "\n",
    "You may either submit your notebook through the workspace here, or you may work from your local machine and submit through the next page.  Either way assure that your code passes the project [RUBRIC](Need to update this).  **Please save regularly.**\n",
    "\n",
    "By following the table of contents, you will build out a number of different methods for making recommendations that can be used for different situations. \n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "I. [Exploratory Data Analysis](#Exploratory-Data-Analysis)<br>\n",
    "II. [Rank Based Recommendations](#Rank)<br>\n",
    "III. [User-User Based Collaborative Filtering](#User-User)<br>\n",
    "IV. [Content Based Recommendations (EXTRA - NOT REQUIRED)](#Content-Recs)<br>\n",
    "V. [Matrix Factorization](#Matrix-Fact)<br>\n",
    "VI. [Extras & Concluding](#conclusions)\n",
    "\n",
    "At the end of the notebook, you will find directions for how to submit your work.  Let's get started by importing the necessary libraries and reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>using pixiedust for fast, flexible, and easier data analysis and experimentation</td>\n",
       "      <td>ef5f11f77ba020cd36e1105a00ab868bbdbf7fe7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1314.0</td>\n",
       "      <td>healthcare python streaming application demo</td>\n",
       "      <td>083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1429.0</td>\n",
       "      <td>use deep learning for image classification</td>\n",
       "      <td>b96a4f2e92d8572034b1e9b28f9ac673765cd074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338.0</td>\n",
       "      <td>ml optimization using cognitive assistant</td>\n",
       "      <td>06485706b34a5c9bf2a0ecdac41daf7e7654ceb7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1276.0</td>\n",
       "      <td>deploy your python model as a restful api</td>\n",
       "      <td>f01220c46fc92c6e6b161b1849de11faacd7ccb2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  \\\n",
       "0      1430.0   \n",
       "1      1314.0   \n",
       "2      1429.0   \n",
       "3      1338.0   \n",
       "4      1276.0   \n",
       "\n",
       "                                                                              title  \\\n",
       "0  using pixiedust for fast, flexible, and easier data analysis and experimentation   \n",
       "1                                      healthcare python streaming application demo   \n",
       "2                                        use deep learning for image classification   \n",
       "3                                         ml optimization using cognitive assistant   \n",
       "4                                         deploy your python model as a restful api   \n",
       "\n",
       "                                      email  \n",
       "0  ef5f11f77ba020cd36e1105a00ab868bbdbf7fe7  \n",
       "1  083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b  \n",
       "2  b96a4f2e92d8572034b1e9b28f9ac673765cd074  \n",
       "3  06485706b34a5c9bf2a0ecdac41daf7e7654ceb7  \n",
       "4  f01220c46fc92c6e6b161b1849de11faacd7ccb2  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import project_tests as t\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('data/user-item-interactions.csv')\n",
    "df_content = pd.read_csv('data/articles_community.csv')\n",
    "del df['Unnamed: 0']\n",
    "del df_content['Unnamed: 0']\n",
    "\n",
    "# Show df to get an idea of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>PouchDB-find is a new API and syntax that allows for a simpler way to query PouchDB. It is much more suited to ad-hoc querying and a fair amount easier to learn than PouchDB’s current way of querying documents via Map/Reduce. It is a MongoDB-inspired query language to query a PouchDB database. It works with PouchDB, Cloudant Query and CouchDB Mango (CouchDB 2.0 Release).</td>\n",
       "      <td>PouchDB uses MapReduce as its default search mechanism but that's about to change. Garren Smith gives an insight into how PouchDB's find plugin works and its relationship to the Cloudant Query search technology that uses a MongoDB-style query language.</td>\n",
       "      <td>A look under the covers of PouchDB-find</td>\n",
       "      <td>Live</td>\n",
       "      <td>1046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widely-held belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better.</td>\n",
       "      <td>We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widely-held belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better.</td>\n",
       "      <td>A comparison of logistic regression and naive Bayes</td>\n",
       "      <td>Live</td>\n",
       "      <td>1047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>Essays about data, building products and bootstrapping businesses.\\r\\n\\r\\nHome About Archives Running Talks© 2017. All rights reserved.\\r\\n\\r\\nJEAN-NICHOLAS HOULD DATA SCIENCE &amp; BUSINESS BOOTSTRAPPING\\r\\nWHAT I LEARNED IMPLEMENTING A CLASSIFIER FROM SCRATCH IN PYTHON\\r\\n04 Jan 2017This post is part of the Learning Machine Learning series. It’s based on Chapter 1 and 2 of Python Machine Learning .\\r\\n\\r\\nMachine learning can be intimidating for a newcomer. The concept of a machine\\r\\nlearning things alone is quite abstract. How does that work in practice?\\r\\n\\r\\nIn order to demystify some of the magic behind machine learning algorithms, I\\r\\ndecided to implement a simple machine learning algorithm from scratch. I will\\r\\nnot be using a library such as scikit-learn which already has many algorithms implemented. Instead, I’ll be writing all of\\r\\nthe code in order to have a working binary classifier algorithm. The goal of\\r\\nthis exercise is to understand its inner workings.\\r\\n\\r\\nSO, WHAT THE HECK IS A BINARY CLASSIFIER?\\r\\nA classifier is a machine-learning algorithm that determines the class of an\\r\\ninput element based on a set of features. For example, a classifier could be\\r\\nused to predict the category of a beer based on its characteristics, it’s\\r\\n“features”. These features could include its alcohol content, aroma, appearance,\\r\\netc. A machine learning classifier could potentially be used to predict that a\\r\\nbeer with 8% alcohol content, 100 IBU and with strong aromas of oranges is an\\r\\nIndian Pale Ale.\\r\\n\\r\\nIn machine learning, there are three main types of tasks: unsupervised learning,\\r\\nsupervised learning and reinforcement learning. The classifier algorithm falls\\r\\nunder the supervised learning category. Supervised learning means that we know\\r\\nthe right answer beforehand. The desired outputs are known. In the case of the\\r\\nbeer example, we could realistically have a dataset describing beers and their\\r\\ncategory. We could train the classifier algorithm to predict those categories\\r\\nbased on the beers features.\\r\\n\\r\\nA binary classifier classifies elements in two groups. Zero or one. True or\\r\\nfalse. IPA or not.\\r\\n\\r\\nBUILDING A MACHINE LEARNING MODEL\\r\\nThere are four steps to build and use a machine learning model.\\r\\n\\r\\n 1. Preprocessing\\r\\n 2. Learning\\r\\n 3. Evaluation\\r\\n 4. Prediction\\r\\n\\r\\nSource: Python Machine Learning by Sebastian Raschka.\\r\\n\\r\\nPREPROCESSING\\r\\nThe preprocessing is the first step in building a machine learning model. At\\r\\nthis step, you acquire and prepare the data for future usage. You clean up the\\r\\ndata, tidy it and select the features you want to use from your data.\\r\\n\\r\\nThe following tasks can be considered as part of the “preprocessing”:\\r\\n\\r\\n * Extract features from raw data\\r\\n * Clean and format the data\\r\\n * Remove superfluous features (or highly correlated features)\\r\\n * Reduce the number of features for performance\\r\\n * Standardize the range of feature data (also named Feature Scaling )\\r\\n * Split your dataset randomly: training dataset and test dataset\\r\\n\\r\\nLEARNING OR TRAINING\\r\\nOnce you have your datasets ready to be used, the second step is to select an\\r\\nalgorithm to perform your desired task. In our case, the algorithm we selected\\r\\nis a binary classifier called Perceptron. There are many algorithms designed to\\r\\ndo different tasks. They each have their strengths and weaknesses.\\r\\n\\r\\nAt this step, you can test a few algorithms, see how they perform and select the\\r\\nbest performing one. There are a wide variety of metrics that can be used to\\r\\nmeasure the performance of a machine learning model. According to Raschka, “one\\r\\ncommonly used metric is classification accuracy, which is defined as the\\r\\nproportion of correctly classified instances”. At this step, you will make\\r\\nadjustments to the parameters of your machine learning algorithm. These are\\r\\nnamed hyperparameters.\\r\\n\\r\\nIn this post, we’ll mainly focus on this part of the machine learning work flow.\\r\\nWe’ll deep dive in the algorithm inner workings. If you are interested in the\\r\\nother sections of the machine learning work flow, which you should be, I’ll be\\r\\nlinking to a great notebook at the end of this post.\\r\\n\\r\\nEVALUATION\\r\\nWhen the model has been “trained” on the dataset it can be evaluated on new\\r\\nunseen data. The goal here is to measure the generalization error . This metric measures “how accurately an algorithm is able to predict outcome\\r\\nvalues for previously unseen data”. Once you are satisfied with the results, you\\r\\ncan use your machine learning model to make predictions.\\r\\n\\r\\nINTRODUCING THE PERCEPTRON\\r\\nThe algorithm that we’ll be re-implementing is a Perceptron which is one of the very first machine learning algorithm.\\r\\n\\r\\nThe Perceptron algorithm is simple but powerful. Given a training dataset, the\\r\\nalgorithm automatically learns “the optimal weight coefficients that are then\\r\\nmultiplied with the input features in order to make the decision of whether a\\r\\nneuron fires or not”.\\r\\n\\r\\nBut, how does the algorithm do that?\\r\\n\\r\\nTHE ALGORITHM\\r\\nHere’s the sequence of the algorithm:\\r\\n\\r\\nFirst, we initialize an array with the weights equal to zero. The array length\\r\\nis equal to the number of features plus one. This additional feature is the\\r\\n“threshold”. It’s important to note that in the case of the Perceptron\\r\\nalgorithm, the features must be of numerical value.\\r\\n\\r\\nself.w_=np.zeros(1+X.shape[1])\\r\\n\\r\\nSecondly, we start a loop equal to the number of iterations n_iter . This is an hyperparameter defined by the data scientist.\\r\\n\\r\\nfor_inrange(self.n_iter):\\r\\n\\r\\nThirdly, we start a loop on each training data point and it’s target. The target\\r\\nis the desired output we want the algorithm to eventually predict. Since this is\\r\\na binary classifier, the targets are either -1 or 1 . They are of binary value.\\r\\n\\r\\nBased on the data point features, the algorithm will predict the category: 1 or -1 . The prediction calculation is a matricial multiplication of the features with\\r\\ntheir appropriate weights. To this multiplication we add the value of the\\r\\nthreshold. If the result is above 0, the predicted category is 1 . If the result is below 0, the predicted category is -1 .\\r\\n\\r\\nAt each iteration on a data point, if the prediction is not accurate, the\\r\\nalgorithm will adjust the weights. During the first few iterations, the\\r\\npredictions are not likely to be accurate because the weights haven’t been\\r\\nadjusted many times. They haven’t had a chance to start converging. The\\r\\nadjustments are made proportionally to the difference between the target and the\\r\\npredicted value. This difference is then multiplied by the learning rate eta , an hyperparameter of value between zero and one set by the data scientist.\\r\\nThe higher the eta is, the larger the correction on the weights will be. If the prediction is\\r\\naccurate, the algorithm won’t adjust the weights.\\r\\n\\r\\nself.w_=np.zeros(1+X.shape[1])for_inrange(self.n_iter):forxi,targetinzip(X,y):update=self.eta*(target-self.predict(xi))self.w_[1:]+=update*xiself.w_[0]+=updatedefnet_input(self,X):\"\"\"Calculate net input\"\"\"returnnp.dot(X,self.w_[1:])+self.w_[0]defpredict(self,X):\"\"\"Return class label after unit step\"\"\"returnnp.where(self.net_input(X)&gt;=0.0,1,-1)\\r\\n\\r\\nThe Perceptron will converge only if the two classes are linearly separable.\\r\\nSimply said, if you are able to draw a straight line to entirely separate the\\r\\ntwo classes, the algorithm will converge. Else, the algorithm will keep\\r\\niterating and will readjust weights until it reaches the maximum number of\\r\\niterations n_iter .\\r\\n\\r\\nSource: Python Machine Learning by Sebastian Raschka.\\r\\n\\r\\nCOMPLETE CODE\\r\\nimportnumpyasnpclassPerceptron(object):\"\"\"Perceptron classifier.\\r\\n\\r\\n    Parameters\\r\\n    ------------\\r\\n    eta : float\\r\\n        Learning rate (between 0.0 and 1.0)\\r\\n    n_iter : int\\r\\n        Passes over the training dataset.\\r\\n\\r\\n    Attributes\\r\\n    -----------\\r\\n    w_ : 1d-array\\r\\n        Weights after fitting.\\r\\n    errors_ : list\\r\\n        Number of misclassifications in every epoch.\\r\\n\\r\\n    \"\"\"def__init__(self,eta=0.01,n_iter=10):self.eta=etaself.n_iter=n_iterdeffit(self,X,y):\"\"\"Fit training data.\\r\\n\\r\\n        Parameters\\r\\n        ----------\\r\\n        X : {array-like}, shape = [n_samples, n_features]\\r\\n            Training vectors, where n_samples is the number of samples and\\r\\n            n_features is the number of features.\\r\\n        y : array-like, shape = [n_samples]\\r\\n            Target values.\\r\\n\\r\\n        Returns\\r\\n        -------\\r\\n        self : object\\r\\n\\r\\n        \"\"\"self.w_=np.zeros(1+X.shape[1])self.errors_=[]for_inrange(self.n_iter):errors=0forxi,targetinzip(X,y):update=self.eta*(target-self.predict(xi))self.w_[1:]+=update*xiself.w_[0]+=updateerrors+=int(update!=0.0)self.errors_.append(errors)returnselfdefnet_input(self,X):\"\"\"Calculate net input\"\"\"returnnp.dot(X,self.w_[1:])+self.w_[0]defpredict(self,X):\"\"\"Return class label after unit step\"\"\"returnnp.where(self.net_input(X)&gt;=0.0,1,-1)\\r\\n\\r\\nSource: Python Machine Learning by Sebastian Raschka.\\r\\n\\r\\nTHREE LEARNINGS\\r\\nLEARNING RATE, NUMBER OF ITERATION &amp; CONVERGENCE\\r\\nParameters such as learning rate and number of iteration can seem very abstract if you jump in straight to using an algorithm from a\\r\\nlibrary like scikit-learn . It’s hard to grasp what these really do. By implementing the algorithm, it’s\\r\\nnow clear for me what they represent in the context of the Perceptron.\\r\\n\\r\\nLearning RateThe learning rate is a ratio by which the weights are corrected when the\\r\\nprediction is not accurate. The value needs to be between zero and one. As you\\r\\ncan see in the snippet below, the fit function will iterate on each observation, call the predict function and then adjust the weights based on the difference between the target\\r\\nand the predicted value and then multiplied by the learning rate.\\r\\n\\r\\nA higher learning rate means that the algorithm will adjust the weights more\\r\\naggressively. At each iteration, the weights will be adjusted if the predicted\\r\\nvalue is inaccurate.\\r\\n\\r\\n# Partial portion of the \"fit\" functionforxi,targetinzip(X,y):update=self.eta*(target-self.predict(xi))self.w_[1:]+=update*xiself.w_[0]+=updateerrors+=int(update!=0.0)\\r\\n\\r\\nNumber of iterationsThe number of iteration is the number of times the algorithm will run through\\r\\nthe training dataset. If the number of iteration was set to one, the algorithm\\r\\nwould loop through the dataset only once and update the weights one time for\\r\\neach data point. The resulting model would be more likely to be inaccurate than\\r\\na model with a higher number of iteration. On large datasets, there is a cost to\\r\\nhaving a high number of iterations.\\r\\n\\r\\nfor_inrange(self.n_iter):errors=0forxi,targetinzip(X,y):update=self.eta*(target-self.predict(xi))self.w_[1:]+=update*xiself.w_[0]+=updateerrors+=int(update!=0.0)self.errors_.append(errors)\\r\\n\\r\\nThe learning rate and number of iteration go hands-in-hand. They need to be adjusted together. For example, if you have a\\r\\nvery low learning rate , which means that the algorithm will adjust it’s weight only marginally at\\r\\neach iteration, you will probably need a higher number of iteration.\\r\\n\\r\\nLINEAR ALGEBRA\\r\\nIt’s critical to mention that the capabilities of the Perceptron algorithm are\\r\\nattributable to linear algebra. The whole algorithm can be described through\\r\\nlinear algebra formulas. If you have never done linear algebra in college, the\\r\\nformulas will be cryptic. As usual, Khan Academy is a great place to start with if you want to get familiar with linear algebra.\\r\\nIt’s also a great place to get a refresher on the topic.\\r\\n\\r\\nFor me, the main learning here is how fundamental linear algebra is to this\\r\\nmachine learning algorithm.\\r\\n\\r\\nTYPE EVERYTHING\\r\\nThis learning is actually a concept I re-learned while going through the code\\r\\nfor this post. It’s not specific to machine learning and it has nothing to do\\r\\nwith the Perceptron.\\r\\n\\r\\nBack in 2012 when I was learning to code Ruby on Rails, a web application\\r\\ndevelopment framework, I realized that typing down all of the code examples from\\r\\ntutorials really helped me memorize and understand the concepts. I spent weeks\\r\\nwriting code while following tutorials. No copy and paste. I typed all the code.\\r\\nThis may sound stupid but it was extremely helpful to grasp the concepts. During\\r\\nthe process, I inevitably made typos and spend some time figuring out what was\\r\\nbroken. These moments where crucial because that’s when you usually stop and\\r\\nthink.\\r\\n\\r\\nIf you are going through the Perceptron code, don’t copy and paste the code from\\r\\nthe repository . Type it down in your own Jupyter Notebook. Type everything. Don’t read\\r\\npassively. Get involved, type it down and you’ll assimilate the concepts faster.\\r\\n\\r\\nNEXT STEPS\\r\\nIn this post, my goal was to share my understanding of the algorithm and the\\r\\nlearnings I’ve made while reimplementing it. However, you can do much more than\\r\\nsimply reimplementing the model. You can actually use it with real data in order\\r\\nto do some simple predictions. In Python Machine Learning, Raschka uses the\\r\\nPerceptron to predict the class of Iris flower based on a the sepal and petal length of the flower. With actual data, you can\\r\\nthen evaluate the model and make predictions on unseen data.\\r\\n\\r\\nDON'T MISS MY NEXT POST, SUBSCRIBE TO MY NEWSLETTER:\\r\\nEmail Address First NameRELATED POSTS\\r\\n * 2016 IN REVIEW 27 DEC 2016\\r\\n   \\r\\n * LEARNING MACHINE LEARNING 13 DEC 2016\\r\\n   \\r\\n * TIDY DATA IN PYTHON 06 DEC 2016</td>\n",
       "      <td>In order to demystify some of the magic behind machine learning algorithms, I decided to implement a simple machine learning algorithm from scratch.</td>\n",
       "      <td>What I Learned Implementing a Classifier from Scratch in Python · Jean-Nicholas Hould</td>\n",
       "      <td>Live</td>\n",
       "      <td>1048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Learn how to use IBM dashDB as data store for Apache Spark. See how dashDB lets you analyze data loaded from Spark.</td>\n",
       "      <td>Use dashDB with Spark</td>\n",
       "      <td>Live</td>\n",
       "      <td>1049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>Homepage Follow Sign in / Sign up Homepage * Home\\r\\n * Data Science Experience\\r\\n * \\r\\n * Watson Data Platform\\r\\n * \\r\\n\\r\\nGreg Filla Blocked Unblock Follow Following Product manager &amp; Data scientist — Data Science Experience and Watson Machine\\r\\nLearning Jun 30, 2016\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nJupyter Notebooks with Scala, Python, or R Kernels\\r\\n\\r\\nOnce you get used to developing in a Notebook environment, it can be painful to\\r\\ngo back to traditional IDEs. In traditional IDEs, you execute your entire script\\r\\nand get a single output. This is great for application development, but is less\\r\\nthan ideal when performing data analysis.\\r\\n\\r\\nThere are many things to love about using notebooks for data science. Some\\r\\nobvious things that data scientists all love are features like:\\r\\n\\r\\n * Single line or code block execution . Process one block of code…see output…tweak it… then repeat.\\r\\n * Inline plotting . You won’t need to create an image file and open it up after the script\\r\\n   runs. Instead, execute your code and see the results immediately.\\r\\n\\r\\n * Markdown . Instead of just commenting your code, use markdown to clarify each step\\r\\n   you are taking in your workflow. Between this and plotting, we can easily\\r\\n   transform code into blog posts.\\r\\n\\r\\nGoing beyond these features there is something else to love about notebooks that\\r\\nwill really helps developers and data scientists who need to use different\\r\\nlanguages for different projects — the notebook kernel support for multiple\\r\\nlanguages.\\r\\n\\r\\nOriginally notebooks were limited to scripts written in Python. Data scientists\\r\\nneeded to set up their local environment to work with different languages. But\\r\\nthis can sometimes become a hassle when all you care about is digging into your\\r\\ndata set. This is no longer the case with IBM Data Science Experience- you can\\r\\ncreate notebooks using Python, R, or Scala.\\r\\n\\r\\nBelow is a screenshot from Data Science Experience that shows how easy it is to\\r\\ntoggle between languages for your notebook. This can come in handy when\\r\\ncollaborating with members of your team that prefer different languages. This\\r\\nmakes it easy to have a consistent file format or structure for your data\\r\\nscience code.\\r\\n\\r\\nIf you use any of these three languages and have not used a notebook before,\\r\\nsign up for Data Science Experience to see how easy it is!\\r\\n\\r\\n\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nOriginally published at datascience.ibm.com on June 30, 2016.\\r\\n\\r\\n * Data Science\\r\\n * Python\\r\\n * Jupyter\\r\\n * Notebook\\r\\n\\r\\nA single golf clap? Or a long standing ovation?By clapping more or less, you can signal to us which stories really stand out.\\r\\n\\r\\n3 Blocked Unblock Follow FollowingGREG FILLA\\r\\nProduct manager &amp; Data scientist — Data Science Experience and Watson Machine\\r\\nLearning\\r\\n\\r\\nFollowIBM WATSON DATA PLATFORM\\r\\nBuild smarter applications and quickly visualize, share, and gain insights\\r\\n\\r\\n * 3\\r\\n * \\r\\n * \\r\\n * \\r\\n\\r\\nNever miss a story from IBM Watson Data Platform , when you sign up for Medium. Learn more Never miss a story from IBM Watson Data Platform Get updates Get updates</td>\n",
       "      <td>Once you get used to developing in a Notebook environment, it can be painful to go back to traditional IDEs. In traditional IDEs, you execute your entire script and get a single output. This is great…</td>\n",
       "      <td>Jupyter Notebooks with Scala, Python, or R Kernels</td>\n",
       "      <td>Live</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         doc_body  \\\n",
       "1051                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        PouchDB-find is a new API and syntax that allows for a simpler way to query PouchDB. It is much more suited to ad-hoc querying and a fair amount easier to learn than PouchDB’s current way of querying documents via Map/Reduce. It is a MongoDB-inspired query language to query a PouchDB database. It works with PouchDB, Cloudant Query and CouchDB Mango (CouchDB 2.0 Release).   \n",
       "1052                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widely-held belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better.    \n",
       "1053  Essays about data, building products and bootstrapping businesses.\\r\\n\\r\\nHome About Archives Running Talks© 2017. All rights reserved.\\r\\n\\r\\nJEAN-NICHOLAS HOULD DATA SCIENCE & BUSINESS BOOTSTRAPPING\\r\\nWHAT I LEARNED IMPLEMENTING A CLASSIFIER FROM SCRATCH IN PYTHON\\r\\n04 Jan 2017This post is part of the Learning Machine Learning series. It’s based on Chapter 1 and 2 of Python Machine Learning .\\r\\n\\r\\nMachine learning can be intimidating for a newcomer. The concept of a machine\\r\\nlearning things alone is quite abstract. How does that work in practice?\\r\\n\\r\\nIn order to demystify some of the magic behind machine learning algorithms, I\\r\\ndecided to implement a simple machine learning algorithm from scratch. I will\\r\\nnot be using a library such as scikit-learn which already has many algorithms implemented. Instead, I’ll be writing all of\\r\\nthe code in order to have a working binary classifier algorithm. The goal of\\r\\nthis exercise is to understand its inner workings.\\r\\n\\r\\nSO, WHAT THE HECK IS A BINARY CLASSIFIER?\\r\\nA classifier is a machine-learning algorithm that determines the class of an\\r\\ninput element based on a set of features. For example, a classifier could be\\r\\nused to predict the category of a beer based on its characteristics, it’s\\r\\n“features”. These features could include its alcohol content, aroma, appearance,\\r\\netc. A machine learning classifier could potentially be used to predict that a\\r\\nbeer with 8% alcohol content, 100 IBU and with strong aromas of oranges is an\\r\\nIndian Pale Ale.\\r\\n\\r\\nIn machine learning, there are three main types of tasks: unsupervised learning,\\r\\nsupervised learning and reinforcement learning. The classifier algorithm falls\\r\\nunder the supervised learning category. Supervised learning means that we know\\r\\nthe right answer beforehand. The desired outputs are known. In the case of the\\r\\nbeer example, we could realistically have a dataset describing beers and their\\r\\ncategory. We could train the classifier algorithm to predict those categories\\r\\nbased on the beers features.\\r\\n\\r\\nA binary classifier classifies elements in two groups. Zero or one. True or\\r\\nfalse. IPA or not.\\r\\n\\r\\nBUILDING A MACHINE LEARNING MODEL\\r\\nThere are four steps to build and use a machine learning model.\\r\\n\\r\\n 1. Preprocessing\\r\\n 2. Learning\\r\\n 3. Evaluation\\r\\n 4. Prediction\\r\\n\\r\\nSource: Python Machine Learning by Sebastian Raschka.\\r\\n\\r\\nPREPROCESSING\\r\\nThe preprocessing is the first step in building a machine learning model. At\\r\\nthis step, you acquire and prepare the data for future usage. You clean up the\\r\\ndata, tidy it and select the features you want to use from your data.\\r\\n\\r\\nThe following tasks can be considered as part of the “preprocessing”:\\r\\n\\r\\n * Extract features from raw data\\r\\n * Clean and format the data\\r\\n * Remove superfluous features (or highly correlated features)\\r\\n * Reduce the number of features for performance\\r\\n * Standardize the range of feature data (also named Feature Scaling )\\r\\n * Split your dataset randomly: training dataset and test dataset\\r\\n\\r\\nLEARNING OR TRAINING\\r\\nOnce you have your datasets ready to be used, the second step is to select an\\r\\nalgorithm to perform your desired task. In our case, the algorithm we selected\\r\\nis a binary classifier called Perceptron. There are many algorithms designed to\\r\\ndo different tasks. They each have their strengths and weaknesses.\\r\\n\\r\\nAt this step, you can test a few algorithms, see how they perform and select the\\r\\nbest performing one. There are a wide variety of metrics that can be used to\\r\\nmeasure the performance of a machine learning model. According to Raschka, “one\\r\\ncommonly used metric is classification accuracy, which is defined as the\\r\\nproportion of correctly classified instances”. At this step, you will make\\r\\nadjustments to the parameters of your machine learning algorithm. These are\\r\\nnamed hyperparameters.\\r\\n\\r\\nIn this post, we’ll mainly focus on this part of the machine learning work flow.\\r\\nWe’ll deep dive in the algorithm inner workings. If you are interested in the\\r\\nother sections of the machine learning work flow, which you should be, I’ll be\\r\\nlinking to a great notebook at the end of this post.\\r\\n\\r\\nEVALUATION\\r\\nWhen the model has been “trained” on the dataset it can be evaluated on new\\r\\nunseen data. The goal here is to measure the generalization error . This metric measures “how accurately an algorithm is able to predict outcome\\r\\nvalues for previously unseen data”. Once you are satisfied with the results, you\\r\\ncan use your machine learning model to make predictions.\\r\\n\\r\\nINTRODUCING THE PERCEPTRON\\r\\nThe algorithm that we’ll be re-implementing is a Perceptron which is one of the very first machine learning algorithm.\\r\\n\\r\\nThe Perceptron algorithm is simple but powerful. Given a training dataset, the\\r\\nalgorithm automatically learns “the optimal weight coefficients that are then\\r\\nmultiplied with the input features in order to make the decision of whether a\\r\\nneuron fires or not”.\\r\\n\\r\\nBut, how does the algorithm do that?\\r\\n\\r\\nTHE ALGORITHM\\r\\nHere’s the sequence of the algorithm:\\r\\n\\r\\nFirst, we initialize an array with the weights equal to zero. The array length\\r\\nis equal to the number of features plus one. This additional feature is the\\r\\n“threshold”. It’s important to note that in the case of the Perceptron\\r\\nalgorithm, the features must be of numerical value.\\r\\n\\r\\nself.w_=np.zeros(1+X.shape[1])\\r\\n\\r\\nSecondly, we start a loop equal to the number of iterations n_iter . This is an hyperparameter defined by the data scientist.\\r\\n\\r\\nfor_inrange(self.n_iter):\\r\\n\\r\\nThirdly, we start a loop on each training data point and it’s target. The target\\r\\nis the desired output we want the algorithm to eventually predict. Since this is\\r\\na binary classifier, the targets are either -1 or 1 . They are of binary value.\\r\\n\\r\\nBased on the data point features, the algorithm will predict the category: 1 or -1 . The prediction calculation is a matricial multiplication of the features with\\r\\ntheir appropriate weights. To this multiplication we add the value of the\\r\\nthreshold. If the result is above 0, the predicted category is 1 . If the result is below 0, the predicted category is -1 .\\r\\n\\r\\nAt each iteration on a data point, if the prediction is not accurate, the\\r\\nalgorithm will adjust the weights. During the first few iterations, the\\r\\npredictions are not likely to be accurate because the weights haven’t been\\r\\nadjusted many times. They haven’t had a chance to start converging. The\\r\\nadjustments are made proportionally to the difference between the target and the\\r\\npredicted value. This difference is then multiplied by the learning rate eta , an hyperparameter of value between zero and one set by the data scientist.\\r\\nThe higher the eta is, the larger the correction on the weights will be. If the prediction is\\r\\naccurate, the algorithm won’t adjust the weights.\\r\\n\\r\\nself.w_=np.zeros(1+X.shape[1])for_inrange(self.n_iter):forxi,targetinzip(X,y):update=self.eta*(target-self.predict(xi))self.w_[1:]+=update*xiself.w_[0]+=updatedefnet_input(self,X):\"\"\"Calculate net input\"\"\"returnnp.dot(X,self.w_[1:])+self.w_[0]defpredict(self,X):\"\"\"Return class label after unit step\"\"\"returnnp.where(self.net_input(X)>=0.0,1,-1)\\r\\n\\r\\nThe Perceptron will converge only if the two classes are linearly separable.\\r\\nSimply said, if you are able to draw a straight line to entirely separate the\\r\\ntwo classes, the algorithm will converge. Else, the algorithm will keep\\r\\niterating and will readjust weights until it reaches the maximum number of\\r\\niterations n_iter .\\r\\n\\r\\nSource: Python Machine Learning by Sebastian Raschka.\\r\\n\\r\\nCOMPLETE CODE\\r\\nimportnumpyasnpclassPerceptron(object):\"\"\"Perceptron classifier.\\r\\n\\r\\n    Parameters\\r\\n    ------------\\r\\n    eta : float\\r\\n        Learning rate (between 0.0 and 1.0)\\r\\n    n_iter : int\\r\\n        Passes over the training dataset.\\r\\n\\r\\n    Attributes\\r\\n    -----------\\r\\n    w_ : 1d-array\\r\\n        Weights after fitting.\\r\\n    errors_ : list\\r\\n        Number of misclassifications in every epoch.\\r\\n\\r\\n    \"\"\"def__init__(self,eta=0.01,n_iter=10):self.eta=etaself.n_iter=n_iterdeffit(self,X,y):\"\"\"Fit training data.\\r\\n\\r\\n        Parameters\\r\\n        ----------\\r\\n        X : {array-like}, shape = [n_samples, n_features]\\r\\n            Training vectors, where n_samples is the number of samples and\\r\\n            n_features is the number of features.\\r\\n        y : array-like, shape = [n_samples]\\r\\n            Target values.\\r\\n\\r\\n        Returns\\r\\n        -------\\r\\n        self : object\\r\\n\\r\\n        \"\"\"self.w_=np.zeros(1+X.shape[1])self.errors_=[]for_inrange(self.n_iter):errors=0forxi,targetinzip(X,y):update=self.eta*(target-self.predict(xi))self.w_[1:]+=update*xiself.w_[0]+=updateerrors+=int(update!=0.0)self.errors_.append(errors)returnselfdefnet_input(self,X):\"\"\"Calculate net input\"\"\"returnnp.dot(X,self.w_[1:])+self.w_[0]defpredict(self,X):\"\"\"Return class label after unit step\"\"\"returnnp.where(self.net_input(X)>=0.0,1,-1)\\r\\n\\r\\nSource: Python Machine Learning by Sebastian Raschka.\\r\\n\\r\\nTHREE LEARNINGS\\r\\nLEARNING RATE, NUMBER OF ITERATION & CONVERGENCE\\r\\nParameters such as learning rate and number of iteration can seem very abstract if you jump in straight to using an algorithm from a\\r\\nlibrary like scikit-learn . It’s hard to grasp what these really do. By implementing the algorithm, it’s\\r\\nnow clear for me what they represent in the context of the Perceptron.\\r\\n\\r\\nLearning RateThe learning rate is a ratio by which the weights are corrected when the\\r\\nprediction is not accurate. The value needs to be between zero and one. As you\\r\\ncan see in the snippet below, the fit function will iterate on each observation, call the predict function and then adjust the weights based on the difference between the target\\r\\nand the predicted value and then multiplied by the learning rate.\\r\\n\\r\\nA higher learning rate means that the algorithm will adjust the weights more\\r\\naggressively. At each iteration, the weights will be adjusted if the predicted\\r\\nvalue is inaccurate.\\r\\n\\r\\n# Partial portion of the \"fit\" functionforxi,targetinzip(X,y):update=self.eta*(target-self.predict(xi))self.w_[1:]+=update*xiself.w_[0]+=updateerrors+=int(update!=0.0)\\r\\n\\r\\nNumber of iterationsThe number of iteration is the number of times the algorithm will run through\\r\\nthe training dataset. If the number of iteration was set to one, the algorithm\\r\\nwould loop through the dataset only once and update the weights one time for\\r\\neach data point. The resulting model would be more likely to be inaccurate than\\r\\na model with a higher number of iteration. On large datasets, there is a cost to\\r\\nhaving a high number of iterations.\\r\\n\\r\\nfor_inrange(self.n_iter):errors=0forxi,targetinzip(X,y):update=self.eta*(target-self.predict(xi))self.w_[1:]+=update*xiself.w_[0]+=updateerrors+=int(update!=0.0)self.errors_.append(errors)\\r\\n\\r\\nThe learning rate and number of iteration go hands-in-hand. They need to be adjusted together. For example, if you have a\\r\\nvery low learning rate , which means that the algorithm will adjust it’s weight only marginally at\\r\\neach iteration, you will probably need a higher number of iteration.\\r\\n\\r\\nLINEAR ALGEBRA\\r\\nIt’s critical to mention that the capabilities of the Perceptron algorithm are\\r\\nattributable to linear algebra. The whole algorithm can be described through\\r\\nlinear algebra formulas. If you have never done linear algebra in college, the\\r\\nformulas will be cryptic. As usual, Khan Academy is a great place to start with if you want to get familiar with linear algebra.\\r\\nIt’s also a great place to get a refresher on the topic.\\r\\n\\r\\nFor me, the main learning here is how fundamental linear algebra is to this\\r\\nmachine learning algorithm.\\r\\n\\r\\nTYPE EVERYTHING\\r\\nThis learning is actually a concept I re-learned while going through the code\\r\\nfor this post. It’s not specific to machine learning and it has nothing to do\\r\\nwith the Perceptron.\\r\\n\\r\\nBack in 2012 when I was learning to code Ruby on Rails, a web application\\r\\ndevelopment framework, I realized that typing down all of the code examples from\\r\\ntutorials really helped me memorize and understand the concepts. I spent weeks\\r\\nwriting code while following tutorials. No copy and paste. I typed all the code.\\r\\nThis may sound stupid but it was extremely helpful to grasp the concepts. During\\r\\nthe process, I inevitably made typos and spend some time figuring out what was\\r\\nbroken. These moments where crucial because that’s when you usually stop and\\r\\nthink.\\r\\n\\r\\nIf you are going through the Perceptron code, don’t copy and paste the code from\\r\\nthe repository . Type it down in your own Jupyter Notebook. Type everything. Don’t read\\r\\npassively. Get involved, type it down and you’ll assimilate the concepts faster.\\r\\n\\r\\nNEXT STEPS\\r\\nIn this post, my goal was to share my understanding of the algorithm and the\\r\\nlearnings I’ve made while reimplementing it. However, you can do much more than\\r\\nsimply reimplementing the model. You can actually use it with real data in order\\r\\nto do some simple predictions. In Python Machine Learning, Raschka uses the\\r\\nPerceptron to predict the class of Iris flower based on a the sepal and petal length of the flower. With actual data, you can\\r\\nthen evaluate the model and make predictions on unseen data.\\r\\n\\r\\nDON'T MISS MY NEXT POST, SUBSCRIBE TO MY NEWSLETTER:\\r\\nEmail Address First NameRELATED POSTS\\r\\n * 2016 IN REVIEW 27 DEC 2016\\r\\n   \\r\\n * LEARNING MACHINE LEARNING 13 DEC 2016\\r\\n   \\r\\n * TIDY DATA IN PYTHON 06 DEC 2016   \n",
       "1054                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          NaN   \n",
       "1055                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Homepage Follow Sign in / Sign up Homepage * Home\\r\\n * Data Science Experience\\r\\n * \\r\\n * Watson Data Platform\\r\\n * \\r\\n\\r\\nGreg Filla Blocked Unblock Follow Following Product manager & Data scientist — Data Science Experience and Watson Machine\\r\\nLearning Jun 30, 2016\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nJupyter Notebooks with Scala, Python, or R Kernels\\r\\n\\r\\nOnce you get used to developing in a Notebook environment, it can be painful to\\r\\ngo back to traditional IDEs. In traditional IDEs, you execute your entire script\\r\\nand get a single output. This is great for application development, but is less\\r\\nthan ideal when performing data analysis.\\r\\n\\r\\nThere are many things to love about using notebooks for data science. Some\\r\\nobvious things that data scientists all love are features like:\\r\\n\\r\\n * Single line or code block execution . Process one block of code…see output…tweak it… then repeat.\\r\\n * Inline plotting . You won’t need to create an image file and open it up after the script\\r\\n   runs. Instead, execute your code and see the results immediately.\\r\\n\\r\\n * Markdown . Instead of just commenting your code, use markdown to clarify each step\\r\\n   you are taking in your workflow. Between this and plotting, we can easily\\r\\n   transform code into blog posts.\\r\\n\\r\\nGoing beyond these features there is something else to love about notebooks that\\r\\nwill really helps developers and data scientists who need to use different\\r\\nlanguages for different projects — the notebook kernel support for multiple\\r\\nlanguages.\\r\\n\\r\\nOriginally notebooks were limited to scripts written in Python. Data scientists\\r\\nneeded to set up their local environment to work with different languages. But\\r\\nthis can sometimes become a hassle when all you care about is digging into your\\r\\ndata set. This is no longer the case with IBM Data Science Experience- you can\\r\\ncreate notebooks using Python, R, or Scala.\\r\\n\\r\\nBelow is a screenshot from Data Science Experience that shows how easy it is to\\r\\ntoggle between languages for your notebook. This can come in handy when\\r\\ncollaborating with members of your team that prefer different languages. This\\r\\nmakes it easy to have a consistent file format or structure for your data\\r\\nscience code.\\r\\n\\r\\nIf you use any of these three languages and have not used a notebook before,\\r\\nsign up for Data Science Experience to see how easy it is!\\r\\n\\r\\n\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nOriginally published at datascience.ibm.com on June 30, 2016.\\r\\n\\r\\n * Data Science\\r\\n * Python\\r\\n * Jupyter\\r\\n * Notebook\\r\\n\\r\\nA single golf clap? Or a long standing ovation?By clapping more or less, you can signal to us which stories really stand out.\\r\\n\\r\\n3 Blocked Unblock Follow FollowingGREG FILLA\\r\\nProduct manager & Data scientist — Data Science Experience and Watson Machine\\r\\nLearning\\r\\n\\r\\nFollowIBM WATSON DATA PLATFORM\\r\\nBuild smarter applications and quickly visualize, share, and gain insights\\r\\n\\r\\n * 3\\r\\n * \\r\\n * \\r\\n * \\r\\n\\r\\nNever miss a story from IBM Watson Data Platform , when you sign up for Medium. Learn more Never miss a story from IBM Watson Data Platform Get updates Get updates   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                      doc_description  \\\n",
       "1051                                                                                                     PouchDB uses MapReduce as its default search mechanism but that's about to change. Garren Smith gives an insight into how PouchDB's find plugin works and its relationship to the Cloudant Query search technology that uses a MongoDB-style query language.   \n",
       "1052  We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widely-held belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better.    \n",
       "1053                                                                                                                                                                                                             In order to demystify some of the magic behind machine learning algorithms, I decided to implement a simple machine learning algorithm from scratch.   \n",
       "1054                                                                                                                                                                                                                                             Learn how to use IBM dashDB as data store for Apache Spark. See how dashDB lets you analyze data loaded from Spark.    \n",
       "1055                                                                                                                                                         Once you get used to developing in a Notebook environment, it can be painful to go back to traditional IDEs. In traditional IDEs, you execute your entire script and get a single output. This is great…   \n",
       "\n",
       "                                                                              doc_full_name  \\\n",
       "1051                                                A look under the covers of PouchDB-find   \n",
       "1052                                   A comparison of logistic regression and naive Bayes    \n",
       "1053  What I Learned Implementing a Classifier from Scratch in Python · Jean-Nicholas Hould   \n",
       "1054                                                                  Use dashDB with Spark   \n",
       "1055                                     Jupyter Notebooks with Scala, Python, or R Kernels   \n",
       "\n",
       "     doc_status  article_id  \n",
       "1051       Live        1046  \n",
       "1052       Live        1047  \n",
       "1053       Live        1048  \n",
       "1054       Live        1049  \n",
       "1055       Live        1050  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show df_content to get an idea of the data\n",
    "df_content.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Exploratory-Data-Analysis\">Part I : Exploratory Data Analysis</a>\n",
    "\n",
    "Use the dictionary and cells below to provide some insight into the descriptive statistics of the data.\n",
    "\n",
    "`1.` What is the distribution of how many articles a user interacts with in the dataset?  Provide visual and descriptive statistics to assist with giving a look at the number of times each user interacts with an article.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45993, 3)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the shape of df and df_content\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1056, 5)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5149"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the numbero unique users\n",
    "len(df['email'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.285e+03, 4.350e+02, 1.950e+02, 8.900e+01, 5.500e+01, 2.300e+01,\n",
       "        2.800e+01, 1.100e+01, 7.000e+00, 2.000e+00, 2.000e+00, 7.000e+00,\n",
       "        4.000e+00, 3.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 2.000e+00]),\n",
       " array([  1. ,  13.1,  25.2,  37.3,  49.4,  61.5,  73.6,  85.7,  97.8,\n",
       "        109.9, 122. , 134.1, 146.2, 158.3, 170.4, 182.5, 194.6, 206.7,\n",
       "        218.8, 230.9, 243. , 255.1, 267.2, 279.3, 291.4, 303.5, 315.6,\n",
       "        327.7, 339.8, 351.9, 364. ]),\n",
       " <BarContainer object of 30 artists>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check the distribution of the number of article per user\n",
    "plt.hist(df.groupby(['email']).count()['article_id'], bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5148.000000\n",
       "mean        8.930847\n",
       "std        16.802267\n",
       "min         1.000000\n",
       "25%         1.000000\n",
       "50%         3.000000\n",
       "75%         9.000000\n",
       "max       364.000000\n",
       "Name: article_id, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# descriptive statistics of the number of articles per user\n",
    "df.groupby(['email']).count()['article_id'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50% of individuals interact with 3 number of articles or fewer.\n",
      "The maximum number of user-article interactions by any 1 user is 364.\n"
     ]
    }
   ],
   "source": [
    "# Fill in the median and maximum number of user_article interactios below\n",
    "\n",
    "median_val = int(df.groupby(['email']).count()['article_id'].describe()['50%'])\n",
    "print(\"50% of individuals interact with {} number of articles or fewer.\".format(median_val))\n",
    "max_views_by_user = int(df.groupby(['email']).count()['article_id'].describe()['max'].round())\n",
    "print(\"The maximum number of user-article interactions by any 1 user is {}.\".format(max_views_by_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Explore and remove duplicate articles from the **df_content** dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Follow Sign in / Sign up Home About Insight Data Science Data Engineering Health Data AI Never miss a story from Insight Data , when you sign up for Medium. Learn more Never miss a story from Insight Data Get updates Get updates Sebastien Dery Blocked Unblock Follow Following I don’t know what I’m doing; but then neither do you so it’s all good. Master\\r\\nof Layers, Protector of the Graph, Wielder of Knowledge. #OpenScience Oct 16\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nGRAPH-BASED MACHINE LEARNING: PART I\\r\\nCOMMUNITY DETECTION AT SCALE\\r\\nDuring the seven-week Insight Data Engineering Fellows Program recent grads and experienced software engineers learn the latest open source technologies by building a data platform to handle large, real-time datasets.\\r\\n\\r\\nSebastien Dery (now a Data Science Engineer at Yewno ) discusses his project on community detection on large datasets.\\r\\n\\r\\n\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\n#tltr : Graph-based machine learning is a powerful tool that can easily be merged\\r\\ninto ongoing efforts. Using modularity as an optimization goal provides a\\r\\nprincipled approach to community detection. Local modularity increment can be\\r\\ntweaked to your own dataset to reflect interpretable quantities. This is useful\\r\\nin many scenarios, making it a prime candidate for your everyday toolbox.Many important problems can be represented and studied using graphs — social\\r\\nnetworks, interacting bacterias, brain network modules, hierarchical image\\r\\nclustering and many more.\\r\\n\\r\\nIf we accept graphs as a basic means of structuring and analyzing data about the\\r\\nworld, we shouldn’t be surprised to see them being widely used in Machine\\r\\nLearning as a powerful tool that can enable intuitive properties and power a lot\\r\\nof useful features. Graph-based machine learning is destined to become a\\r\\nresilient piece of logic, transcending a lot of other techniques. See more in\\r\\nthis recent blog post from Google Research\\r\\n\\r\\nThis post explores the tendencies of nodes in a graph to spontaneously form\\r\\nclusters of internally dense linkage (hereby termed “community”); a remarkable\\r\\nand almost universal property of biological networks. This is particularly\\r\\ninteresting knowing that a lot of information can be extrapolated from a node’s\\r\\nneighbor (e.g. think recommendation system, respondent analysis, portfolio\\r\\nclustering). So how can we extract this kind of information?\\r\\n\\r\\nCommunity Detection aims to partition a graph into clusters of densely connected nodes, with the\\r\\nnodes belonging to different communities being only sparsely connected.\\r\\n\\r\\nGraph analytics concerns itself with the study of nodes (depicted as disks) and\\r\\ntheir interactions with other nodes (lines). Community Detection aims to\\r\\nclassify nodes by their “clique”.“ Is it the same as clustering? ”\\r\\n\\r\\n * Short answer: Yes .\\r\\n * Long answer: For all intents and purposes, yes it is .\\r\\n\\r\\nSo why shouldn’t I just use my good old K-Means? You absolutely should, unless\\r\\nyour data and requirements don’t work well with that algorithm’s assumptions,\\r\\nnamely:\\r\\n\\r\\n 1. K number of clusters\\r\\n 2. Sum of Squared Error (SSE) as the right optimization cost\\r\\n 3. All variable have the same variance\\r\\n 4. The variance of the distribution of each attribute is spherical\\r\\n\\r\\nFor a more in-depth look click here .\\r\\n\\r\\nFirst off, let’s drop this idea of SSE and choose a more relevant notation of\\r\\nwhat we’re looking for: the internal versus external relationships between nodes\\r\\nof a community. Let’s discuss the notion of modularity.\\r\\n\\r\\nwhere: nc is the number of communities; lc number of edges within; dc sum of vertex degree; and m the size of the graph (number of edges). We will be using this equation as a\\r\\nglobal metric of goodness during our search for an optimal partitioning. In a nutshell: Higher score will be given to a community configuration offering higher\\r\\ninternal versus external linkage.So all I have to do is optimize this and we’re done, right?\\r\\n\\r\\nA major problem in the theoretical formulation of this optimization scheme is\\r\\nthat we need an all-knowing knowledge of the graph topology (geometric\\r\\nproperties and spatial relations). This is rather, let’s say, intractable . Apparently we can’t do any better than to try all possible subsets of the\\r\\nvertices and check to see which, if any, form communities. The problem of finding the largest clique in a graph is thus said to be NP-hard .\\r\\n\\r\\nHowever, several algorithms have been proposed over the years to find reasonably good partitions in reasonable amounts of time, each with its own particular flavor. This post focuses on a\\r\\nspecific family of algorithms called agglomerative . These algorithms work very simply by collecting (or merging) nodes together.\\r\\nThis has a lot of advantages since it typically only requires a knowledge of first degree neighbors and small incremental merging steps , to bring the global solution towards stepwise equilibriums.\\r\\n\\r\\nYou might point out that the modularity metric gives a global perspective on the\\r\\nstate of the graph and not a local indicator. So, how does this translate to the\\r\\nsmall local increment that I just mentioned?\\r\\n\\r\\nThe basic approach does indeed consists of iteratively merging nodes that\\r\\noptimize a local modularity so let’s go ahead and define that as well:\\r\\n\\r\\nwhere ∑ in is the sum of weighted links inside C, ∑ tot sum of weighted links incident to nodes in C, k i sum of weighted links incident to node i , k i, in sum of weighted links going from i to nodes in C and m a normalizing factor as the sum of weighted links for the whole graph. (Sorry, Medium doesn’t allow subscript and superscript)This is part of the magic for me as this local optimization function can easily\\r\\nbe translated to an interpretable metric within the domain of your graph. For\\r\\nexample,\\r\\n\\r\\n * Community Strength: Sum of Weighted Link within a community.\\r\\n * Community Popularity: Sum of Weighted Link incident to nodes within a specific community.\\r\\n * Node Belonging: Sum of Weighted Link from a node to a community.\\r\\n\\r\\nThere’s also nothing stopping from adding more terms to the previous equation\\r\\nthat are specific to your dataset. In other words, the weighted links can be a\\r\\nfunction of the type of nodes computed on-the-fly (useful if you’re dealing with\\r\\na multidimensional graph with various types of relationships and nodes).\\r\\n\\r\\nExample of converging iterations before the Compress phaseNow that we’re all set with our optimization function and local cost, the\\r\\ntypical agglomerative strategy consists of two iterative phases ( Transfer and Compress ). Assuming a weighted network of N nodes, we begin by assigning a different community to each node of the network.\\r\\n\\r\\n 1. Transfer : For each node i, consider its neighbors j and evaluate the gain in modularity by swapping c_i for c_j . The greedy process transfers the node into the neighboring community,\\r\\n    maximizing the gain in modularity (assuming the gain is positive). If no\\r\\n    positive gain is possible, the node i stays in its original community. This process is applied to all nodes until\\r\\n    no individual move can improve the modularity (i.e. a local maxima of\\r\\n    modularity is attained — a state of equilibrium).\\r\\n 2. Compress : building a new network whose nodes are the communities found during the\\r\\n    first phase; a process termed compression (see Figure below). To do so, edge weights between communities are computed\\r\\n    as the sum of the internal edges between nodes in the corresponding two\\r\\n    communities.\\r\\n\\r\\nAgglomerative process: Phase one converges to a local equilibrium of local\\r\\nmodularity. Phase two consist in compressing the graph for the next iteration,\\r\\nthus reducing the number of nodes to consider and incidentally computation time\\r\\nas well.Now the tricky part: as this is a greedy algorithm , you’ll have to define a stopping criteria based on your case scenario and the\\r\\ndata at hand.\\r\\n\\r\\nHow to define this criteria? It can be a lot of things: a maximum number of iterations, a minimum modularity\\r\\ngain during the transfer phase, or any other relevant piece of information\\r\\nrelated to your data that would inform you that it needs to stop.\\r\\n\\r\\nStill not sure when to stop ? Just make sure you save every intermediate step of the iterative process\\r\\nsomewhere, let the optimization run until there’s only one node left in your\\r\\ngraph, and then look back at your data! The interesting part is that by keeping\\r\\ntrack of each step, you also profit from a hierarchical view of your communities\\r\\nwhich can be further explored and leveraged.\\r\\n\\r\\nIn a follow up post, I will discuss how we can achieve this on a distributed\\r\\nsystem using Spark GraphX , part of my project while at the Insight Data Engineering Fellows Program .\\r\\n\\r\\n[0803.0476] Fast unfolding of communities in large networks Abstract: We propose\\r\\na simple method to extract the community structure of large networks. Our method\\r\\nis a heuristic… arxiv.org\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nWant to learn Spark, machine learning with graphs, and other big data tools from\\r\\ntop data engineers in Silicon Valley or New York? The Insight Data Engineering Fellows Program is a free 7-week professional training where you can build cutting edge big\\r\\ndata platforms and transition to a career in data engineering at top teams like\\r\\nFacebook, Uber, Slack and Squarespace.\\r\\n\\r\\nLearn more about the program and apply today .\\r\\n\\r\\nBig Data Data Science Machine Learning Social Network Analysis Insight Data Engineering 4 Blocked Unblock Follow FollowingSEBASTIEN DERY\\r\\nI don’t know what I’m doing; but then neither do you so it’s all good. Master of\\r\\nLayers, Protector of the Graph, Wielder of Knowledge. #OpenScience\\r\\n\\r\\nFollowINSIGHT DATA\\r\\nInsight Fellows Program —Your bridge to careers in Data Science and Data\\r\\nEngineering.</td>\n",
       "      <td>Community Detection at Scale</td>\n",
       "      <td>Graph-based machine learning</td>\n",
       "      <td>Live</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>* United States\\r\\n\\r\\nIBM® * Site map\\r\\n\\r\\nSearch within Bluemix Blog Bluemix Blog * About Bluemix * What is Bluemix\\r\\n    * Getting Started\\r\\n    * Case Studies\\r\\n    * Hybrid Architecture\\r\\n    * Open Source\\r\\n    * Trust, Security, Privacy\\r\\n    * Data Centers\\r\\n    * Our Network\\r\\n    * Automation\\r\\n    * Architecture Center\\r\\n   \\r\\n   \\r\\n * Products * Compute Infrastructure\\r\\n    * Compute Services\\r\\n    * Hybrid Deployments\\r\\n    * Watson\\r\\n    * Internet of Things\\r\\n    * Mobile\\r\\n    * DevOps\\r\\n    * Data Analytics\\r\\n    * Network\\r\\n    * Open Source\\r\\n    * Storage\\r\\n    * Security\\r\\n   \\r\\n   \\r\\n * Services * Bluemix Services\\r\\n    * Garage\\r\\n   \\r\\n   \\r\\n * Pricing\\r\\n * Support * Support\\r\\n    * Contact Us\\r\\n    * Resources\\r\\n    * Docs\\r\\n   \\r\\n   \\r\\n * Blog * How-tos\\r\\n    * Trending\\r\\n    * What's New\\r\\n    * Events\\r\\n   \\r\\n   \\r\\n * Partners * Partners\\r\\n    * Become a Partner\\r\\n    * Find a Partner\\r\\n   \\r\\n   \\r\\n * Sign up\\r\\n\\r\\nDATA ANALYTICSHOW SMART CATALOGS CAN TURN THE BIG DATA FLOOD INTO AN OCEAN OF OPPORTUNITY\\r\\nAugust 1, 2017 | Written by: Jay Limburn\\r\\n\\r\\nCategorized: Data Analytics\\r\\n\\r\\nShare this post:\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nOne of the earliest documented catalogs was compiled at the great library of\\r\\nAlexandria in the third century BC, to help scholars manage, understand and\\r\\naccess its vast collection of literature. While that cataloging process\\r\\nrepresented a massive undertaking for the Alexandrian librarians, it pales in\\r\\ncomparison to the task of wrangling the volume and variety of data that modern\\r\\norganizations generate.\\r\\n\\r\\nNowadays, data is often described as an organization’s most valuable asset, but\\r\\nunless users can easily sift through data artifacts to find the information they\\r\\nneed, the value of that data may remain unrealized. Catalogs can solve this\\r\\nproblem by providing an indexed set of information about the organization’s\\r\\ndata, storing metadata that describes all assets and providing a reference to\\r\\nwhere they can be found or accessed.\\r\\n\\r\\nIt’s not just the size and complexity of the data that makes cataloging a tough\\r\\nchallenge: organizations also need to be able to perform increasingly\\r\\ncomplicated operations on that data at high speed, and even in real-time. As a\\r\\nresult, technology leaders must continually find better ways to solve today’s\\r\\nversion of the same cataloging challenges faced in Alexandria all those years\\r\\nago.\\r\\n\\r\\n\\r\\n\\r\\nENTER IBM\\r\\nIBM’s aim with Watson Data Platform is to make data accessible for anyone who uses it. An integral part of Watson\\r\\nData Platform will be a new intelligent asset catalog, IBM Data Manager, a\\r\\nsolution underpinned by a central repository of metadata describing all the\\r\\ninformation managed by the platform. Unlike many other catalog solutions on the\\r\\nmarket, the intelligent asset catalog will also offer full end-to-end\\r\\ncapabilities around data lifecycle and governance.\\r\\n\\r\\nBecause all the elements of Watson Data Platform can utilize the same catalog,\\r\\nusers will be able to share data with their colleagues more easily, regardless\\r\\nof what the data is, where it is stored, or how they intend to use it. In this\\r\\nway, the intelligent asset catalog will unlock the value held within that data\\r\\nacross user groups—helping organizations use this key asset to its full\\r\\npotential.\\r\\n\\r\\n\\r\\n\\r\\nBREAKING DOWN SILOS\\r\\nWith Watson Data Platform, data engineers, data scientists and other knowledge\\r\\nworkers throughout an enterprise can search for, share and leverage assets\\r\\n(including datasets, files, connections, notebooks, data flows, models and\\r\\nmore). Assets can be accessed using the Data Science Experience web user interface to analyze data,\\r\\n\\r\\nTo collaborate with colleagues, users can put assets into a Project that acts as\\r\\na shared sandbox where the whole team can access and utilize them. Once their\\r\\nwork is complete, they can submit any resulting content to the catalog for\\r\\nfurther reuse by other people and groups across the organization.\\r\\n\\r\\nRich metadata about each asset makes it easy for knowledge workers to find and\\r\\naccess relevant resources. Along with data files, the catalog can also include\\r\\nconnections to databases and other data sources, both on- and off-premises,\\r\\ngiving users a full 360-degree view to all information relevant to their\\r\\nbusiness, regardless of where or how it is stored.\\r\\n\\r\\n\\r\\n\\r\\nMANAGING DATA OVER TIME\\r\\nIt’s important to look at data as an evolving asset, rather than something that\\r\\nstays fixed over time. To help manage and trace this evolution, IBM Data Manager\\r\\nwill keep a complete track of which users have added or modified each asset, so\\r\\nthat it is always clear who is responsible for any changes.\\r\\n\\r\\n\\r\\n\\r\\nSMART CATALOG CAPABILITIES FOR BIG DATA MANAGEMENT\\r\\nThe concept of catalogs may be simple, but when they’re being used to make sense\\r\\nof huge amounts of constantly changing data, smart capabilities make all the\\r\\ndifference. Here are some of the key smart catalog functionalities that we see\\r\\nas integral to tackling the big data challenge, and that we will be aiming to\\r\\ninclude in upcoming releases of IBM Data Manager.\\r\\n\\r\\n\\r\\n\\r\\nDATA AND ASSET TYPE AWARENESS\\r\\nWhen a user chooses to preview or view an asset of a particular type, the data\\r\\nand asset type awareness feature will automatically launch the data in the best\\r\\nviewer—such as a shaper for a dataset, or a canvas for a data flow. This will\\r\\nsave time and boost productivity for users, optimizing discovery and making it\\r\\neasier to work with a variety of data types without switching tools.\\r\\n\\r\\n\\r\\n\\r\\nINTELLIGENT SEARCH AND EXPLORATION\\r\\nBy combining metadata, machine learning-based algorithms and user interaction\\r\\ndata, it is possible to fine-tune search results over time. Presenting users\\r\\nwith the most relevant data for their purpose will increase usefulness of the\\r\\nsolution the more it is used.\\r\\n\\r\\n\\r\\n\\r\\nSOCIAL CURATION\\r\\nEffective use of data throughout your organization is a two-way street: when\\r\\nusers discover a useful dataset, it’s important for them to help others find it\\r\\ntoo. Users can be encouraged to engage by taking advantage of curation features,\\r\\nenabling them to tag, rank and comment on assets within the catalog. By\\r\\naugmenting the metadata for each asset, this can help the catalog’s intelligent\\r\\nsearch algorithms guide users to the assets that are most relevant to their\\r\\nneeds.\\r\\n\\r\\n\\r\\n\\r\\nDATA LINEAGE\\r\\nIf data is incomplete or inaccurate, utilizing it can cause more problems than\\r\\nit solves. On the other hand, if data is accurate but users do not trust it,\\r\\nthey might not use it when it could make a real difference. In either scenario,\\r\\ndata lineage can help.\\r\\n\\r\\nData lineage captures the complete history of an asset in the catalog: from its\\r\\noriginal source, through all the operations and transformations it has\\r\\nundergone, to its current state. By exploring this lineage, users can be\\r\\nconfident they know where assets have come from, how those assets have evolved,\\r\\nand whether they can be trusted.\\r\\n\\r\\n\\r\\n\\r\\nMONITORING\\r\\nTaking a step back to a higher-level view, monitoring features will help users\\r\\nkeep track of overall usage of the catalog. Real-time dashboards help chief data\\r\\nofficers and other data professionals monitor how data is being used, and\\r\\nidentify ways to increase its usage in different areas of the organization.\\r\\n\\r\\n\\r\\n\\r\\nMETADATA DISCOVERY\\r\\nWe have already mentioned that data needs to be seen as an evolving asset—which\\r\\nmeans our catalogs must evolve with it. We plan to make it easy for users to\\r\\naugment assets with metadata manually; in the future, it may also be possible to\\r\\nintegrate algorithms that can discover assets and capture their metadata\\r\\nautomatically.\\r\\n\\r\\n\\r\\n\\r\\nDATA GOVERNANCE\\r\\nFor many organizations, keeping data secure while ensuring access for authorized\\r\\nusers is one of the most significant information management challenges. You can\\r\\nmitigate this challenge with rule-based access control and automatic enforcement\\r\\nof data governance policies.\\r\\n\\r\\n\\r\\n\\r\\nAPIS\\r\\nFinally, the catalog will enable access to all these capabilities and more\\r\\nthrough a set of well-defined, RESTful APIs. IBM is committed to offering\\r\\napplication developers easy access to additional components of Watson Data Platform , such as persistence stores and data sets. We hope that they can use our\\r\\nservices to extend their current suite of data and analytics tools, to innovate\\r\\nand create smart new ways of working with data.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nIn our next post, we’ll discuss the challenges around data governance, and\\r\\nexplore how IBM Data Manager can help you make light work of addressing them.\\r\\n\\r\\nJAY LIMBURN\\r\\nJay Limburn\\r\\n\\r\\nTHOMAS SCHAECK\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nPrevious Post\\r\\n\\r\\nWebSphere on the Cloud: Application Modernization (Phase 1)Next Post\\r\\n\\r\\nMaximize Control with IBM Bluemix Virtual serversADD COMMENT NO COMMENTS\\r\\nLEAVE A REPLY CANCEL REPLY\\r\\nYour email address will not be published. Required fields are marked *\\r\\n\\r\\nComment\\r\\n\\r\\nName *\\r\\n\\r\\nEmail *\\r\\n\\r\\nWebsite\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nSearch for:RECENT POSTS\\r\\n * IBM Watson Machine Learning – General Availability\\r\\n * Locating IoT with Skyhook Precision Location\\r\\n * Monitoring IBM Bluemix Container Service with Sysdig Container Intelligence\\r\\n * Mobile Foundation Service integration with Mobile Analytics Service\\r\\n * Intel® Optane™ SSD DC P4800X Available Now on IBM Cloud\\r\\n\\r\\nARCHIVES\\r\\nArchives Select Month August 2017 July 2017 June 2017 May 2017 April 2017 March 2017 February 2017 January 2017 December 2016 November 2016 October 2016 September 2016 August 2016 July 2016 June 2016 May 2016 April 2016 March 2016 February 2016 January 2016 December 2015 November 2015 October 2015 September 2015 August 2015 July 2015 June 2015 May 2015 April 2015 March 2015 February 2015 January 2015 December 2014 November 2014 October 2014 September 2014 August 2014 July 2014 June 2014 May 2014 April 2014 March 2014 February 2014 November 2013TAGS\\r\\nanalytics announcements api apps Architecture Center best-of-bluemix Bluemix bluemix-support-notifications buildpacks client success cloud cloudant cloud foundry conference conferences containers dashdb deployment devops docker eclipse garage garage-method hackathon homepage hybrid interconnect iot java Kubernetes liberty local microservices mobile MobileFirst node.js OpenStack openwhisk security Spark swift twilio video watson webinar More Data Analytics StoriesData Analytics\\r\\n\\r\\nMEDTRONIC MAKES DIABETES MANAGEMENT EASIER WITH REAL-TIME INSIGHTS FROM IBM\\r\\nSTREAMS\\r\\nWith cases of both type I and type II diabetes rising, Medtronic recognized the\\r\\nneed to create a new generation of glucose monitoring solutions that would give\\r\\npeople the tools to manage their diabetes more easily, in combination with\\r\\nroutine support from healthcare professionals. Find out how they are working\\r\\nwith IBM Watson to help.\\r\\n\\r\\nContinue reading\\r\\n\\r\\n\\r\\n\\r\\nShare this post:\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nData Analytics\\r\\n\\r\\nINTRODUCING A NEW LOOK AND FEEL FOR DB2 WAREHOUSE ON CLOUD\\r\\nToday, we're proud to announce the launch and immediate availability of the\\r\\nbrand new Db2 Warehouse on Cloud Web console and REST APIs! We want to make your\\r\\ninteraction with our world-class cloud data warehouse offering as seamless as\\r\\npossible, so we set out to completely redesign these two integral parts of our\\r\\nuser experience.\\r\\n\\r\\nContinue reading\\r\\n\\r\\n\\r\\n\\r\\nShare this post:\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nData Analytics\\r\\n\\r\\nIBM DASHDB FOR ANALYTICS IS NOW DB2 WAREHOUSE ON CLOUD\\r\\nWe're rebranding dashDB for Analytics to Db2 Warehouse on Cloud.\\r\\n\\r\\nContinue reading\\r\\n\\r\\n\\r\\n\\r\\nShare this post:\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nSIGN UP FOR A BLUEMIX TRIAL TODAY\\r\\n\\r\\n\\r\\nGet started free Learn more about Bluemix\\r\\n\\r\\nCONNECT WITH US\\r\\n\\r\\n\\r\\n * Contact\\r\\n * Privacy\\r\\n * Terms of use\\r\\n * Accessibility</td>\n",
       "      <td>When used to make sense of huge amounts of constantly changing data, smart catalog capabilities can make all the difference.</td>\n",
       "      <td>How smart catalogs can turn the big data flood into an ocean of opportunity</td>\n",
       "      <td>Live</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>Homepage Follow Sign in Get started Homepage * Home\\r\\n * Data Science Experience\\r\\n * Data Catalog\\r\\n * IBM Data Refinery\\r\\n * \\r\\n * Watson Data Platform\\r\\n * \\r\\n\\r\\nCarmen Ruppach Blocked Unblock Follow Following Offering Manager for Data Refinery on Watson Data Platform at IBM Nov 14\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nSELF-SERVICE DATA PREPARATION WITH IBM DATA REFINERY\\r\\nIf you are like most data scientists, you are probably spending a lot of time to\\r\\ncleanse, shape and prepare your data before you can actually start with the more\\r\\nenjoyable part of building and training machine learning models. As a data\\r\\nanalyst, you might face similar struggles to obtain data in a format you need to\\r\\nbuild your reports. In many companies data scientists and analysts need to wait\\r\\nfor their IT teams to get access to cleaned data in a consumable format.\\r\\n\\r\\nIBM Data Refinery addresses this issue. It provides an intuitive self-service\\r\\ndata preparation environment where you can quickly analyze, cleanse and prepare\\r\\ndata sets. It is a fully managed cloud service, available in open beta now.\\r\\n\\r\\nAnalyze and prepare your data\\r\\n\\r\\nWith IBM Data Refinery, you can interactively explore your data and use a wide\\r\\nrange of transformations to cleanse and transform data into the format you need\\r\\nfor analysis.\\r\\n\\r\\nYou can use a simple point-and-click interface for selecting and combining a\\r\\nwide range of built-in operations, such as filtering, replacing, and deriving\\r\\nvalues. It is also possible to quickly remove duplicates, split and concatenate\\r\\nvalues, and choose from a comprehensive list of text and math operations.\\r\\n\\r\\nInteractive data exploration and preparationIf you prefer to code, in IBM Data Refinery you can directly enter R commands\\r\\nvia R libraries such as dplyr. We provide code templates and in-context\\r\\ndocumentation to help you become productive with the R syntax more quickly.\\r\\n\\r\\nCode templates to help users with R syntaxIf you’re not satisfied with the shaping results, you can easily undo and change\\r\\noperations in the Steps side bar.\\r\\n\\r\\nThe interactive user interface works on a subset of the data to give you a\\r\\nfaster preview of the operations and results. Once you’re happy with the sample\\r\\noutput, you can apply the transformations on the entire data set and save all\\r\\ntransformation steps in a data flow. You can repeat the data flow later and\\r\\ntrack changes that were applied to your data. To accelerate the job execution,\\r\\nApache Spark is used as the execution engine.\\r\\n\\r\\nData profiling and visualization\\r\\n\\r\\nData shaping is an iterative and time-consuming process. In a traditional data\\r\\nscience workflow, you might use one tool to apply various transformations to\\r\\nyour data set, and then load the data into another tool to visualize and\\r\\nevaluate the results. Over many cycles, this continual tool hopping can become\\r\\nfrustrating.\\r\\n\\r\\nIBM Data Refinery soothes the pain by integrating both data transformations and\\r\\nvisualizations in a single interface, so you can move between views with a\\r\\nsimple click. You can use the Profile tab to view descriptive statistics of your\\r\\ndata columns in order to better understand the distribution of values. You can\\r\\ncontinue to apply transformations and the corresponding profile information\\r\\nadjusts automatically.\\r\\n\\r\\nOn the Visualization tab you can select a combination of columns to build charts\\r\\nusing Brunel (open source visualization library). IBM Data Refinery\\r\\nautomatically suggests appropriate plots and you can choose between 12\\r\\npre-defined chart types. You can adjust the appearance of the charts using\\r\\nBrunel syntax.\\r\\n\\r\\nConnecting to data wherever it resides\\r\\n\\r\\nIBM Data Refinery comes with a comprehensive set of 30 prebuilt data connectors\\r\\nso that you can set up connections to a wide range of commonly used on-premises\\r\\nand cloud data stores. You can connect to IBM as well as non-IBM services. If\\r\\nyour data service is hosted on IBM Cloud (formerly IBM Bluemix), you can\\r\\ndirectly access the data service instance from IBM Data Refinery.\\r\\n\\r\\nOnce you specify a connection and connect the data object to your data, you can\\r\\nstart to analyze and refine your data wherever it resides.\\r\\n\\r\\nTry out IBM Data Refinery! Sign up for free at: https://www.ibm.com/cloud/data-refinery\\r\\n\\r\\n * Data Science\\r\\n * Data Visualization\\r\\n * Data Analysis\\r\\n * Data Refinery\\r\\n\\r\\nOne clap, two clap, three clap, forty?By clapping more or less, you can signal to us which stories really stand out.\\r\\n\\r\\nBlocked Unblock Follow FollowingCARMEN RUPPACH\\r\\nOffering Manager for Data Refinery on Watson Data Platform at IBM\\r\\n\\r\\nFollowIBM WATSON DATA PLATFORM\\r\\nBuild smarter applications and quickly visualize, share, and gain insights\\r\\n\\r\\n * \\r\\n * \\r\\n * \\r\\n * \\r\\n\\r\\nNever miss a story from IBM Watson Data Platform , when you sign up for Medium. Learn more Never miss a story from IBM Watson Data Platform Get updates Get updates</td>\n",
       "      <td>If you are like most data scientists, you are probably spending a lot of time to cleanse, shape and prepare your data before you can actually start with the more enjoyable part of building and…</td>\n",
       "      <td>Self-service data preparation with IBM Data Refinery</td>\n",
       "      <td>Live</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Follow Sign in / Sign up Home About Insight Data Science Data Engineering Health Data AI 5 * Share\\r\\n * 5\\r\\n * \\r\\n * \\r\\n\\r\\nNever miss a story from Insight Data , when you sign up for Medium. Learn more Never miss a story from Insight Data Get updates Get updates Sebastien Dery Blocked Unblock Follow Following Master of Layers, Protector of the Graph, Wielder of Knowledge. #OpenScience\\r\\n#NoBullshit 2 days ago\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nGRAPH-BASED MACHINE LEARNING: PART 2\\r\\nCOMMUNITY DETECTION AT SCALE\\r\\nDuring the seven-week Insight Data Engineering Fellows Program recent grads and experienced software engineers learn the latest open source technologies by building a data platform to handle large, real-time datasets.\\r\\n\\r\\nSebastien Dery (now a Data Science Engineer at Yewno ) discusses his project on community detection on large datasets.\\r\\n\\r\\n\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\n#tltr : Graph-based machine learning is a powerful tool that can easily be merged\\r\\ninto ongoing efforts. This work reviews the feasibility of performing community\\r\\ndetection through a distributed implementation using GraphX. Embedded within the\\r\\nHadoop ecosystem, this modularity optimization approach allows the study of\\r\\nnetworks of unprecedented size. This change of scales, previously limited by\\r\\nRAM, opens exciting perspectives as the self modular structure of complex\\r\\nsystems have been shown to hold crucial information to understanding their\\r\\nnature.In my previous post , we discussed the foundation of community detection using modularity\\r\\noptimization. One major constraint however, is that your graph needs to fit in\\r\\nmemory. This quickly turns problematic as your number of nodes surpass billions, and\\r\\nthe number of edges becomes trillions.\\r\\n\\r\\nThankfully we can leverage distributed computation systems in order to solve this limitation. To do this we first need to define the state\\r\\nof a node so that it contains all the information needed during computation;\\r\\nthis will serve as a basic structure to pass around between the machines of our\\r\\ndistributed cluster.\\r\\n\\r\\n“Node” and “Vertex” are often used interchangeably in the literature. This class\\r\\nserves as structure for the nodes within the graph.Let’s also briefly review the process behind modularity optimization. This works\\r\\nby iteratively merging nodes that optimize for local modularity to yield a new, and smaller, graph. Repeat until satisfied.\\r\\n\\r\\nTwo great properties emerge from this approach\\r\\n\\r\\n 1. Locality : Each node requires knowledge from only its first-degree neighbors. This\\r\\n    means a minimal amount of data needs to be passed around between clusters.\\r\\n    This way, you don’t need to extensively jump from node to node across the\\r\\n    clusters in order to get the necessary information.\\r\\n 2. Independence : Each local computation occurs independently of the graph layout. Within\\r\\n    an iteration, every node can asynchronously send its information to its\\r\\n    neighbors without waiting for a blocking sequential set of operations to\\r\\n    happen.\\r\\n\\r\\nThese are important points to highlight as they make distributed computation a\\r\\nprime candidate for this memory problem. Turns out we can easily implement the\\r\\nlogic behind those properties using nothing but a simple iteration and a\\r\\ndeveloper-defined halting criteria. As previously discussed this can take many forms; here are a few ideas for brainstorming:\\r\\n\\r\\n * Scheduled based on a predefined number of iterations\\r\\n * Hits a specific total number of communities\\r\\n * Modularity gain between iteration is below a threshold\\r\\n\\r\\nSimple iteration over the two stage process of our optimization: transfer and\\r\\ncompress.Let’s dive into the initial step of transferring community between nodes.\\r\\nRemember that each node needs the information from its neighbors in order to\\r\\ncompute the gradient for local modularity.\\r\\n\\r\\nTRANSFER\\r\\nThe best way to do this at scale (when you don’t know where the information\\r\\nultimately is on disk) is by using distributed transactions (aka passing messages ). This type of architecture is ubiquitous in modern computer software; it is\\r\\nused as a way for the objects that make up a program to work with each other and\\r\\nas a way for objects and systems running on different computers (e.g., the\\r\\nInternet) to interact. In algorithms, you’ll often find it referenced under the\\r\\nname of Belief Propagation or simply message passing . In the context of community detection, each node sends a message to its\\r\\nneighbors with content along the lines of:\\r\\n\\r\\n“ Hey I’m your friendly neighbor Node 3 from Community 12 ”\\r\\n\\r\\nBy independently sending messages to their first degree neighbors, each node can\\r\\nretrieve all the information necessary for them to optimize for local\\r\\nmodularity. The content of each message can easily be tweaked thus adding\\r\\nconsiderable flexibility to your approach.If you’ve ever worked with graphs you’re likely to be very familiar with the\\r\\nconcepts of vertices and edges . Should we perform the message passing exhaustively you’d basically go through\\r\\neach vertex and send a message for each of its edges. This is not an\\r\\nintrinsically bad approach if that’s all you have to work with. Turns out that\\r\\nin the world of GraphX we have access to a third primitive for easy manipulation of our data: the triplet .\\r\\n\\r\\nThe three different types of view allowed within GraphX. Taken from AMPLab .The triplet logically joins the vertex and edge properties for a simplified and\\r\\nuseful view. Literally, the EdgeTriplet class extends the Edge class by simply adding the srcAttr and dstAttr members containing the source and destination properties respectively.\\r\\n\\r\\nBy reducing the triplets view, each node receives N messages corresponding to its N first degree neighbors. sendMsg and mergeMsg are both internal functions which perform the necessary aggregation for the\\r\\nlocal modularity update. Independently, and in parallel, each node waits for its\\r\\nturn to reduce all its messages into a coherent local sum of weighted edges, and\\r\\nmake a decision based on the local modularity deltaQ of each neighboring community.\\r\\n\\r\\nA few iterations later, the graph has converged to a local equilibrium (e.g. a\\r\\nminimal amount of nodes feel the need to change community). The algorithm can\\r\\nnow progress to the next step of compressing those communities into a compact\\r\\nrepresentation. This is done by creating a new graph with a new set of nodes\\r\\n(corresponding to each community) and edges being inferred from the edges during\\r\\nthe previous computation (e.g. average or sum of external edges).\\r\\n\\r\\nCOMPRESSION\\r\\nWhat function to choose really depends on the use case (e.g. averaging, total\\r\\nsum, maximum, softmax , etc. are all valid functions, although their respective advantages remains\\r\\nunclear in this particular scenario). When in doubt, let’s use a simple average.\\r\\nNote that additional information, say the internal coherence within a community,\\r\\ncan be propagated in a similar fashion to the condensed node and provide\\r\\nvaluable information.\\r\\n\\r\\nEffect of compressing community into single nodes at each iteration.Finally, here we have a fully functional procedure to perform modularity\\r\\noptimization on graphs of ridiculously large size, assuming we have enough\\r\\ncomputers to store all the information on disk.\\r\\n\\r\\nCAVEATS AND TIPS\\r\\nCOMPUTATION TIME\\r\\nNote that the number of meta-communities naturally decreases at each pass, and\\r\\nas a consequence most of the computing time is used in the first pass. This\\r\\nsuggests pre-ordering of the data would hold considerable benefit in terms of\\r\\ncomputation time.\\r\\n\\r\\nOptimizing for node locality at the cluster level means less transfer between\\r\\nmachines.CONVERGENCE\\r\\nThis approach does not necessarily converge to the optimal solution . To improve this, multiple iterations can increase confidence over the\\r\\nstructure of your data. Conveniently, this also offers a proxy for the\\r\\nprobability of two nodes belonging to the same community.\\r\\n\\r\\nLAYOUT\\r\\nTake into account graph connectivity when determining the usefulness of this\\r\\nstrategy. For example, for a completely connected and unweighted graph, the\\r\\noutput will be degenerate. Consider thresholding the graph beforehand to extract\\r\\na more sparse representation of your data.\\r\\n\\r\\nThe adequateness of modularity optimization is dependent on the connectivity\\r\\npattern of your graph. For example, in a lattice layout this algorithm will\\r\\nperform rather poorly. Modularity optimization doesn’t guarantee adequate\\r\\nclustering; thus obtaining a community at the end is not enough to conclusively\\r\\nsay a node decidedly belongs to that group (or even any group, for that matter).HIERARCHY\\r\\nThe iterative nature of this process offers a hierarchical view between\\r\\ncommunities of subsequent iteration. The intermediary step should therefore be\\r\\nsaved for further investigation as they likely yield valuable information on the\\r\\nstructural complexity of the data. This saving procedure is not covered in this\\r\\npost but should be trivial to introduce (insert configuration state into your\\r\\nfavorite database) between iteration.\\r\\n\\r\\nSUMMARY\\r\\nThis work reviewed the feasibility of performing community detection through a\\r\\ndistributed implementation using GraphX . Embedded within the Hadoop ecosystem , this modularity optimization approach allows the study of networks of\\r\\nunprecedented size. This change of scales, previously limited by RAM, opens\\r\\nexciting perspectives as the self modular structure of complex systems have been\\r\\nshown to hold crucial information to understanding their nature. This enables,\\r\\namong others, targeted marketing , market segmentation , gene clustering , topic modeling , etc.\\r\\n\\r\\nBeing an unsupervised learning technique and an initial starting point for a lot\\r\\nof analysis, the low barriers of entry make this approach applicable to a wide\\r\\nrange of datasets.\\r\\n\\r\\nDid I miss something crucial to get you up and running? Have something to add?\\r\\nWould love to hear your experience with this type of approach!\\r\\n\\r\\n\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nWant to learn Spark, machine learning with graphs, and other big data tools from\\r\\ntop data engineers in Silicon Valley or New York? The Insight Data Engineering Fellows Program is a free 7-week professional training where you can build cutting edge big\\r\\ndata platforms and transition to a career in data engineering.\\r\\n\\r\\nLearn more about the program and apply today .\\r\\n\\r\\nBig Data Data Science Machine Learning Insight Data Engineering Social Network 5 Blocked Unblock Follow FollowingSEBASTIEN DERY\\r\\nMaster of Layers, Protector of the Graph, Wielder of Knowledge. #OpenScience\\r\\n#NoBullshit\\r\\n\\r\\nFollowINSIGHT DATA\\r\\nInsight Fellows Program —Your bridge to careers in Data Science and Data\\r\\nEngineering.</td>\n",
       "      <td>During the seven-week Insight Data Engineering Fellows Program recent grads and experienced software engineers learn the latest open source technologies by building a data platform to handle large…</td>\n",
       "      <td>Graph-based machine learning</td>\n",
       "      <td>Live</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>Homepage Follow Sign in Get started * Home\\r\\n * Data Science Experience\\r\\n * Data Catalog\\r\\n * IBM Data Refinery\\r\\n * \\r\\n * Watson Data Platform\\r\\n * \\r\\n\\r\\nSourav Mazumder Blocked Unblock Follow Following Nov 27\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nUSING APACHE SPARK AS A PARALLEL PROCESSING FRAMEWORK FOR ACCESSING REST BASED\\r\\nDATA SERVICES\\r\\nToday’s world of data science leverages data from various sources. Commonly,\\r\\nthese sources are Hadoop File System, Enterprise Data Warehouse, Relational\\r\\nDatabase systems, Enterprise file systems, etc. The data from these sources are\\r\\naccessed in bulk using connectors specific to the underlying technology and\\r\\noptimized for accessing large volume of data.\\r\\n\\r\\nHowever, many a times, a data science exploration/modeling exercise also needs\\r\\nto access data from sources that support only API-based data access. These\\r\\nAPI-based data sources/data services can be of various types. For example:\\r\\n\\r\\n * Data services (external or internal), which can provide curated/enriched data\\r\\n   in record-by-record manner.\\r\\n * Validation services for verifying the data using an API. For example Address\\r\\n   validation.\\r\\n * Machine learning/AI services, which provide prediction, recommendations, and\\r\\n   insights based on a single input record.\\r\\n * Service from internal systems (like CRM, MDM, etc.) of the organization,\\r\\n   which supports data access through API only in record-by-record manner.\\r\\n * And many more …\\r\\n\\r\\nThese API-based data services are commonly implemented using REST architectural\\r\\nstyle ( https://en.wikipedia.org/wiki/Representational_state_transfer ) and are designed to be called for single item (or a limited set of items) per\\r\\nrequest. While this works well when the API needs to be called from an online\\r\\napplication, the approach breaks down in situations when the API has to be\\r\\ncalled in bulk. For example, during an online sign-up process an address\\r\\nvalidation API can be called for the particular address of the user. But, say in\\r\\na health care analytics application, where addresses of thousands of doctors,\\r\\nwhich already exist in a database or were obtained as part of a bulk load from\\r\\nan external source, have to be verified, this approach will not work. Because of\\r\\nthe “single item per request” design of the API, you’d have to call the API\\r\\nthousands of times.\\r\\n\\r\\nCalling data service APIs in sequence — Processing Time = (# of Records)*(API\\r\\nresponse time)\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nThe above pseudo code snippet shows how calling a target REST API service is\\r\\nhandled in a sequential manner. You must first load the list of parameter values\\r\\nfrom a file or table in the memory. Next run a loop. In the loop, the target\\r\\nREST API has to be called for each set of parameter values. From the response\\r\\nreturned by each call the output must be extracted. The output is typically\\r\\npopulated in a complex object like JSON, XML, etc. Next, the necessary part of\\r\\nthe output has to be added to a result array or collection. For that, you must\\r\\nknow the schema of the result beforehand so that you can process the result\\r\\naccordingly. Finally, you can filter, exploring, aggregating data from the\\r\\nresult array or collection. For all of these steps, you have to use\\r\\nlanguage-specific complex code.\\r\\n\\r\\nAlternatively, you could use a programming language-specific library related to\\r\\nmulti-processing/multi-threading that can parallelize the call to the API.\\r\\nHowever, with that approach the parallelization achieved from a single machine\\r\\nwould be minuscule — limited to the number of cores of the machine. Consider, a\\r\\ncase where someone is trying to get personality insights from tweets or Facebook\\r\\ncomments using a Natural Language Processing service. The tweets and comments\\r\\ncan be in tens to hundreds of thousands. So, using a single machine could take a\\r\\nnumber of hours to get the result. Hence, the approach should be to use a\\r\\ndistributed processing framework to make the API calls parallelized using\\r\\nmultiple cores of multiple machines with the least coding effort. Though it is\\r\\npossible to get distributed computing libraries or frameworks to achieve the\\r\\nsame in some programming languages like Java, C++ etc., they require a\\r\\nreasonable amount of coding and setup to achieve the same result. Achieving this\\r\\nin popular data science languages, like R or Python is actually more difficult\\r\\nas they are originally designed to run in single threaded/single machine\\r\\nenvironment.\\r\\n\\r\\nHere enters distributed computing frameworks like Apache Spark ( https://spark.apache.org/ ). REST APIs are inherently conducive to parallelization as each call to the\\r\\nAPI is completely independent of any other call to the same API. This fact, in\\r\\nconjunction with the parallel computing capability of Spark, can be leveraged to\\r\\ncreate a solution that solves the problem by delegating the API call to Spark’s\\r\\nparallel workers. Under this approach, one can package a specification for how\\r\\nto call the API along with the input data, and pass that to Spark to divide the\\r\\neffort among its workers (and tasks). The output can be assembled in set-level\\r\\nabstractions supported by Spark (like Dataframes or Datasets ) and passed back to the calling program. This approach not only helps you turn a\\r\\nsequential execution into a parallel one with the least coding effort, but also\\r\\nmakes it much easier to analyze and transform the returned result with an easier\\r\\ndata abstraction model to work with.\\r\\n\\r\\nThe performance benefit you get is tremendous in this approach. This turns a\\r\\nproblem that takes incremental time for computation (that increases linearly\\r\\nwith the number of records to process), to one that is much more efficient and\\r\\nscales linearly on a much lower slope — number of records to process divided by\\r\\nthe number of cores available to process them. Theoretically, one can make the\\r\\nprocess constant time by having enough cores to process ALL of the records at\\r\\nonce.\\r\\n\\r\\nTo enable the benefits of using Spark to call REST APIs, we are introducing a\\r\\ncustom data source for Spark, namely REST Data Source. It has been built by\\r\\nextending Spark’s Data Source API. This helps in delegating calls to the target\\r\\nREST API to a Spark level Task for each set of input parameter values/record.\\r\\nThis also enables the results from multiple API calls to be returned as one\\r\\nSpark Dataframe. The REST Data Source expects the input to be in the format of a\\r\\nSpark Temporary table. The results from the API calls are returned in a single\\r\\nDataframe of Rows including the input parameters in their corresponding column\\r\\nnames, as well as the output from the REST call in a structure matching that of\\r\\nthe target API’s response. You can check the schema of this Dataframe, and\\r\\naccess the result as necessary using Spark SQL.\\r\\n\\r\\nThe architecture of REST Data SourceThe above figure shows how REST Data Source works.\\r\\n\\r\\n 1. You first read different sets of parameter values (that have to be sent to\\r\\n    target REST API) from a file/table to a Spark Dataframe (say Input Data\\r\\n    Frame).\\r\\n 2. Then the Input Data Frame is passed to the REST Data Source.\\r\\n 3. The REST Data Source returns the results to another Dataframe, say Result\\r\\n    Data Frame.\\r\\n 4. Now you can use Spark SQL to explore, aggregate, and filter the result using\\r\\n    the Result Data Frame.\\r\\n\\r\\nREST Data Source internally calls the target REST API in parallel by executing\\r\\nmultiple tasks spawned by multiple worker processes running in different\\r\\nmachines. Each task is responsible for calling the target REST API Service for a\\r\\npart of the input (part of sets of parameter values).\\r\\n\\r\\nThe code snippet below demonstrates how to use REST Data Source in Python to get\\r\\nresults from Socrata Data Service (SODA API) for multiple sets of parameter\\r\\nvalues by calling the appropriate REST API in parallel.\\r\\n\\r\\nA sample code snippet showing use of REST Data Source to call REST API in\\r\\nparallelYou can configure the REST Data Source for different extent of parallelization.\\r\\nDepending on the volume of input sets of parameter values to be processed and\\r\\nthroughput supported by the target REST API server, you can pass the number of\\r\\npartitions to be used, and that can limit or extend the level of parallelization\\r\\nas needed. You can use this framework in all programming languages supported by\\r\\nSpark — Python, Scala, R, or Java — without any additional coding specific to\\r\\nthat programming language. Last, but not the least, you can also use this\\r\\nframework to ensure that the target API is called only once for a given set of\\r\\nparameter values. In this way you can avoid calling the target REST API multiple\\r\\ntimes for same set of parameter values. This is especially useful when you must\\r\\npay for the REST API being called or there is a limit per day for the same.\\r\\n\\r\\nSee \\r\\nhttps://github.com/sourav-mazumder/Data-Science-Extensions/tree/master/spark-datasource-rest for details of the REST Data Source.\\r\\n\\r\\nYou can also refer to this notebook \\r\\nhttps://dataplatform.ibm.com/analytics/notebooks/ae63f056-e267-443e-bfc0-b9331f51d68a/view?access_token=0ec63c6e031aa57d065a4e1c4b71733729db43b1490c331a44323cce28725b7d for an example of how to use the REST Data Source.\\r\\n\\r\\nSign up for a free Data Science Experience account ( https://datascience.ibm.com/ ) to try out this technique on a Spark cluster.\\r\\n\\r\\n * Big Data\\r\\n * Spark\\r\\n * Artificial Intelligence\\r\\n * Data Science\\r\\n * Rest Api\\r\\n\\r\\nOne clap, two clap, three clap, forty?By clapping more or less, you can signal to us which stories really stand out.\\r\\n\\r\\n21 Blocked Unblock Follow FollowingSOURAV MAZUMDER\\r\\nMedium member since Nov 2017 FollowIBM WATSON DATA PLATFORM\\r\\nBuild smarter applications and quickly visualize, share, and gain insights\\r\\n\\r\\n * 21\\r\\n * \\r\\n * \\r\\n * \\r\\n\\r\\nNever miss a story from IBM Watson Data Platform , when you sign up for Medium. Learn more Never miss a story from IBM Watson Data Platform Get updates Get updates</td>\n",
       "      <td>Today’s world of data science leverages data from various sources. Commonly, these sources are Hadoop File System, Enterprise Data Warehouse, Relational Database systems, Enterprise file systems, etc…</td>\n",
       "      <td>Using Apache Spark as a parallel processing framework for accessing REST based data services</td>\n",
       "      <td>Live</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>This video shows you how to construct queries to access the primary index through the API.Visit http://www.cloudant.com/sign-up to sign up for a free Cloudant account. Find more videos and tutorials in the Cloudant Learning Center: http://www.cloudant.com/learning-center</td>\n",
       "      <td>This video shows you how to construct queries to access the primary index through Cloudant's API.</td>\n",
       "      <td>Use the Primary Index</td>\n",
       "      <td>Live</td>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>Homepage Follow Sign in / Sign up Homepage * Home\\r\\n * Data Science Experience\\r\\n * Data Catalog\\r\\n * \\r\\n * Watson Data Platform\\r\\n * \\r\\n\\r\\nSusanna Tai Blocked Unblock Follow Following Offering Manager, Watson Data Platform | Data Catalog Oct 30\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nHOW SMART CATALOGS CAN TURN THE BIG DATA FLOOD INTO AN OCEAN OF OPPORTUNITY\\r\\nOne of the earliest documented catalogs was compiled at the great library of\\r\\nAlexandria in the third century BC, to help scholars manage, understand and\\r\\naccess its vast collection of literature. While that cataloging process\\r\\nrepresented a massive undertaking for the Alexandrian librarians, it pales in\\r\\ncomparison to the task of wrangling the volume and variety of data that modern\\r\\norganizations generate.\\r\\n\\r\\nNowadays, data is often described as an organization’s most valuable asset, but\\r\\nunless users can easily sift through data artifacts to find the information they\\r\\nneed, the value of that data may remain unrealized. Catalogs can solve this\\r\\nproblem by providing an indexed set of information about the organization’s\\r\\ndata, storing metadata that describes all assets and providing a reference to\\r\\nwhere they can be found or accessed.\\r\\n\\r\\nIt’s not just the size and complexity of the data that makes cataloging a tough\\r\\nchallenge: organizations also need to be able to perform increasingly\\r\\ncomplicated operations on that data at high speed, and even in real-time. As a\\r\\nresult, technology leaders must continually find better ways to solve today’s\\r\\nversion of the same cataloging challenges faced in Alexandria all those years\\r\\nago.\\r\\n\\r\\nENTER IBM\\r\\nIBM’s aim with Watson Data Platform is to make data accessible for anyone who uses it. An integral part of Watson\\r\\nData Platform will be a new intelligent asset catalog, IBM Data Catalog , a solution underpinned by a central repository of metadata describing all the\\r\\ninformation managed by the platform. Unlike many other catalog solutions on the\\r\\nmarket, the intelligent asset catalog will also offer full end-to-end\\r\\ncapabilities around data lifecycle and governance.\\r\\n\\r\\nBecause all the elements of Watson Data Platform can utilize the same catalog,\\r\\nusers will be able to share data with their colleagues more easily, regardless\\r\\nof what the data is, where it is stored, or how they intend to use it. In this\\r\\nway, the intelligent asset catalog will unlock the value held within that data\\r\\nacross user groups — helping organizations use this key asset to its full\\r\\npotential.\\r\\n\\r\\nBREAKING DOWN SILOS\\r\\nWith Watson Data Platform, data engineers, data scientists and other knowledge\\r\\nworkers throughout an enterprise can search for, share and leverage assets\\r\\n(including datasets, files, connections, notebooks, data flows, models and\\r\\nmore). Assets can be accessed using the Data Science Experience web user interface to analyze data,\\r\\n\\r\\nTo collaborate with colleagues, users can put assets into a Project that acts as\\r\\na shared sandbox where the whole team can access and utilize them. Once their\\r\\nwork is complete, they can submit any resulting content to the catalog for\\r\\nfurther reuse by other people and groups across the organization.\\r\\n\\r\\nRich metadata about each asset makes it easy for knowledge workers to find and\\r\\naccess relevant resources. Along with data files, the catalog can also include\\r\\nconnections to databases and other data sources, both on- and off-premises,\\r\\ngiving users a full 360-degree view to all information relevant to their\\r\\nbusiness, regardless of where or how it is stored.\\r\\n\\r\\nMANAGING DATA OVER TIME\\r\\nIt’s important to look at data as an evolving asset, rather than something that\\r\\nstays fixed over time. To help manage and trace this evolution, IBM Data Catalog\\r\\nwill keep a complete track of which users have added or modified each asset, so\\r\\nthat it is always clear who is responsible for any changes.\\r\\n\\r\\nSMART CATALOG CAPABILITIES FOR BIG DATA MANAGEMENT\\r\\nThe concept of catalogs may be simple, but when they’re being used to make sense\\r\\nof huge amounts of constantly changing data, smart capabilities make all the\\r\\ndifference. Here are some of the key smart catalog functionalities that we see\\r\\nas integral to tackling the big data challenge.\\r\\n\\r\\nDATA AND ASSET TYPE AWARENESS\\r\\nWhen a user chooses to preview or view an asset of a particular type, the data\\r\\nand asset type awareness feature will automatically launch the data in the best\\r\\nviewer — such as a shaper for a dataset, or a canvas for a data flow. This will\\r\\nsave time and boost productivity for users, optimizing discovery and making it\\r\\neasier to work with a variety of data types without switching tools.\\r\\n\\r\\nINTELLIGENT SEARCH AND EXPLORATION\\r\\nBy combining metadata, machine learning-based algorithms and user interaction\\r\\ndata, it is possible to fine-tune search results over time. Presenting users\\r\\nwith the most relevant data for their purpose will increase usefulness of the\\r\\nsolution the more it is used.\\r\\n\\r\\nSOCIAL CURATION\\r\\nEffective use of data throughout your organization is a two-way street: when\\r\\nusers discover a useful dataset, it’s important for them to help others find it\\r\\ntoo. Users can be encouraged to engage by taking advantage of curation features,\\r\\nenabling them to tag, rank and comment on assets within the catalog. By\\r\\naugmenting the metadata for each asset, this can help the catalog’s intelligent\\r\\nsearch algorithms guide users to the assets that are most relevant to their\\r\\nneeds.\\r\\n\\r\\nDATA LINEAGE\\r\\nIf data is incomplete or inaccurate, utilizing it can cause more problems than\\r\\nit solves. On the other hand, if data is accurate but users do not trust it,\\r\\nthey might not use it when it could make a real difference. In either scenario,\\r\\ndata lineage can help.\\r\\n\\r\\nData lineage captures the complete history of an asset in the catalog: from its\\r\\noriginal source, through all the operations and transformations it has\\r\\nundergone, to its current state. By exploring this lineage, users can be\\r\\nconfident they know where assets have come from, how those assets have evolved,\\r\\nand whether they can be trusted.\\r\\n\\r\\nMONITORING\\r\\nTaking a step back to a higher-level view, monitoring features will help users\\r\\nkeep track of overall usage of the catalog. Real-time dashboards help chief data\\r\\nofficers and other data professionals monitor how data is being used, and\\r\\nidentify ways to increase its usage in different areas of the organization.\\r\\n\\r\\nMETADATA DISCOVERY\\r\\nWe have already mentioned that data needs to be seen as an evolving asset —\\r\\nwhich means our catalogs must evolve with it. We plan to make it easy for users\\r\\nto augment assets with metadata manually; in the future, it may also be possible\\r\\nto integrate algorithms that can discover assets and capture their metadata\\r\\nautomatically.\\r\\n\\r\\nDATA GOVERNANCE\\r\\nFor many organizations, keeping data secure while ensuring access for authorized\\r\\nusers is one of the most significant information management challenges. You can\\r\\nmitigate this challenge with rule-based access control and automatic enforcement\\r\\nof data governance policies.\\r\\n\\r\\nAPIS\\r\\nFinally, the catalog will enable access to all these capabilities and more\\r\\nthrough a set of well-defined, RESTful APIs. IBM is committed to offering\\r\\napplication developers easy access to additional components of Watson Data Platform , such as persistence stores and data sets. We hope that they can use our\\r\\nservices to extend their current suite of data and analytics tools, to innovate\\r\\nand create smart new ways of working with the data.\\r\\n\\r\\nLearn more about IBM Data Catalog\\r\\n\\r\\n\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nWritten by Jay Limburn\\r\\nDistinguished Engineer and Offering Lead, Watson Data Platform\\r\\n\\r\\nOriginally published at www.ibm.com on August 1, 2017\\r\\n\\r\\n * Data Catalog\\r\\n * Data Management\\r\\n * Data Analytics\\r\\n * IBM\\r\\n * Ibm Watson\\r\\n\\r\\nA single golf clap? Or a long standing ovation?By clapping more or less, you can signal to us which stories really stand out.\\r\\n\\r\\nBlocked Unblock Follow FollowingSUSANNA TAI\\r\\nOffering Manager, Watson Data Platform | Data Catalog\\r\\n\\r\\nFollowIBM WATSON DATA PLATFORM\\r\\nBuild smarter applications and quickly visualize, share, and gain insights\\r\\n\\r\\n * \\r\\n * \\r\\n * \\r\\n * \\r\\n\\r\\nNever miss a story from IBM Watson Data Platform , when you sign up for Medium. Learn more Never miss a story from IBM Watson Data Platform Get updates Get updates</td>\n",
       "      <td>One of the earliest documented catalogs was compiled at the great library of Alexandria in the third century BC, to help scholars manage, understand and access its vast collection of literature…</td>\n",
       "      <td>How smart catalogs can turn the big data flood into an ocean of opportunity</td>\n",
       "      <td>Live</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>Homepage Follow Sign in Get started Homepage * Home\\r\\n * Data Science Experience\\r\\n * Data Catalog\\r\\n * IBM Data Refinery\\r\\n * \\r\\n * Watson Data Platform\\r\\n * \\r\\n\\r\\nSourav Mazumder Blocked Unblock Follow Following Nov 27\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nUSING APACHE SPARK AS A PARALLEL PROCESSING FRAMEWORK FOR ACCESSING REST BASED\\r\\nDATA SERVICES\\r\\nToday’s world of data science leverages data from various sources. Commonly,\\r\\nthese sources are Hadoop File System, Enterprise Data Warehouse, Relational\\r\\nDatabase systems, Enterprise file systems, etc. The data from these sources are\\r\\naccessed in bulk using connectors specific to the underlying technology and\\r\\noptimized for accessing large volume of data.\\r\\n\\r\\nHowever, many a times, a data science exploration/modeling exercise also needs\\r\\nto access data from sources that support only API-based data access. These\\r\\nAPI-based data sources/data services can be of various types. For example:\\r\\n\\r\\n * Data services (external or internal), which can provide curated/enriched data\\r\\n   in record-by-record manner.\\r\\n * Validation services for verifying the data using an API. For example Address\\r\\n   validation.\\r\\n * Machine learning/AI services, which provide prediction, recommendations, and\\r\\n   insights based on a single input record.\\r\\n * Service from internal systems (like CRM, MDM, etc.) of the organization,\\r\\n   which supports data access through API only in record-by-record manner.\\r\\n * And many more …\\r\\n\\r\\nThese API-based data services are commonly implemented using REST architectural\\r\\nstyle ( https://en.wikipedia.org/wiki/Representational_state_transfer ) and are designed to be called for single item (or a limited set of items) per\\r\\nrequest. While this works well when the API needs to be called from an online\\r\\napplication, the approach breaks down in situations when the API has to be\\r\\ncalled in bulk. For example, during an online sign-up process an address\\r\\nvalidation API can be called for the particular address of the user. But, say in\\r\\na health care analytics application, where addresses of thousands of doctors,\\r\\nwhich already exist in a database or were obtained as part of a bulk load from\\r\\nan external source, have to be verified, this approach will not work. Because of\\r\\nthe “single item per request” design of the API, you’d have to call the API\\r\\nthousands of times.\\r\\n\\r\\nCalling data service APIs in sequence — Processing Time = (# of Records)*(API\\r\\nresponse time)\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nThe above pseudo code snippet shows how calling a target REST API service is\\r\\nhandled in a sequential manner. You must first load the list of parameter values\\r\\nfrom a file or table in the memory. Next run a loop. In the loop, the target\\r\\nREST API has to be called for each set of parameter values. From the response\\r\\nreturned by each call the output must be extracted. The output is typically\\r\\npopulated in a complex object like JSON, XML, etc. Next, the necessary part of\\r\\nthe output has to be added to a result array or collection. For that, you must\\r\\nknow the schema of the result beforehand so that you can process the result\\r\\naccordingly. Finally, you can filter, exploring, aggregating data from the\\r\\nresult array or collection. For all of these steps, you have to use\\r\\nlanguage-specific complex code.\\r\\n\\r\\nAlternatively, you could use a programming language-specific library related to\\r\\nmulti-processing/multi-threading that can parallelize the call to the API.\\r\\nHowever, with that approach the parallelization achieved from a single machine\\r\\nwould be minuscule — limited to the number of cores of the machine. Consider, a\\r\\ncase where someone is trying to get personality insights from tweets or Facebook\\r\\ncomments using a Natural Language Processing service. The tweets and comments\\r\\ncan be in tens to hundreds of thousands. So, using a single machine could take a\\r\\nnumber of hours to get the result. Hence, the approach should be to use a\\r\\ndistributed processing framework to make the API calls parallelized using\\r\\nmultiple cores of multiple machines with the least coding effort. Though it is\\r\\npossible to get distributed computing libraries or frameworks to achieve the\\r\\nsame in some programming languages like Java, C++ etc., they require a\\r\\nreasonable amount of coding and setup to achieve the same result. Achieving this\\r\\nin popular data science languages, like R or Python is actually more difficult\\r\\nas they are originally designed to run in single threaded/single machine\\r\\nenvironment.\\r\\n\\r\\nHere enters distributed computing frameworks like Apache Spark ( https://spark.apache.org/ ). REST APIs are inherently conducive to parallelization as each call to the\\r\\nAPI is completely independent of any other call to the same API. This fact, in\\r\\nconjunction with the parallel computing capability of Spark, can be leveraged to\\r\\ncreate a solution that solves the problem by delegating the API call to Spark’s\\r\\nparallel workers. Under this approach, one can package a specification for how\\r\\nto call the API along with the input data, and pass that to Spark to divide the\\r\\neffort among its workers (and tasks). The output can be assembled in set-level\\r\\nabstractions supported by Spark (like dataframes or data sets ) and passed back to the calling program. This approach not only helps you turn a\\r\\nsequential execution into a parallel one with the least coding effort, but also\\r\\nmakes it much easier to analyze and transform the returned result with an easier\\r\\ndata abstraction model to work with.\\r\\n\\r\\nThe performance benefit you gets is tremendous in this approach. This turns a\\r\\nproblem that takes incremental time for computation (that increases linearly\\r\\nwith the number of records to process), to one that is much more efficient and\\r\\nscales linearly on a much lower slope — number of records to process divided by\\r\\nthe number of cores available to process them. Theoretically, one can make the\\r\\nprocess constant time by having enough cores to process ALL of the records at\\r\\nonce.\\r\\n\\r\\nTo enable the benefits of using Spark to call REST APIs, we are introducing a\\r\\ncustom data source for Spark, namely REST Data Source. It has been built by\\r\\nextending Spark’s Data Source API. This helps in delegating calls to the target\\r\\nREST API to a Spark level Task for each set of input parameter values/record.\\r\\nThis also enables the results from multiple API calls to be returned as one\\r\\nSpark Dataframe. The REST Data Source expects the input to be in the format of a\\r\\nSpark Temporary table. The results from the API calls are returned in a single\\r\\nDataframe of Rows including the input parameters in their corresponding column\\r\\nnames, as well as the output from the REST call in a structure matching that of\\r\\nthe target API’s response. You can check the schema of this Dataframe, and\\r\\naccess the result as necessary using Spark SQL.\\r\\n\\r\\nThe architecture of REST Data SourceThe above figure shows how REST Data Source works.\\r\\n\\r\\n 1. You first read different sets of parameter values (that have to be sent to\\r\\n    target REST API) from a file/table to a Spark Dataframe (say Input Data\\r\\n    Frame).\\r\\n 2. Then the Input Data Frame is passed to the REST Data Source.\\r\\n 3. The REST Data Source returns the results to another Dataframe, say Result\\r\\n    Data Frame.\\r\\n 4. Now you can use Spark SQL to explore, aggregate, and filter the result using\\r\\n    the Result Data Frame.\\r\\n\\r\\nREST Data Source internally calls the target REST API in parallel by executing\\r\\nmultiple tasks spawned by multiple worker processes running in different\\r\\nmachines. Each task is responsible for calling the target REST API Service for a\\r\\npart of the input (part of sets of parameter values).\\r\\n\\r\\nThe code snippet below demonstrates how to use REST Data Source in Python to get\\r\\nresults from Socrata Data Service (SODA API) for multiple sets of parameter\\r\\nvalues by calling the appropriate REST API in parallel.\\r\\n\\r\\nA sample code snippet showing use of REST Data Source to call REST API in\\r\\nparallelYou can configure the REST Data Source for different levels of parallelization.\\r\\nDepending on the volume of input sets of parameter values to be processed and\\r\\nthroughput supported by the target REST API server, you can pass the number of\\r\\npartitions to be used, and that can limit or extend the level of parallelization\\r\\nas needed. You can use this framework in all programming languages supported by\\r\\nSpark — Python, Scala, R, or Java — without any additional coding specific to\\r\\nthat programming language. Last, but not the least, you can also use this\\r\\nframework to ensure that the target API is called only once for a given set of\\r\\nparameter values. In this way you can avoid calling the target REST API multiple\\r\\ntimes for same set of parameter values. This is especially useful when you must\\r\\npay for the REST API being called or there is a limit per day for the same.\\r\\n\\r\\nSee \\r\\nhttps://github.com/sourav-mazumder/Data-Science-Extensions/tree/master/spark-datasource-rest for details of the REST Data Source. Also see this notebook \\r\\nhttps://dataplatform.ibm.com/analytics/notebooks/ae63f056-e267-443e-bfc0-b9331f51d68a/view?access_token=0ec63c6e031aa57d065a4e1c4b71733729db43b1490c331a44323cce28725b7d for an example of how to use the REST Data Source.\\r\\n\\r\\n * Big Data\\r\\n * Spark\\r\\n * Artificial Intelligence\\r\\n * Data Science\\r\\n * Rest Api\\r\\n\\r\\nOne clap, two clap, three clap, forty?By clapping more or less, you can signal to us which stories really stand out.\\r\\n\\r\\n9 Blocked Unblock Follow FollowingSOURAV MAZUMDER\\r\\nMedium member since Nov 2017 FollowIBM WATSON DATA PLATFORM\\r\\nBuild smarter applications and quickly visualize, share, and gain insights\\r\\n\\r\\n * 9\\r\\n * \\r\\n * \\r\\n * \\r\\n\\r\\nNever miss a story from IBM Watson Data Platform , when you sign up for Medium. Learn more Never miss a story from IBM Watson Data Platform Get updates Get updates</td>\n",
       "      <td>Today’s world of data science leverages data from various sources. Commonly, these sources are Hadoop File System, Enterprise Data Warehouse, Relational Database systems, Enterprise file systems, etc…</td>\n",
       "      <td>Using Apache Spark as a parallel processing framework for accessing REST based data services</td>\n",
       "      <td>Live</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>This video shows you how to construct queries to access the primary index through the API.Visit http://www.cloudant.com/sign-up to sign up for a free Cloudant account.</td>\n",
       "      <td>This video shows you how to construct queries to access the primary index through the API</td>\n",
       "      <td>Use the Primary Index</td>\n",
       "      <td>Live</td>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>Homepage Follow Sign in Get started * Home\\r\\n * Data Science Experience\\r\\n * Data Catalog\\r\\n * IBM Data Refinery\\r\\n * \\r\\n * Watson Data Platform\\r\\n * \\r\\n\\r\\nCarmen Ruppach Blocked Unblock Follow Following Offering Manager for Data Refinery on Watson Data Platform at IBM Nov 14, 2017\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nSELF-SERVICE DATA PREPARATION WITH IBM DATA REFINERY\\r\\nIf you are like most data scientists, you are probably spending a lot of time to\\r\\ncleanse, shape and prepare your data before you can actually start with the more\\r\\nenjoyable part of building and training machine learning models. As a data\\r\\nanalyst, you might face similar struggles to obtain data in a format you need to\\r\\nbuild your reports. In many companies data scientists and analysts need to wait\\r\\nfor their IT teams to get access to cleaned data in a consumable format.\\r\\n\\r\\nIBM Data Refinery addresses this issue. It provides an intuitive self-service\\r\\ndata preparation environment where you can quickly analyze, cleanse and prepare\\r\\ndata sets. It is a fully managed cloud service, available in open beta now.\\r\\n\\r\\nAnalyze and prepare your data\\r\\n\\r\\nWith IBM Data Refinery, you can interactively explore your data and use a wide\\r\\nrange of transformations to cleanse and transform data into the format you need\\r\\nfor analysis.\\r\\n\\r\\nYou can use a simple point-and-click interface for selecting and combining a\\r\\nwide range of built-in operations, such as filtering, replacing, and deriving\\r\\nvalues. It is also possible to quickly remove duplicates, split and concatenate\\r\\nvalues, and choose from a comprehensive list of text and math operations.\\r\\n\\r\\nInteractive data exploration and preparationIf you prefer to code, in IBM Data Refinery you can directly enter R commands\\r\\nvia R libraries such as dplyr. We provide code templates and in-context\\r\\ndocumentation to help you become productive with the R syntax more quickly.\\r\\n\\r\\nCode templates to help users with R syntaxIf you’re not satisfied with the shaping results, you can easily undo and change\\r\\noperations in the Steps side bar.\\r\\n\\r\\nThe interactive user interface works on a subset of the data to give you a\\r\\nfaster preview of the operations and results. Once you’re happy with the sample\\r\\noutput, you can apply the transformations on the entire data set and save all\\r\\ntransformation steps in a data flow. You can repeat the data flow later and\\r\\ntrack changes that were applied to your data. To accelerate the job execution,\\r\\nApache Spark is used as the execution engine.\\r\\n\\r\\nProfile and visualize data\\r\\n\\r\\nData shaping is an iterative and time-consuming process. In a traditional data\\r\\nscience workflow, you might use one tool to apply various transformations to\\r\\nyour data set, and then load the data into another tool to visualize and\\r\\nevaluate the results. Over many cycles, this continual tool hopping can become\\r\\nfrustrating.\\r\\n\\r\\nIBM Data Refinery soothes the pain by integrating both data transformations and\\r\\nvisualizations in a single interface, so you can move between views with a\\r\\nsimple click. You can use the Profile tab to view descriptive statistics of your\\r\\ndata columns in order to better understand the distribution of values. You can\\r\\ncontinue to apply transformations and the corresponding profile information\\r\\nadjusts automatically.\\r\\n\\r\\nOn the Visualization tab you can select a combination of columns to build charts\\r\\nusing Brunel (open source visualization library). IBM Data Refinery\\r\\nautomatically suggests appropriate plots and you can choose between 12\\r\\npre-defined chart types. You can adjust the appearance of the charts using\\r\\nBrunel syntax.\\r\\n\\r\\nConnect to your data wherever it resides\\r\\n\\r\\nIBM Data Refinery comes with a comprehensive set of 30 prebuilt data connectors\\r\\nso that you can set up connections to a wide range of commonly used on-premises\\r\\nand cloud data stores. You can connect to IBM as well as non-IBM services. If\\r\\nyour data service is hosted on IBM Cloud (formerly IBM Bluemix), you can\\r\\ndirectly access the data service instance from IBM Data Refinery.\\r\\n\\r\\nOnce you specify a connection and connect the data object to your data, you can\\r\\nstart to analyze and refine your data wherever it resides.\\r\\n\\r\\nTry out IBM Data Refinery! Sign up for free at: https://www.ibm.com/cloud/data-refinery\\r\\n\\r\\n * Data Science\\r\\n * Data Visualization\\r\\n * Data Analysis\\r\\n * Data Refinery\\r\\n\\r\\nOne clap, two clap, three clap, forty?By clapping more or less, you can signal to us which stories really stand out.\\r\\n\\r\\n27 Blocked Unblock Follow FollowingCARMEN RUPPACH\\r\\nOffering Manager for Data Refinery on Watson Data Platform at IBM\\r\\n\\r\\nFollowIBM WATSON DATA\\r\\nBuild smarter applications and quickly visualize, share, and gain insights\\r\\n\\r\\n * 27\\r\\n * \\r\\n * \\r\\n * \\r\\n\\r\\nNever miss a story from IBM Watson Data , when you sign up for Medium. Learn more Never miss a story from IBM Watson Data Get updates Get updates</td>\n",
       "      <td>If you are like most data scientists, you are probably spending a lot of time to cleanse, shape and prepare your data before you can actually start with the more enjoyable part of building and…</td>\n",
       "      <td>Self-service data preparation with IBM Data Refinery</td>\n",
       "      <td>Live</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               doc_body  \\\n",
       "50                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Follow Sign in / Sign up Home About Insight Data Science Data Engineering Health Data AI Never miss a story from Insight Data , when you sign up for Medium. Learn more Never miss a story from Insight Data Get updates Get updates Sebastien Dery Blocked Unblock Follow Following I don’t know what I’m doing; but then neither do you so it’s all good. Master\\r\\nof Layers, Protector of the Graph, Wielder of Knowledge. #OpenScience Oct 16\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nGRAPH-BASED MACHINE LEARNING: PART I\\r\\nCOMMUNITY DETECTION AT SCALE\\r\\nDuring the seven-week Insight Data Engineering Fellows Program recent grads and experienced software engineers learn the latest open source technologies by building a data platform to handle large, real-time datasets.\\r\\n\\r\\nSebastien Dery (now a Data Science Engineer at Yewno ) discusses his project on community detection on large datasets.\\r\\n\\r\\n\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\n#tltr : Graph-based machine learning is a powerful tool that can easily be merged\\r\\ninto ongoing efforts. Using modularity as an optimization goal provides a\\r\\nprincipled approach to community detection. Local modularity increment can be\\r\\ntweaked to your own dataset to reflect interpretable quantities. This is useful\\r\\nin many scenarios, making it a prime candidate for your everyday toolbox.Many important problems can be represented and studied using graphs — social\\r\\nnetworks, interacting bacterias, brain network modules, hierarchical image\\r\\nclustering and many more.\\r\\n\\r\\nIf we accept graphs as a basic means of structuring and analyzing data about the\\r\\nworld, we shouldn’t be surprised to see them being widely used in Machine\\r\\nLearning as a powerful tool that can enable intuitive properties and power a lot\\r\\nof useful features. Graph-based machine learning is destined to become a\\r\\nresilient piece of logic, transcending a lot of other techniques. See more in\\r\\nthis recent blog post from Google Research\\r\\n\\r\\nThis post explores the tendencies of nodes in a graph to spontaneously form\\r\\nclusters of internally dense linkage (hereby termed “community”); a remarkable\\r\\nand almost universal property of biological networks. This is particularly\\r\\ninteresting knowing that a lot of information can be extrapolated from a node’s\\r\\nneighbor (e.g. think recommendation system, respondent analysis, portfolio\\r\\nclustering). So how can we extract this kind of information?\\r\\n\\r\\nCommunity Detection aims to partition a graph into clusters of densely connected nodes, with the\\r\\nnodes belonging to different communities being only sparsely connected.\\r\\n\\r\\nGraph analytics concerns itself with the study of nodes (depicted as disks) and\\r\\ntheir interactions with other nodes (lines). Community Detection aims to\\r\\nclassify nodes by their “clique”.“ Is it the same as clustering? ”\\r\\n\\r\\n * Short answer: Yes .\\r\\n * Long answer: For all intents and purposes, yes it is .\\r\\n\\r\\nSo why shouldn’t I just use my good old K-Means? You absolutely should, unless\\r\\nyour data and requirements don’t work well with that algorithm’s assumptions,\\r\\nnamely:\\r\\n\\r\\n 1. K number of clusters\\r\\n 2. Sum of Squared Error (SSE) as the right optimization cost\\r\\n 3. All variable have the same variance\\r\\n 4. The variance of the distribution of each attribute is spherical\\r\\n\\r\\nFor a more in-depth look click here .\\r\\n\\r\\nFirst off, let’s drop this idea of SSE and choose a more relevant notation of\\r\\nwhat we’re looking for: the internal versus external relationships between nodes\\r\\nof a community. Let’s discuss the notion of modularity.\\r\\n\\r\\nwhere: nc is the number of communities; lc number of edges within; dc sum of vertex degree; and m the size of the graph (number of edges). We will be using this equation as a\\r\\nglobal metric of goodness during our search for an optimal partitioning. In a nutshell: Higher score will be given to a community configuration offering higher\\r\\ninternal versus external linkage.So all I have to do is optimize this and we’re done, right?\\r\\n\\r\\nA major problem in the theoretical formulation of this optimization scheme is\\r\\nthat we need an all-knowing knowledge of the graph topology (geometric\\r\\nproperties and spatial relations). This is rather, let’s say, intractable . Apparently we can’t do any better than to try all possible subsets of the\\r\\nvertices and check to see which, if any, form communities. The problem of finding the largest clique in a graph is thus said to be NP-hard .\\r\\n\\r\\nHowever, several algorithms have been proposed over the years to find reasonably good partitions in reasonable amounts of time, each with its own particular flavor. This post focuses on a\\r\\nspecific family of algorithms called agglomerative . These algorithms work very simply by collecting (or merging) nodes together.\\r\\nThis has a lot of advantages since it typically only requires a knowledge of first degree neighbors and small incremental merging steps , to bring the global solution towards stepwise equilibriums.\\r\\n\\r\\nYou might point out that the modularity metric gives a global perspective on the\\r\\nstate of the graph and not a local indicator. So, how does this translate to the\\r\\nsmall local increment that I just mentioned?\\r\\n\\r\\nThe basic approach does indeed consists of iteratively merging nodes that\\r\\noptimize a local modularity so let’s go ahead and define that as well:\\r\\n\\r\\nwhere ∑ in is the sum of weighted links inside C, ∑ tot sum of weighted links incident to nodes in C, k i sum of weighted links incident to node i , k i, in sum of weighted links going from i to nodes in C and m a normalizing factor as the sum of weighted links for the whole graph. (Sorry, Medium doesn’t allow subscript and superscript)This is part of the magic for me as this local optimization function can easily\\r\\nbe translated to an interpretable metric within the domain of your graph. For\\r\\nexample,\\r\\n\\r\\n * Community Strength: Sum of Weighted Link within a community.\\r\\n * Community Popularity: Sum of Weighted Link incident to nodes within a specific community.\\r\\n * Node Belonging: Sum of Weighted Link from a node to a community.\\r\\n\\r\\nThere’s also nothing stopping from adding more terms to the previous equation\\r\\nthat are specific to your dataset. In other words, the weighted links can be a\\r\\nfunction of the type of nodes computed on-the-fly (useful if you’re dealing with\\r\\na multidimensional graph with various types of relationships and nodes).\\r\\n\\r\\nExample of converging iterations before the Compress phaseNow that we’re all set with our optimization function and local cost, the\\r\\ntypical agglomerative strategy consists of two iterative phases ( Transfer and Compress ). Assuming a weighted network of N nodes, we begin by assigning a different community to each node of the network.\\r\\n\\r\\n 1. Transfer : For each node i, consider its neighbors j and evaluate the gain in modularity by swapping c_i for c_j . The greedy process transfers the node into the neighboring community,\\r\\n    maximizing the gain in modularity (assuming the gain is positive). If no\\r\\n    positive gain is possible, the node i stays in its original community. This process is applied to all nodes until\\r\\n    no individual move can improve the modularity (i.e. a local maxima of\\r\\n    modularity is attained — a state of equilibrium).\\r\\n 2. Compress : building a new network whose nodes are the communities found during the\\r\\n    first phase; a process termed compression (see Figure below). To do so, edge weights between communities are computed\\r\\n    as the sum of the internal edges between nodes in the corresponding two\\r\\n    communities.\\r\\n\\r\\nAgglomerative process: Phase one converges to a local equilibrium of local\\r\\nmodularity. Phase two consist in compressing the graph for the next iteration,\\r\\nthus reducing the number of nodes to consider and incidentally computation time\\r\\nas well.Now the tricky part: as this is a greedy algorithm , you’ll have to define a stopping criteria based on your case scenario and the\\r\\ndata at hand.\\r\\n\\r\\nHow to define this criteria? It can be a lot of things: a maximum number of iterations, a minimum modularity\\r\\ngain during the transfer phase, or any other relevant piece of information\\r\\nrelated to your data that would inform you that it needs to stop.\\r\\n\\r\\nStill not sure when to stop ? Just make sure you save every intermediate step of the iterative process\\r\\nsomewhere, let the optimization run until there’s only one node left in your\\r\\ngraph, and then look back at your data! The interesting part is that by keeping\\r\\ntrack of each step, you also profit from a hierarchical view of your communities\\r\\nwhich can be further explored and leveraged.\\r\\n\\r\\nIn a follow up post, I will discuss how we can achieve this on a distributed\\r\\nsystem using Spark GraphX , part of my project while at the Insight Data Engineering Fellows Program .\\r\\n\\r\\n[0803.0476] Fast unfolding of communities in large networks Abstract: We propose\\r\\na simple method to extract the community structure of large networks. Our method\\r\\nis a heuristic… arxiv.org\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nWant to learn Spark, machine learning with graphs, and other big data tools from\\r\\ntop data engineers in Silicon Valley or New York? The Insight Data Engineering Fellows Program is a free 7-week professional training where you can build cutting edge big\\r\\ndata platforms and transition to a career in data engineering at top teams like\\r\\nFacebook, Uber, Slack and Squarespace.\\r\\n\\r\\nLearn more about the program and apply today .\\r\\n\\r\\nBig Data Data Science Machine Learning Social Network Analysis Insight Data Engineering 4 Blocked Unblock Follow FollowingSEBASTIEN DERY\\r\\nI don’t know what I’m doing; but then neither do you so it’s all good. Master of\\r\\nLayers, Protector of the Graph, Wielder of Knowledge. #OpenScience\\r\\n\\r\\nFollowINSIGHT DATA\\r\\nInsight Fellows Program —Your bridge to careers in Data Science and Data\\r\\nEngineering.   \n",
       "221  * United States\\r\\n\\r\\nIBM® * Site map\\r\\n\\r\\nSearch within Bluemix Blog Bluemix Blog * About Bluemix * What is Bluemix\\r\\n    * Getting Started\\r\\n    * Case Studies\\r\\n    * Hybrid Architecture\\r\\n    * Open Source\\r\\n    * Trust, Security, Privacy\\r\\n    * Data Centers\\r\\n    * Our Network\\r\\n    * Automation\\r\\n    * Architecture Center\\r\\n   \\r\\n   \\r\\n * Products * Compute Infrastructure\\r\\n    * Compute Services\\r\\n    * Hybrid Deployments\\r\\n    * Watson\\r\\n    * Internet of Things\\r\\n    * Mobile\\r\\n    * DevOps\\r\\n    * Data Analytics\\r\\n    * Network\\r\\n    * Open Source\\r\\n    * Storage\\r\\n    * Security\\r\\n   \\r\\n   \\r\\n * Services * Bluemix Services\\r\\n    * Garage\\r\\n   \\r\\n   \\r\\n * Pricing\\r\\n * Support * Support\\r\\n    * Contact Us\\r\\n    * Resources\\r\\n    * Docs\\r\\n   \\r\\n   \\r\\n * Blog * How-tos\\r\\n    * Trending\\r\\n    * What's New\\r\\n    * Events\\r\\n   \\r\\n   \\r\\n * Partners * Partners\\r\\n    * Become a Partner\\r\\n    * Find a Partner\\r\\n   \\r\\n   \\r\\n * Sign up\\r\\n\\r\\nDATA ANALYTICSHOW SMART CATALOGS CAN TURN THE BIG DATA FLOOD INTO AN OCEAN OF OPPORTUNITY\\r\\nAugust 1, 2017 | Written by: Jay Limburn\\r\\n\\r\\nCategorized: Data Analytics\\r\\n\\r\\nShare this post:\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nOne of the earliest documented catalogs was compiled at the great library of\\r\\nAlexandria in the third century BC, to help scholars manage, understand and\\r\\naccess its vast collection of literature. While that cataloging process\\r\\nrepresented a massive undertaking for the Alexandrian librarians, it pales in\\r\\ncomparison to the task of wrangling the volume and variety of data that modern\\r\\norganizations generate.\\r\\n\\r\\nNowadays, data is often described as an organization’s most valuable asset, but\\r\\nunless users can easily sift through data artifacts to find the information they\\r\\nneed, the value of that data may remain unrealized. Catalogs can solve this\\r\\nproblem by providing an indexed set of information about the organization’s\\r\\ndata, storing metadata that describes all assets and providing a reference to\\r\\nwhere they can be found or accessed.\\r\\n\\r\\nIt’s not just the size and complexity of the data that makes cataloging a tough\\r\\nchallenge: organizations also need to be able to perform increasingly\\r\\ncomplicated operations on that data at high speed, and even in real-time. As a\\r\\nresult, technology leaders must continually find better ways to solve today’s\\r\\nversion of the same cataloging challenges faced in Alexandria all those years\\r\\nago.\\r\\n\\r\\n\\r\\n\\r\\nENTER IBM\\r\\nIBM’s aim with Watson Data Platform is to make data accessible for anyone who uses it. An integral part of Watson\\r\\nData Platform will be a new intelligent asset catalog, IBM Data Manager, a\\r\\nsolution underpinned by a central repository of metadata describing all the\\r\\ninformation managed by the platform. Unlike many other catalog solutions on the\\r\\nmarket, the intelligent asset catalog will also offer full end-to-end\\r\\ncapabilities around data lifecycle and governance.\\r\\n\\r\\nBecause all the elements of Watson Data Platform can utilize the same catalog,\\r\\nusers will be able to share data with their colleagues more easily, regardless\\r\\nof what the data is, where it is stored, or how they intend to use it. In this\\r\\nway, the intelligent asset catalog will unlock the value held within that data\\r\\nacross user groups—helping organizations use this key asset to its full\\r\\npotential.\\r\\n\\r\\n\\r\\n\\r\\nBREAKING DOWN SILOS\\r\\nWith Watson Data Platform, data engineers, data scientists and other knowledge\\r\\nworkers throughout an enterprise can search for, share and leverage assets\\r\\n(including datasets, files, connections, notebooks, data flows, models and\\r\\nmore). Assets can be accessed using the Data Science Experience web user interface to analyze data,\\r\\n\\r\\nTo collaborate with colleagues, users can put assets into a Project that acts as\\r\\na shared sandbox where the whole team can access and utilize them. Once their\\r\\nwork is complete, they can submit any resulting content to the catalog for\\r\\nfurther reuse by other people and groups across the organization.\\r\\n\\r\\nRich metadata about each asset makes it easy for knowledge workers to find and\\r\\naccess relevant resources. Along with data files, the catalog can also include\\r\\nconnections to databases and other data sources, both on- and off-premises,\\r\\ngiving users a full 360-degree view to all information relevant to their\\r\\nbusiness, regardless of where or how it is stored.\\r\\n\\r\\n\\r\\n\\r\\nMANAGING DATA OVER TIME\\r\\nIt’s important to look at data as an evolving asset, rather than something that\\r\\nstays fixed over time. To help manage and trace this evolution, IBM Data Manager\\r\\nwill keep a complete track of which users have added or modified each asset, so\\r\\nthat it is always clear who is responsible for any changes.\\r\\n\\r\\n\\r\\n\\r\\nSMART CATALOG CAPABILITIES FOR BIG DATA MANAGEMENT\\r\\nThe concept of catalogs may be simple, but when they’re being used to make sense\\r\\nof huge amounts of constantly changing data, smart capabilities make all the\\r\\ndifference. Here are some of the key smart catalog functionalities that we see\\r\\nas integral to tackling the big data challenge, and that we will be aiming to\\r\\ninclude in upcoming releases of IBM Data Manager.\\r\\n\\r\\n\\r\\n\\r\\nDATA AND ASSET TYPE AWARENESS\\r\\nWhen a user chooses to preview or view an asset of a particular type, the data\\r\\nand asset type awareness feature will automatically launch the data in the best\\r\\nviewer—such as a shaper for a dataset, or a canvas for a data flow. This will\\r\\nsave time and boost productivity for users, optimizing discovery and making it\\r\\neasier to work with a variety of data types without switching tools.\\r\\n\\r\\n\\r\\n\\r\\nINTELLIGENT SEARCH AND EXPLORATION\\r\\nBy combining metadata, machine learning-based algorithms and user interaction\\r\\ndata, it is possible to fine-tune search results over time. Presenting users\\r\\nwith the most relevant data for their purpose will increase usefulness of the\\r\\nsolution the more it is used.\\r\\n\\r\\n\\r\\n\\r\\nSOCIAL CURATION\\r\\nEffective use of data throughout your organization is a two-way street: when\\r\\nusers discover a useful dataset, it’s important for them to help others find it\\r\\ntoo. Users can be encouraged to engage by taking advantage of curation features,\\r\\nenabling them to tag, rank and comment on assets within the catalog. By\\r\\naugmenting the metadata for each asset, this can help the catalog’s intelligent\\r\\nsearch algorithms guide users to the assets that are most relevant to their\\r\\nneeds.\\r\\n\\r\\n\\r\\n\\r\\nDATA LINEAGE\\r\\nIf data is incomplete or inaccurate, utilizing it can cause more problems than\\r\\nit solves. On the other hand, if data is accurate but users do not trust it,\\r\\nthey might not use it when it could make a real difference. In either scenario,\\r\\ndata lineage can help.\\r\\n\\r\\nData lineage captures the complete history of an asset in the catalog: from its\\r\\noriginal source, through all the operations and transformations it has\\r\\nundergone, to its current state. By exploring this lineage, users can be\\r\\nconfident they know where assets have come from, how those assets have evolved,\\r\\nand whether they can be trusted.\\r\\n\\r\\n\\r\\n\\r\\nMONITORING\\r\\nTaking a step back to a higher-level view, monitoring features will help users\\r\\nkeep track of overall usage of the catalog. Real-time dashboards help chief data\\r\\nofficers and other data professionals monitor how data is being used, and\\r\\nidentify ways to increase its usage in different areas of the organization.\\r\\n\\r\\n\\r\\n\\r\\nMETADATA DISCOVERY\\r\\nWe have already mentioned that data needs to be seen as an evolving asset—which\\r\\nmeans our catalogs must evolve with it. We plan to make it easy for users to\\r\\naugment assets with metadata manually; in the future, it may also be possible to\\r\\nintegrate algorithms that can discover assets and capture their metadata\\r\\nautomatically.\\r\\n\\r\\n\\r\\n\\r\\nDATA GOVERNANCE\\r\\nFor many organizations, keeping data secure while ensuring access for authorized\\r\\nusers is one of the most significant information management challenges. You can\\r\\nmitigate this challenge with rule-based access control and automatic enforcement\\r\\nof data governance policies.\\r\\n\\r\\n\\r\\n\\r\\nAPIS\\r\\nFinally, the catalog will enable access to all these capabilities and more\\r\\nthrough a set of well-defined, RESTful APIs. IBM is committed to offering\\r\\napplication developers easy access to additional components of Watson Data Platform , such as persistence stores and data sets. We hope that they can use our\\r\\nservices to extend their current suite of data and analytics tools, to innovate\\r\\nand create smart new ways of working with data.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nIn our next post, we’ll discuss the challenges around data governance, and\\r\\nexplore how IBM Data Manager can help you make light work of addressing them.\\r\\n\\r\\nJAY LIMBURN\\r\\nJay Limburn\\r\\n\\r\\nTHOMAS SCHAECK\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nPrevious Post\\r\\n\\r\\nWebSphere on the Cloud: Application Modernization (Phase 1)Next Post\\r\\n\\r\\nMaximize Control with IBM Bluemix Virtual serversADD COMMENT NO COMMENTS\\r\\nLEAVE A REPLY CANCEL REPLY\\r\\nYour email address will not be published. Required fields are marked *\\r\\n\\r\\nComment\\r\\n\\r\\nName *\\r\\n\\r\\nEmail *\\r\\n\\r\\nWebsite\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nSearch for:RECENT POSTS\\r\\n * IBM Watson Machine Learning – General Availability\\r\\n * Locating IoT with Skyhook Precision Location\\r\\n * Monitoring IBM Bluemix Container Service with Sysdig Container Intelligence\\r\\n * Mobile Foundation Service integration with Mobile Analytics Service\\r\\n * Intel® Optane™ SSD DC P4800X Available Now on IBM Cloud\\r\\n\\r\\nARCHIVES\\r\\nArchives Select Month August 2017 July 2017 June 2017 May 2017 April 2017 March 2017 February 2017 January 2017 December 2016 November 2016 October 2016 September 2016 August 2016 July 2016 June 2016 May 2016 April 2016 March 2016 February 2016 January 2016 December 2015 November 2015 October 2015 September 2015 August 2015 July 2015 June 2015 May 2015 April 2015 March 2015 February 2015 January 2015 December 2014 November 2014 October 2014 September 2014 August 2014 July 2014 June 2014 May 2014 April 2014 March 2014 February 2014 November 2013TAGS\\r\\nanalytics announcements api apps Architecture Center best-of-bluemix Bluemix bluemix-support-notifications buildpacks client success cloud cloudant cloud foundry conference conferences containers dashdb deployment devops docker eclipse garage garage-method hackathon homepage hybrid interconnect iot java Kubernetes liberty local microservices mobile MobileFirst node.js OpenStack openwhisk security Spark swift twilio video watson webinar More Data Analytics StoriesData Analytics\\r\\n\\r\\nMEDTRONIC MAKES DIABETES MANAGEMENT EASIER WITH REAL-TIME INSIGHTS FROM IBM\\r\\nSTREAMS\\r\\nWith cases of both type I and type II diabetes rising, Medtronic recognized the\\r\\nneed to create a new generation of glucose monitoring solutions that would give\\r\\npeople the tools to manage their diabetes more easily, in combination with\\r\\nroutine support from healthcare professionals. Find out how they are working\\r\\nwith IBM Watson to help.\\r\\n\\r\\nContinue reading\\r\\n\\r\\n\\r\\n\\r\\nShare this post:\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nData Analytics\\r\\n\\r\\nINTRODUCING A NEW LOOK AND FEEL FOR DB2 WAREHOUSE ON CLOUD\\r\\nToday, we're proud to announce the launch and immediate availability of the\\r\\nbrand new Db2 Warehouse on Cloud Web console and REST APIs! We want to make your\\r\\ninteraction with our world-class cloud data warehouse offering as seamless as\\r\\npossible, so we set out to completely redesign these two integral parts of our\\r\\nuser experience.\\r\\n\\r\\nContinue reading\\r\\n\\r\\n\\r\\n\\r\\nShare this post:\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nData Analytics\\r\\n\\r\\nIBM DASHDB FOR ANALYTICS IS NOW DB2 WAREHOUSE ON CLOUD\\r\\nWe're rebranding dashDB for Analytics to Db2 Warehouse on Cloud.\\r\\n\\r\\nContinue reading\\r\\n\\r\\n\\r\\n\\r\\nShare this post:\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nSIGN UP FOR A BLUEMIX TRIAL TODAY\\r\\n\\r\\n\\r\\nGet started free Learn more about Bluemix\\r\\n\\r\\nCONNECT WITH US\\r\\n\\r\\n\\r\\n * Contact\\r\\n * Privacy\\r\\n * Terms of use\\r\\n * Accessibility   \n",
       "232                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Homepage Follow Sign in Get started Homepage * Home\\r\\n * Data Science Experience\\r\\n * Data Catalog\\r\\n * IBM Data Refinery\\r\\n * \\r\\n * Watson Data Platform\\r\\n * \\r\\n\\r\\nCarmen Ruppach Blocked Unblock Follow Following Offering Manager for Data Refinery on Watson Data Platform at IBM Nov 14\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nSELF-SERVICE DATA PREPARATION WITH IBM DATA REFINERY\\r\\nIf you are like most data scientists, you are probably spending a lot of time to\\r\\ncleanse, shape and prepare your data before you can actually start with the more\\r\\nenjoyable part of building and training machine learning models. As a data\\r\\nanalyst, you might face similar struggles to obtain data in a format you need to\\r\\nbuild your reports. In many companies data scientists and analysts need to wait\\r\\nfor their IT teams to get access to cleaned data in a consumable format.\\r\\n\\r\\nIBM Data Refinery addresses this issue. It provides an intuitive self-service\\r\\ndata preparation environment where you can quickly analyze, cleanse and prepare\\r\\ndata sets. It is a fully managed cloud service, available in open beta now.\\r\\n\\r\\nAnalyze and prepare your data\\r\\n\\r\\nWith IBM Data Refinery, you can interactively explore your data and use a wide\\r\\nrange of transformations to cleanse and transform data into the format you need\\r\\nfor analysis.\\r\\n\\r\\nYou can use a simple point-and-click interface for selecting and combining a\\r\\nwide range of built-in operations, such as filtering, replacing, and deriving\\r\\nvalues. It is also possible to quickly remove duplicates, split and concatenate\\r\\nvalues, and choose from a comprehensive list of text and math operations.\\r\\n\\r\\nInteractive data exploration and preparationIf you prefer to code, in IBM Data Refinery you can directly enter R commands\\r\\nvia R libraries such as dplyr. We provide code templates and in-context\\r\\ndocumentation to help you become productive with the R syntax more quickly.\\r\\n\\r\\nCode templates to help users with R syntaxIf you’re not satisfied with the shaping results, you can easily undo and change\\r\\noperations in the Steps side bar.\\r\\n\\r\\nThe interactive user interface works on a subset of the data to give you a\\r\\nfaster preview of the operations and results. Once you’re happy with the sample\\r\\noutput, you can apply the transformations on the entire data set and save all\\r\\ntransformation steps in a data flow. You can repeat the data flow later and\\r\\ntrack changes that were applied to your data. To accelerate the job execution,\\r\\nApache Spark is used as the execution engine.\\r\\n\\r\\nData profiling and visualization\\r\\n\\r\\nData shaping is an iterative and time-consuming process. In a traditional data\\r\\nscience workflow, you might use one tool to apply various transformations to\\r\\nyour data set, and then load the data into another tool to visualize and\\r\\nevaluate the results. Over many cycles, this continual tool hopping can become\\r\\nfrustrating.\\r\\n\\r\\nIBM Data Refinery soothes the pain by integrating both data transformations and\\r\\nvisualizations in a single interface, so you can move between views with a\\r\\nsimple click. You can use the Profile tab to view descriptive statistics of your\\r\\ndata columns in order to better understand the distribution of values. You can\\r\\ncontinue to apply transformations and the corresponding profile information\\r\\nadjusts automatically.\\r\\n\\r\\nOn the Visualization tab you can select a combination of columns to build charts\\r\\nusing Brunel (open source visualization library). IBM Data Refinery\\r\\nautomatically suggests appropriate plots and you can choose between 12\\r\\npre-defined chart types. You can adjust the appearance of the charts using\\r\\nBrunel syntax.\\r\\n\\r\\nConnecting to data wherever it resides\\r\\n\\r\\nIBM Data Refinery comes with a comprehensive set of 30 prebuilt data connectors\\r\\nso that you can set up connections to a wide range of commonly used on-premises\\r\\nand cloud data stores. You can connect to IBM as well as non-IBM services. If\\r\\nyour data service is hosted on IBM Cloud (formerly IBM Bluemix), you can\\r\\ndirectly access the data service instance from IBM Data Refinery.\\r\\n\\r\\nOnce you specify a connection and connect the data object to your data, you can\\r\\nstart to analyze and refine your data wherever it resides.\\r\\n\\r\\nTry out IBM Data Refinery! Sign up for free at: https://www.ibm.com/cloud/data-refinery\\r\\n\\r\\n * Data Science\\r\\n * Data Visualization\\r\\n * Data Analysis\\r\\n * Data Refinery\\r\\n\\r\\nOne clap, two clap, three clap, forty?By clapping more or less, you can signal to us which stories really stand out.\\r\\n\\r\\nBlocked Unblock Follow FollowingCARMEN RUPPACH\\r\\nOffering Manager for Data Refinery on Watson Data Platform at IBM\\r\\n\\r\\nFollowIBM WATSON DATA PLATFORM\\r\\nBuild smarter applications and quickly visualize, share, and gain insights\\r\\n\\r\\n * \\r\\n * \\r\\n * \\r\\n * \\r\\n\\r\\nNever miss a story from IBM Watson Data Platform , when you sign up for Medium. Learn more Never miss a story from IBM Watson Data Platform Get updates Get updates   \n",
       "365                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Follow Sign in / Sign up Home About Insight Data Science Data Engineering Health Data AI 5 * Share\\r\\n * 5\\r\\n * \\r\\n * \\r\\n\\r\\nNever miss a story from Insight Data , when you sign up for Medium. Learn more Never miss a story from Insight Data Get updates Get updates Sebastien Dery Blocked Unblock Follow Following Master of Layers, Protector of the Graph, Wielder of Knowledge. #OpenScience\\r\\n#NoBullshit 2 days ago\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nGRAPH-BASED MACHINE LEARNING: PART 2\\r\\nCOMMUNITY DETECTION AT SCALE\\r\\nDuring the seven-week Insight Data Engineering Fellows Program recent grads and experienced software engineers learn the latest open source technologies by building a data platform to handle large, real-time datasets.\\r\\n\\r\\nSebastien Dery (now a Data Science Engineer at Yewno ) discusses his project on community detection on large datasets.\\r\\n\\r\\n\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\n#tltr : Graph-based machine learning is a powerful tool that can easily be merged\\r\\ninto ongoing efforts. This work reviews the feasibility of performing community\\r\\ndetection through a distributed implementation using GraphX. Embedded within the\\r\\nHadoop ecosystem, this modularity optimization approach allows the study of\\r\\nnetworks of unprecedented size. This change of scales, previously limited by\\r\\nRAM, opens exciting perspectives as the self modular structure of complex\\r\\nsystems have been shown to hold crucial information to understanding their\\r\\nnature.In my previous post , we discussed the foundation of community detection using modularity\\r\\noptimization. One major constraint however, is that your graph needs to fit in\\r\\nmemory. This quickly turns problematic as your number of nodes surpass billions, and\\r\\nthe number of edges becomes trillions.\\r\\n\\r\\nThankfully we can leverage distributed computation systems in order to solve this limitation. To do this we first need to define the state\\r\\nof a node so that it contains all the information needed during computation;\\r\\nthis will serve as a basic structure to pass around between the machines of our\\r\\ndistributed cluster.\\r\\n\\r\\n“Node” and “Vertex” are often used interchangeably in the literature. This class\\r\\nserves as structure for the nodes within the graph.Let’s also briefly review the process behind modularity optimization. This works\\r\\nby iteratively merging nodes that optimize for local modularity to yield a new, and smaller, graph. Repeat until satisfied.\\r\\n\\r\\nTwo great properties emerge from this approach\\r\\n\\r\\n 1. Locality : Each node requires knowledge from only its first-degree neighbors. This\\r\\n    means a minimal amount of data needs to be passed around between clusters.\\r\\n    This way, you don’t need to extensively jump from node to node across the\\r\\n    clusters in order to get the necessary information.\\r\\n 2. Independence : Each local computation occurs independently of the graph layout. Within\\r\\n    an iteration, every node can asynchronously send its information to its\\r\\n    neighbors without waiting for a blocking sequential set of operations to\\r\\n    happen.\\r\\n\\r\\nThese are important points to highlight as they make distributed computation a\\r\\nprime candidate for this memory problem. Turns out we can easily implement the\\r\\nlogic behind those properties using nothing but a simple iteration and a\\r\\ndeveloper-defined halting criteria. As previously discussed this can take many forms; here are a few ideas for brainstorming:\\r\\n\\r\\n * Scheduled based on a predefined number of iterations\\r\\n * Hits a specific total number of communities\\r\\n * Modularity gain between iteration is below a threshold\\r\\n\\r\\nSimple iteration over the two stage process of our optimization: transfer and\\r\\ncompress.Let’s dive into the initial step of transferring community between nodes.\\r\\nRemember that each node needs the information from its neighbors in order to\\r\\ncompute the gradient for local modularity.\\r\\n\\r\\nTRANSFER\\r\\nThe best way to do this at scale (when you don’t know where the information\\r\\nultimately is on disk) is by using distributed transactions (aka passing messages ). This type of architecture is ubiquitous in modern computer software; it is\\r\\nused as a way for the objects that make up a program to work with each other and\\r\\nas a way for objects and systems running on different computers (e.g., the\\r\\nInternet) to interact. In algorithms, you’ll often find it referenced under the\\r\\nname of Belief Propagation or simply message passing . In the context of community detection, each node sends a message to its\\r\\nneighbors with content along the lines of:\\r\\n\\r\\n“ Hey I’m your friendly neighbor Node 3 from Community 12 ”\\r\\n\\r\\nBy independently sending messages to their first degree neighbors, each node can\\r\\nretrieve all the information necessary for them to optimize for local\\r\\nmodularity. The content of each message can easily be tweaked thus adding\\r\\nconsiderable flexibility to your approach.If you’ve ever worked with graphs you’re likely to be very familiar with the\\r\\nconcepts of vertices and edges . Should we perform the message passing exhaustively you’d basically go through\\r\\neach vertex and send a message for each of its edges. This is not an\\r\\nintrinsically bad approach if that’s all you have to work with. Turns out that\\r\\nin the world of GraphX we have access to a third primitive for easy manipulation of our data: the triplet .\\r\\n\\r\\nThe three different types of view allowed within GraphX. Taken from AMPLab .The triplet logically joins the vertex and edge properties for a simplified and\\r\\nuseful view. Literally, the EdgeTriplet class extends the Edge class by simply adding the srcAttr and dstAttr members containing the source and destination properties respectively.\\r\\n\\r\\nBy reducing the triplets view, each node receives N messages corresponding to its N first degree neighbors. sendMsg and mergeMsg are both internal functions which perform the necessary aggregation for the\\r\\nlocal modularity update. Independently, and in parallel, each node waits for its\\r\\nturn to reduce all its messages into a coherent local sum of weighted edges, and\\r\\nmake a decision based on the local modularity deltaQ of each neighboring community.\\r\\n\\r\\nA few iterations later, the graph has converged to a local equilibrium (e.g. a\\r\\nminimal amount of nodes feel the need to change community). The algorithm can\\r\\nnow progress to the next step of compressing those communities into a compact\\r\\nrepresentation. This is done by creating a new graph with a new set of nodes\\r\\n(corresponding to each community) and edges being inferred from the edges during\\r\\nthe previous computation (e.g. average or sum of external edges).\\r\\n\\r\\nCOMPRESSION\\r\\nWhat function to choose really depends on the use case (e.g. averaging, total\\r\\nsum, maximum, softmax , etc. are all valid functions, although their respective advantages remains\\r\\nunclear in this particular scenario). When in doubt, let’s use a simple average.\\r\\nNote that additional information, say the internal coherence within a community,\\r\\ncan be propagated in a similar fashion to the condensed node and provide\\r\\nvaluable information.\\r\\n\\r\\nEffect of compressing community into single nodes at each iteration.Finally, here we have a fully functional procedure to perform modularity\\r\\noptimization on graphs of ridiculously large size, assuming we have enough\\r\\ncomputers to store all the information on disk.\\r\\n\\r\\nCAVEATS AND TIPS\\r\\nCOMPUTATION TIME\\r\\nNote that the number of meta-communities naturally decreases at each pass, and\\r\\nas a consequence most of the computing time is used in the first pass. This\\r\\nsuggests pre-ordering of the data would hold considerable benefit in terms of\\r\\ncomputation time.\\r\\n\\r\\nOptimizing for node locality at the cluster level means less transfer between\\r\\nmachines.CONVERGENCE\\r\\nThis approach does not necessarily converge to the optimal solution . To improve this, multiple iterations can increase confidence over the\\r\\nstructure of your data. Conveniently, this also offers a proxy for the\\r\\nprobability of two nodes belonging to the same community.\\r\\n\\r\\nLAYOUT\\r\\nTake into account graph connectivity when determining the usefulness of this\\r\\nstrategy. For example, for a completely connected and unweighted graph, the\\r\\noutput will be degenerate. Consider thresholding the graph beforehand to extract\\r\\na more sparse representation of your data.\\r\\n\\r\\nThe adequateness of modularity optimization is dependent on the connectivity\\r\\npattern of your graph. For example, in a lattice layout this algorithm will\\r\\nperform rather poorly. Modularity optimization doesn’t guarantee adequate\\r\\nclustering; thus obtaining a community at the end is not enough to conclusively\\r\\nsay a node decidedly belongs to that group (or even any group, for that matter).HIERARCHY\\r\\nThe iterative nature of this process offers a hierarchical view between\\r\\ncommunities of subsequent iteration. The intermediary step should therefore be\\r\\nsaved for further investigation as they likely yield valuable information on the\\r\\nstructural complexity of the data. This saving procedure is not covered in this\\r\\npost but should be trivial to introduce (insert configuration state into your\\r\\nfavorite database) between iteration.\\r\\n\\r\\nSUMMARY\\r\\nThis work reviewed the feasibility of performing community detection through a\\r\\ndistributed implementation using GraphX . Embedded within the Hadoop ecosystem , this modularity optimization approach allows the study of networks of\\r\\nunprecedented size. This change of scales, previously limited by RAM, opens\\r\\nexciting perspectives as the self modular structure of complex systems have been\\r\\nshown to hold crucial information to understanding their nature. This enables,\\r\\namong others, targeted marketing , market segmentation , gene clustering , topic modeling , etc.\\r\\n\\r\\nBeing an unsupervised learning technique and an initial starting point for a lot\\r\\nof analysis, the low barriers of entry make this approach applicable to a wide\\r\\nrange of datasets.\\r\\n\\r\\nDid I miss something crucial to get you up and running? Have something to add?\\r\\nWould love to hear your experience with this type of approach!\\r\\n\\r\\n\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nWant to learn Spark, machine learning with graphs, and other big data tools from\\r\\ntop data engineers in Silicon Valley or New York? The Insight Data Engineering Fellows Program is a free 7-week professional training where you can build cutting edge big\\r\\ndata platforms and transition to a career in data engineering.\\r\\n\\r\\nLearn more about the program and apply today .\\r\\n\\r\\nBig Data Data Science Machine Learning Insight Data Engineering Social Network 5 Blocked Unblock Follow FollowingSEBASTIEN DERY\\r\\nMaster of Layers, Protector of the Graph, Wielder of Knowledge. #OpenScience\\r\\n#NoBullshit\\r\\n\\r\\nFollowINSIGHT DATA\\r\\nInsight Fellows Program —Your bridge to careers in Data Science and Data\\r\\nEngineering.   \n",
       "399                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Homepage Follow Sign in Get started * Home\\r\\n * Data Science Experience\\r\\n * Data Catalog\\r\\n * IBM Data Refinery\\r\\n * \\r\\n * Watson Data Platform\\r\\n * \\r\\n\\r\\nSourav Mazumder Blocked Unblock Follow Following Nov 27\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nUSING APACHE SPARK AS A PARALLEL PROCESSING FRAMEWORK FOR ACCESSING REST BASED\\r\\nDATA SERVICES\\r\\nToday’s world of data science leverages data from various sources. Commonly,\\r\\nthese sources are Hadoop File System, Enterprise Data Warehouse, Relational\\r\\nDatabase systems, Enterprise file systems, etc. The data from these sources are\\r\\naccessed in bulk using connectors specific to the underlying technology and\\r\\noptimized for accessing large volume of data.\\r\\n\\r\\nHowever, many a times, a data science exploration/modeling exercise also needs\\r\\nto access data from sources that support only API-based data access. These\\r\\nAPI-based data sources/data services can be of various types. For example:\\r\\n\\r\\n * Data services (external or internal), which can provide curated/enriched data\\r\\n   in record-by-record manner.\\r\\n * Validation services for verifying the data using an API. For example Address\\r\\n   validation.\\r\\n * Machine learning/AI services, which provide prediction, recommendations, and\\r\\n   insights based on a single input record.\\r\\n * Service from internal systems (like CRM, MDM, etc.) of the organization,\\r\\n   which supports data access through API only in record-by-record manner.\\r\\n * And many more …\\r\\n\\r\\nThese API-based data services are commonly implemented using REST architectural\\r\\nstyle ( https://en.wikipedia.org/wiki/Representational_state_transfer ) and are designed to be called for single item (or a limited set of items) per\\r\\nrequest. While this works well when the API needs to be called from an online\\r\\napplication, the approach breaks down in situations when the API has to be\\r\\ncalled in bulk. For example, during an online sign-up process an address\\r\\nvalidation API can be called for the particular address of the user. But, say in\\r\\na health care analytics application, where addresses of thousands of doctors,\\r\\nwhich already exist in a database or were obtained as part of a bulk load from\\r\\nan external source, have to be verified, this approach will not work. Because of\\r\\nthe “single item per request” design of the API, you’d have to call the API\\r\\nthousands of times.\\r\\n\\r\\nCalling data service APIs in sequence — Processing Time = (# of Records)*(API\\r\\nresponse time)\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nThe above pseudo code snippet shows how calling a target REST API service is\\r\\nhandled in a sequential manner. You must first load the list of parameter values\\r\\nfrom a file or table in the memory. Next run a loop. In the loop, the target\\r\\nREST API has to be called for each set of parameter values. From the response\\r\\nreturned by each call the output must be extracted. The output is typically\\r\\npopulated in a complex object like JSON, XML, etc. Next, the necessary part of\\r\\nthe output has to be added to a result array or collection. For that, you must\\r\\nknow the schema of the result beforehand so that you can process the result\\r\\naccordingly. Finally, you can filter, exploring, aggregating data from the\\r\\nresult array or collection. For all of these steps, you have to use\\r\\nlanguage-specific complex code.\\r\\n\\r\\nAlternatively, you could use a programming language-specific library related to\\r\\nmulti-processing/multi-threading that can parallelize the call to the API.\\r\\nHowever, with that approach the parallelization achieved from a single machine\\r\\nwould be minuscule — limited to the number of cores of the machine. Consider, a\\r\\ncase where someone is trying to get personality insights from tweets or Facebook\\r\\ncomments using a Natural Language Processing service. The tweets and comments\\r\\ncan be in tens to hundreds of thousands. So, using a single machine could take a\\r\\nnumber of hours to get the result. Hence, the approach should be to use a\\r\\ndistributed processing framework to make the API calls parallelized using\\r\\nmultiple cores of multiple machines with the least coding effort. Though it is\\r\\npossible to get distributed computing libraries or frameworks to achieve the\\r\\nsame in some programming languages like Java, C++ etc., they require a\\r\\nreasonable amount of coding and setup to achieve the same result. Achieving this\\r\\nin popular data science languages, like R or Python is actually more difficult\\r\\nas they are originally designed to run in single threaded/single machine\\r\\nenvironment.\\r\\n\\r\\nHere enters distributed computing frameworks like Apache Spark ( https://spark.apache.org/ ). REST APIs are inherently conducive to parallelization as each call to the\\r\\nAPI is completely independent of any other call to the same API. This fact, in\\r\\nconjunction with the parallel computing capability of Spark, can be leveraged to\\r\\ncreate a solution that solves the problem by delegating the API call to Spark’s\\r\\nparallel workers. Under this approach, one can package a specification for how\\r\\nto call the API along with the input data, and pass that to Spark to divide the\\r\\neffort among its workers (and tasks). The output can be assembled in set-level\\r\\nabstractions supported by Spark (like Dataframes or Datasets ) and passed back to the calling program. This approach not only helps you turn a\\r\\nsequential execution into a parallel one with the least coding effort, but also\\r\\nmakes it much easier to analyze and transform the returned result with an easier\\r\\ndata abstraction model to work with.\\r\\n\\r\\nThe performance benefit you get is tremendous in this approach. This turns a\\r\\nproblem that takes incremental time for computation (that increases linearly\\r\\nwith the number of records to process), to one that is much more efficient and\\r\\nscales linearly on a much lower slope — number of records to process divided by\\r\\nthe number of cores available to process them. Theoretically, one can make the\\r\\nprocess constant time by having enough cores to process ALL of the records at\\r\\nonce.\\r\\n\\r\\nTo enable the benefits of using Spark to call REST APIs, we are introducing a\\r\\ncustom data source for Spark, namely REST Data Source. It has been built by\\r\\nextending Spark’s Data Source API. This helps in delegating calls to the target\\r\\nREST API to a Spark level Task for each set of input parameter values/record.\\r\\nThis also enables the results from multiple API calls to be returned as one\\r\\nSpark Dataframe. The REST Data Source expects the input to be in the format of a\\r\\nSpark Temporary table. The results from the API calls are returned in a single\\r\\nDataframe of Rows including the input parameters in their corresponding column\\r\\nnames, as well as the output from the REST call in a structure matching that of\\r\\nthe target API’s response. You can check the schema of this Dataframe, and\\r\\naccess the result as necessary using Spark SQL.\\r\\n\\r\\nThe architecture of REST Data SourceThe above figure shows how REST Data Source works.\\r\\n\\r\\n 1. You first read different sets of parameter values (that have to be sent to\\r\\n    target REST API) from a file/table to a Spark Dataframe (say Input Data\\r\\n    Frame).\\r\\n 2. Then the Input Data Frame is passed to the REST Data Source.\\r\\n 3. The REST Data Source returns the results to another Dataframe, say Result\\r\\n    Data Frame.\\r\\n 4. Now you can use Spark SQL to explore, aggregate, and filter the result using\\r\\n    the Result Data Frame.\\r\\n\\r\\nREST Data Source internally calls the target REST API in parallel by executing\\r\\nmultiple tasks spawned by multiple worker processes running in different\\r\\nmachines. Each task is responsible for calling the target REST API Service for a\\r\\npart of the input (part of sets of parameter values).\\r\\n\\r\\nThe code snippet below demonstrates how to use REST Data Source in Python to get\\r\\nresults from Socrata Data Service (SODA API) for multiple sets of parameter\\r\\nvalues by calling the appropriate REST API in parallel.\\r\\n\\r\\nA sample code snippet showing use of REST Data Source to call REST API in\\r\\nparallelYou can configure the REST Data Source for different extent of parallelization.\\r\\nDepending on the volume of input sets of parameter values to be processed and\\r\\nthroughput supported by the target REST API server, you can pass the number of\\r\\npartitions to be used, and that can limit or extend the level of parallelization\\r\\nas needed. You can use this framework in all programming languages supported by\\r\\nSpark — Python, Scala, R, or Java — without any additional coding specific to\\r\\nthat programming language. Last, but not the least, you can also use this\\r\\nframework to ensure that the target API is called only once for a given set of\\r\\nparameter values. In this way you can avoid calling the target REST API multiple\\r\\ntimes for same set of parameter values. This is especially useful when you must\\r\\npay for the REST API being called or there is a limit per day for the same.\\r\\n\\r\\nSee \\r\\nhttps://github.com/sourav-mazumder/Data-Science-Extensions/tree/master/spark-datasource-rest for details of the REST Data Source.\\r\\n\\r\\nYou can also refer to this notebook \\r\\nhttps://dataplatform.ibm.com/analytics/notebooks/ae63f056-e267-443e-bfc0-b9331f51d68a/view?access_token=0ec63c6e031aa57d065a4e1c4b71733729db43b1490c331a44323cce28725b7d for an example of how to use the REST Data Source.\\r\\n\\r\\nSign up for a free Data Science Experience account ( https://datascience.ibm.com/ ) to try out this technique on a Spark cluster.\\r\\n\\r\\n * Big Data\\r\\n * Spark\\r\\n * Artificial Intelligence\\r\\n * Data Science\\r\\n * Rest Api\\r\\n\\r\\nOne clap, two clap, three clap, forty?By clapping more or less, you can signal to us which stories really stand out.\\r\\n\\r\\n21 Blocked Unblock Follow FollowingSOURAV MAZUMDER\\r\\nMedium member since Nov 2017 FollowIBM WATSON DATA PLATFORM\\r\\nBuild smarter applications and quickly visualize, share, and gain insights\\r\\n\\r\\n * 21\\r\\n * \\r\\n * \\r\\n * \\r\\n\\r\\nNever miss a story from IBM Watson Data Platform , when you sign up for Medium. Learn more Never miss a story from IBM Watson Data Platform Get updates Get updates   \n",
       "578                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     This video shows you how to construct queries to access the primary index through the API.Visit http://www.cloudant.com/sign-up to sign up for a free Cloudant account. Find more videos and tutorials in the Cloudant Learning Center: http://www.cloudant.com/learning-center   \n",
       "692                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Homepage Follow Sign in / Sign up Homepage * Home\\r\\n * Data Science Experience\\r\\n * Data Catalog\\r\\n * \\r\\n * Watson Data Platform\\r\\n * \\r\\n\\r\\nSusanna Tai Blocked Unblock Follow Following Offering Manager, Watson Data Platform | Data Catalog Oct 30\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nHOW SMART CATALOGS CAN TURN THE BIG DATA FLOOD INTO AN OCEAN OF OPPORTUNITY\\r\\nOne of the earliest documented catalogs was compiled at the great library of\\r\\nAlexandria in the third century BC, to help scholars manage, understand and\\r\\naccess its vast collection of literature. While that cataloging process\\r\\nrepresented a massive undertaking for the Alexandrian librarians, it pales in\\r\\ncomparison to the task of wrangling the volume and variety of data that modern\\r\\norganizations generate.\\r\\n\\r\\nNowadays, data is often described as an organization’s most valuable asset, but\\r\\nunless users can easily sift through data artifacts to find the information they\\r\\nneed, the value of that data may remain unrealized. Catalogs can solve this\\r\\nproblem by providing an indexed set of information about the organization’s\\r\\ndata, storing metadata that describes all assets and providing a reference to\\r\\nwhere they can be found or accessed.\\r\\n\\r\\nIt’s not just the size and complexity of the data that makes cataloging a tough\\r\\nchallenge: organizations also need to be able to perform increasingly\\r\\ncomplicated operations on that data at high speed, and even in real-time. As a\\r\\nresult, technology leaders must continually find better ways to solve today’s\\r\\nversion of the same cataloging challenges faced in Alexandria all those years\\r\\nago.\\r\\n\\r\\nENTER IBM\\r\\nIBM’s aim with Watson Data Platform is to make data accessible for anyone who uses it. An integral part of Watson\\r\\nData Platform will be a new intelligent asset catalog, IBM Data Catalog , a solution underpinned by a central repository of metadata describing all the\\r\\ninformation managed by the platform. Unlike many other catalog solutions on the\\r\\nmarket, the intelligent asset catalog will also offer full end-to-end\\r\\ncapabilities around data lifecycle and governance.\\r\\n\\r\\nBecause all the elements of Watson Data Platform can utilize the same catalog,\\r\\nusers will be able to share data with their colleagues more easily, regardless\\r\\nof what the data is, where it is stored, or how they intend to use it. In this\\r\\nway, the intelligent asset catalog will unlock the value held within that data\\r\\nacross user groups — helping organizations use this key asset to its full\\r\\npotential.\\r\\n\\r\\nBREAKING DOWN SILOS\\r\\nWith Watson Data Platform, data engineers, data scientists and other knowledge\\r\\nworkers throughout an enterprise can search for, share and leverage assets\\r\\n(including datasets, files, connections, notebooks, data flows, models and\\r\\nmore). Assets can be accessed using the Data Science Experience web user interface to analyze data,\\r\\n\\r\\nTo collaborate with colleagues, users can put assets into a Project that acts as\\r\\na shared sandbox where the whole team can access and utilize them. Once their\\r\\nwork is complete, they can submit any resulting content to the catalog for\\r\\nfurther reuse by other people and groups across the organization.\\r\\n\\r\\nRich metadata about each asset makes it easy for knowledge workers to find and\\r\\naccess relevant resources. Along with data files, the catalog can also include\\r\\nconnections to databases and other data sources, both on- and off-premises,\\r\\ngiving users a full 360-degree view to all information relevant to their\\r\\nbusiness, regardless of where or how it is stored.\\r\\n\\r\\nMANAGING DATA OVER TIME\\r\\nIt’s important to look at data as an evolving asset, rather than something that\\r\\nstays fixed over time. To help manage and trace this evolution, IBM Data Catalog\\r\\nwill keep a complete track of which users have added or modified each asset, so\\r\\nthat it is always clear who is responsible for any changes.\\r\\n\\r\\nSMART CATALOG CAPABILITIES FOR BIG DATA MANAGEMENT\\r\\nThe concept of catalogs may be simple, but when they’re being used to make sense\\r\\nof huge amounts of constantly changing data, smart capabilities make all the\\r\\ndifference. Here are some of the key smart catalog functionalities that we see\\r\\nas integral to tackling the big data challenge.\\r\\n\\r\\nDATA AND ASSET TYPE AWARENESS\\r\\nWhen a user chooses to preview or view an asset of a particular type, the data\\r\\nand asset type awareness feature will automatically launch the data in the best\\r\\nviewer — such as a shaper for a dataset, or a canvas for a data flow. This will\\r\\nsave time and boost productivity for users, optimizing discovery and making it\\r\\neasier to work with a variety of data types without switching tools.\\r\\n\\r\\nINTELLIGENT SEARCH AND EXPLORATION\\r\\nBy combining metadata, machine learning-based algorithms and user interaction\\r\\ndata, it is possible to fine-tune search results over time. Presenting users\\r\\nwith the most relevant data for their purpose will increase usefulness of the\\r\\nsolution the more it is used.\\r\\n\\r\\nSOCIAL CURATION\\r\\nEffective use of data throughout your organization is a two-way street: when\\r\\nusers discover a useful dataset, it’s important for them to help others find it\\r\\ntoo. Users can be encouraged to engage by taking advantage of curation features,\\r\\nenabling them to tag, rank and comment on assets within the catalog. By\\r\\naugmenting the metadata for each asset, this can help the catalog’s intelligent\\r\\nsearch algorithms guide users to the assets that are most relevant to their\\r\\nneeds.\\r\\n\\r\\nDATA LINEAGE\\r\\nIf data is incomplete or inaccurate, utilizing it can cause more problems than\\r\\nit solves. On the other hand, if data is accurate but users do not trust it,\\r\\nthey might not use it when it could make a real difference. In either scenario,\\r\\ndata lineage can help.\\r\\n\\r\\nData lineage captures the complete history of an asset in the catalog: from its\\r\\noriginal source, through all the operations and transformations it has\\r\\nundergone, to its current state. By exploring this lineage, users can be\\r\\nconfident they know where assets have come from, how those assets have evolved,\\r\\nand whether they can be trusted.\\r\\n\\r\\nMONITORING\\r\\nTaking a step back to a higher-level view, monitoring features will help users\\r\\nkeep track of overall usage of the catalog. Real-time dashboards help chief data\\r\\nofficers and other data professionals monitor how data is being used, and\\r\\nidentify ways to increase its usage in different areas of the organization.\\r\\n\\r\\nMETADATA DISCOVERY\\r\\nWe have already mentioned that data needs to be seen as an evolving asset —\\r\\nwhich means our catalogs must evolve with it. We plan to make it easy for users\\r\\nto augment assets with metadata manually; in the future, it may also be possible\\r\\nto integrate algorithms that can discover assets and capture their metadata\\r\\nautomatically.\\r\\n\\r\\nDATA GOVERNANCE\\r\\nFor many organizations, keeping data secure while ensuring access for authorized\\r\\nusers is one of the most significant information management challenges. You can\\r\\nmitigate this challenge with rule-based access control and automatic enforcement\\r\\nof data governance policies.\\r\\n\\r\\nAPIS\\r\\nFinally, the catalog will enable access to all these capabilities and more\\r\\nthrough a set of well-defined, RESTful APIs. IBM is committed to offering\\r\\napplication developers easy access to additional components of Watson Data Platform , such as persistence stores and data sets. We hope that they can use our\\r\\nservices to extend their current suite of data and analytics tools, to innovate\\r\\nand create smart new ways of working with the data.\\r\\n\\r\\nLearn more about IBM Data Catalog\\r\\n\\r\\n\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nWritten by Jay Limburn\\r\\nDistinguished Engineer and Offering Lead, Watson Data Platform\\r\\n\\r\\nOriginally published at www.ibm.com on August 1, 2017\\r\\n\\r\\n * Data Catalog\\r\\n * Data Management\\r\\n * Data Analytics\\r\\n * IBM\\r\\n * Ibm Watson\\r\\n\\r\\nA single golf clap? Or a long standing ovation?By clapping more or less, you can signal to us which stories really stand out.\\r\\n\\r\\nBlocked Unblock Follow FollowingSUSANNA TAI\\r\\nOffering Manager, Watson Data Platform | Data Catalog\\r\\n\\r\\nFollowIBM WATSON DATA PLATFORM\\r\\nBuild smarter applications and quickly visualize, share, and gain insights\\r\\n\\r\\n * \\r\\n * \\r\\n * \\r\\n * \\r\\n\\r\\nNever miss a story from IBM Watson Data Platform , when you sign up for Medium. Learn more Never miss a story from IBM Watson Data Platform Get updates Get updates   \n",
       "761                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Homepage Follow Sign in Get started Homepage * Home\\r\\n * Data Science Experience\\r\\n * Data Catalog\\r\\n * IBM Data Refinery\\r\\n * \\r\\n * Watson Data Platform\\r\\n * \\r\\n\\r\\nSourav Mazumder Blocked Unblock Follow Following Nov 27\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nUSING APACHE SPARK AS A PARALLEL PROCESSING FRAMEWORK FOR ACCESSING REST BASED\\r\\nDATA SERVICES\\r\\nToday’s world of data science leverages data from various sources. Commonly,\\r\\nthese sources are Hadoop File System, Enterprise Data Warehouse, Relational\\r\\nDatabase systems, Enterprise file systems, etc. The data from these sources are\\r\\naccessed in bulk using connectors specific to the underlying technology and\\r\\noptimized for accessing large volume of data.\\r\\n\\r\\nHowever, many a times, a data science exploration/modeling exercise also needs\\r\\nto access data from sources that support only API-based data access. These\\r\\nAPI-based data sources/data services can be of various types. For example:\\r\\n\\r\\n * Data services (external or internal), which can provide curated/enriched data\\r\\n   in record-by-record manner.\\r\\n * Validation services for verifying the data using an API. For example Address\\r\\n   validation.\\r\\n * Machine learning/AI services, which provide prediction, recommendations, and\\r\\n   insights based on a single input record.\\r\\n * Service from internal systems (like CRM, MDM, etc.) of the organization,\\r\\n   which supports data access through API only in record-by-record manner.\\r\\n * And many more …\\r\\n\\r\\nThese API-based data services are commonly implemented using REST architectural\\r\\nstyle ( https://en.wikipedia.org/wiki/Representational_state_transfer ) and are designed to be called for single item (or a limited set of items) per\\r\\nrequest. While this works well when the API needs to be called from an online\\r\\napplication, the approach breaks down in situations when the API has to be\\r\\ncalled in bulk. For example, during an online sign-up process an address\\r\\nvalidation API can be called for the particular address of the user. But, say in\\r\\na health care analytics application, where addresses of thousands of doctors,\\r\\nwhich already exist in a database or were obtained as part of a bulk load from\\r\\nan external source, have to be verified, this approach will not work. Because of\\r\\nthe “single item per request” design of the API, you’d have to call the API\\r\\nthousands of times.\\r\\n\\r\\nCalling data service APIs in sequence — Processing Time = (# of Records)*(API\\r\\nresponse time)\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nThe above pseudo code snippet shows how calling a target REST API service is\\r\\nhandled in a sequential manner. You must first load the list of parameter values\\r\\nfrom a file or table in the memory. Next run a loop. In the loop, the target\\r\\nREST API has to be called for each set of parameter values. From the response\\r\\nreturned by each call the output must be extracted. The output is typically\\r\\npopulated in a complex object like JSON, XML, etc. Next, the necessary part of\\r\\nthe output has to be added to a result array or collection. For that, you must\\r\\nknow the schema of the result beforehand so that you can process the result\\r\\naccordingly. Finally, you can filter, exploring, aggregating data from the\\r\\nresult array or collection. For all of these steps, you have to use\\r\\nlanguage-specific complex code.\\r\\n\\r\\nAlternatively, you could use a programming language-specific library related to\\r\\nmulti-processing/multi-threading that can parallelize the call to the API.\\r\\nHowever, with that approach the parallelization achieved from a single machine\\r\\nwould be minuscule — limited to the number of cores of the machine. Consider, a\\r\\ncase where someone is trying to get personality insights from tweets or Facebook\\r\\ncomments using a Natural Language Processing service. The tweets and comments\\r\\ncan be in tens to hundreds of thousands. So, using a single machine could take a\\r\\nnumber of hours to get the result. Hence, the approach should be to use a\\r\\ndistributed processing framework to make the API calls parallelized using\\r\\nmultiple cores of multiple machines with the least coding effort. Though it is\\r\\npossible to get distributed computing libraries or frameworks to achieve the\\r\\nsame in some programming languages like Java, C++ etc., they require a\\r\\nreasonable amount of coding and setup to achieve the same result. Achieving this\\r\\nin popular data science languages, like R or Python is actually more difficult\\r\\nas they are originally designed to run in single threaded/single machine\\r\\nenvironment.\\r\\n\\r\\nHere enters distributed computing frameworks like Apache Spark ( https://spark.apache.org/ ). REST APIs are inherently conducive to parallelization as each call to the\\r\\nAPI is completely independent of any other call to the same API. This fact, in\\r\\nconjunction with the parallel computing capability of Spark, can be leveraged to\\r\\ncreate a solution that solves the problem by delegating the API call to Spark’s\\r\\nparallel workers. Under this approach, one can package a specification for how\\r\\nto call the API along with the input data, and pass that to Spark to divide the\\r\\neffort among its workers (and tasks). The output can be assembled in set-level\\r\\nabstractions supported by Spark (like dataframes or data sets ) and passed back to the calling program. This approach not only helps you turn a\\r\\nsequential execution into a parallel one with the least coding effort, but also\\r\\nmakes it much easier to analyze and transform the returned result with an easier\\r\\ndata abstraction model to work with.\\r\\n\\r\\nThe performance benefit you gets is tremendous in this approach. This turns a\\r\\nproblem that takes incremental time for computation (that increases linearly\\r\\nwith the number of records to process), to one that is much more efficient and\\r\\nscales linearly on a much lower slope — number of records to process divided by\\r\\nthe number of cores available to process them. Theoretically, one can make the\\r\\nprocess constant time by having enough cores to process ALL of the records at\\r\\nonce.\\r\\n\\r\\nTo enable the benefits of using Spark to call REST APIs, we are introducing a\\r\\ncustom data source for Spark, namely REST Data Source. It has been built by\\r\\nextending Spark’s Data Source API. This helps in delegating calls to the target\\r\\nREST API to a Spark level Task for each set of input parameter values/record.\\r\\nThis also enables the results from multiple API calls to be returned as one\\r\\nSpark Dataframe. The REST Data Source expects the input to be in the format of a\\r\\nSpark Temporary table. The results from the API calls are returned in a single\\r\\nDataframe of Rows including the input parameters in their corresponding column\\r\\nnames, as well as the output from the REST call in a structure matching that of\\r\\nthe target API’s response. You can check the schema of this Dataframe, and\\r\\naccess the result as necessary using Spark SQL.\\r\\n\\r\\nThe architecture of REST Data SourceThe above figure shows how REST Data Source works.\\r\\n\\r\\n 1. You first read different sets of parameter values (that have to be sent to\\r\\n    target REST API) from a file/table to a Spark Dataframe (say Input Data\\r\\n    Frame).\\r\\n 2. Then the Input Data Frame is passed to the REST Data Source.\\r\\n 3. The REST Data Source returns the results to another Dataframe, say Result\\r\\n    Data Frame.\\r\\n 4. Now you can use Spark SQL to explore, aggregate, and filter the result using\\r\\n    the Result Data Frame.\\r\\n\\r\\nREST Data Source internally calls the target REST API in parallel by executing\\r\\nmultiple tasks spawned by multiple worker processes running in different\\r\\nmachines. Each task is responsible for calling the target REST API Service for a\\r\\npart of the input (part of sets of parameter values).\\r\\n\\r\\nThe code snippet below demonstrates how to use REST Data Source in Python to get\\r\\nresults from Socrata Data Service (SODA API) for multiple sets of parameter\\r\\nvalues by calling the appropriate REST API in parallel.\\r\\n\\r\\nA sample code snippet showing use of REST Data Source to call REST API in\\r\\nparallelYou can configure the REST Data Source for different levels of parallelization.\\r\\nDepending on the volume of input sets of parameter values to be processed and\\r\\nthroughput supported by the target REST API server, you can pass the number of\\r\\npartitions to be used, and that can limit or extend the level of parallelization\\r\\nas needed. You can use this framework in all programming languages supported by\\r\\nSpark — Python, Scala, R, or Java — without any additional coding specific to\\r\\nthat programming language. Last, but not the least, you can also use this\\r\\nframework to ensure that the target API is called only once for a given set of\\r\\nparameter values. In this way you can avoid calling the target REST API multiple\\r\\ntimes for same set of parameter values. This is especially useful when you must\\r\\npay for the REST API being called or there is a limit per day for the same.\\r\\n\\r\\nSee \\r\\nhttps://github.com/sourav-mazumder/Data-Science-Extensions/tree/master/spark-datasource-rest for details of the REST Data Source. Also see this notebook \\r\\nhttps://dataplatform.ibm.com/analytics/notebooks/ae63f056-e267-443e-bfc0-b9331f51d68a/view?access_token=0ec63c6e031aa57d065a4e1c4b71733729db43b1490c331a44323cce28725b7d for an example of how to use the REST Data Source.\\r\\n\\r\\n * Big Data\\r\\n * Spark\\r\\n * Artificial Intelligence\\r\\n * Data Science\\r\\n * Rest Api\\r\\n\\r\\nOne clap, two clap, three clap, forty?By clapping more or less, you can signal to us which stories really stand out.\\r\\n\\r\\n9 Blocked Unblock Follow FollowingSOURAV MAZUMDER\\r\\nMedium member since Nov 2017 FollowIBM WATSON DATA PLATFORM\\r\\nBuild smarter applications and quickly visualize, share, and gain insights\\r\\n\\r\\n * 9\\r\\n * \\r\\n * \\r\\n * \\r\\n\\r\\nNever miss a story from IBM Watson Data Platform , when you sign up for Medium. Learn more Never miss a story from IBM Watson Data Platform Get updates Get updates   \n",
       "970                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             This video shows you how to construct queries to access the primary index through the API.Visit http://www.cloudant.com/sign-up to sign up for a free Cloudant account.   \n",
       "971                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Homepage Follow Sign in Get started * Home\\r\\n * Data Science Experience\\r\\n * Data Catalog\\r\\n * IBM Data Refinery\\r\\n * \\r\\n * Watson Data Platform\\r\\n * \\r\\n\\r\\nCarmen Ruppach Blocked Unblock Follow Following Offering Manager for Data Refinery on Watson Data Platform at IBM Nov 14, 2017\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nSELF-SERVICE DATA PREPARATION WITH IBM DATA REFINERY\\r\\nIf you are like most data scientists, you are probably spending a lot of time to\\r\\ncleanse, shape and prepare your data before you can actually start with the more\\r\\nenjoyable part of building and training machine learning models. As a data\\r\\nanalyst, you might face similar struggles to obtain data in a format you need to\\r\\nbuild your reports. In many companies data scientists and analysts need to wait\\r\\nfor their IT teams to get access to cleaned data in a consumable format.\\r\\n\\r\\nIBM Data Refinery addresses this issue. It provides an intuitive self-service\\r\\ndata preparation environment where you can quickly analyze, cleanse and prepare\\r\\ndata sets. It is a fully managed cloud service, available in open beta now.\\r\\n\\r\\nAnalyze and prepare your data\\r\\n\\r\\nWith IBM Data Refinery, you can interactively explore your data and use a wide\\r\\nrange of transformations to cleanse and transform data into the format you need\\r\\nfor analysis.\\r\\n\\r\\nYou can use a simple point-and-click interface for selecting and combining a\\r\\nwide range of built-in operations, such as filtering, replacing, and deriving\\r\\nvalues. It is also possible to quickly remove duplicates, split and concatenate\\r\\nvalues, and choose from a comprehensive list of text and math operations.\\r\\n\\r\\nInteractive data exploration and preparationIf you prefer to code, in IBM Data Refinery you can directly enter R commands\\r\\nvia R libraries such as dplyr. We provide code templates and in-context\\r\\ndocumentation to help you become productive with the R syntax more quickly.\\r\\n\\r\\nCode templates to help users with R syntaxIf you’re not satisfied with the shaping results, you can easily undo and change\\r\\noperations in the Steps side bar.\\r\\n\\r\\nThe interactive user interface works on a subset of the data to give you a\\r\\nfaster preview of the operations and results. Once you’re happy with the sample\\r\\noutput, you can apply the transformations on the entire data set and save all\\r\\ntransformation steps in a data flow. You can repeat the data flow later and\\r\\ntrack changes that were applied to your data. To accelerate the job execution,\\r\\nApache Spark is used as the execution engine.\\r\\n\\r\\nProfile and visualize data\\r\\n\\r\\nData shaping is an iterative and time-consuming process. In a traditional data\\r\\nscience workflow, you might use one tool to apply various transformations to\\r\\nyour data set, and then load the data into another tool to visualize and\\r\\nevaluate the results. Over many cycles, this continual tool hopping can become\\r\\nfrustrating.\\r\\n\\r\\nIBM Data Refinery soothes the pain by integrating both data transformations and\\r\\nvisualizations in a single interface, so you can move between views with a\\r\\nsimple click. You can use the Profile tab to view descriptive statistics of your\\r\\ndata columns in order to better understand the distribution of values. You can\\r\\ncontinue to apply transformations and the corresponding profile information\\r\\nadjusts automatically.\\r\\n\\r\\nOn the Visualization tab you can select a combination of columns to build charts\\r\\nusing Brunel (open source visualization library). IBM Data Refinery\\r\\nautomatically suggests appropriate plots and you can choose between 12\\r\\npre-defined chart types. You can adjust the appearance of the charts using\\r\\nBrunel syntax.\\r\\n\\r\\nConnect to your data wherever it resides\\r\\n\\r\\nIBM Data Refinery comes with a comprehensive set of 30 prebuilt data connectors\\r\\nso that you can set up connections to a wide range of commonly used on-premises\\r\\nand cloud data stores. You can connect to IBM as well as non-IBM services. If\\r\\nyour data service is hosted on IBM Cloud (formerly IBM Bluemix), you can\\r\\ndirectly access the data service instance from IBM Data Refinery.\\r\\n\\r\\nOnce you specify a connection and connect the data object to your data, you can\\r\\nstart to analyze and refine your data wherever it resides.\\r\\n\\r\\nTry out IBM Data Refinery! Sign up for free at: https://www.ibm.com/cloud/data-refinery\\r\\n\\r\\n * Data Science\\r\\n * Data Visualization\\r\\n * Data Analysis\\r\\n * Data Refinery\\r\\n\\r\\nOne clap, two clap, three clap, forty?By clapping more or less, you can signal to us which stories really stand out.\\r\\n\\r\\n27 Blocked Unblock Follow FollowingCARMEN RUPPACH\\r\\nOffering Manager for Data Refinery on Watson Data Platform at IBM\\r\\n\\r\\nFollowIBM WATSON DATA\\r\\nBuild smarter applications and quickly visualize, share, and gain insights\\r\\n\\r\\n * 27\\r\\n * \\r\\n * \\r\\n * \\r\\n\\r\\nNever miss a story from IBM Watson Data , when you sign up for Medium. Learn more Never miss a story from IBM Watson Data Get updates Get updates   \n",
       "\n",
       "                                                                                                                                                                                              doc_description  \\\n",
       "50                                                                                                                                                                               Community Detection at Scale   \n",
       "221                                                                              When used to make sense of huge amounts of constantly changing data, smart catalog capabilities can make all the difference.   \n",
       "232         If you are like most data scientists, you are probably spending a lot of time to cleanse, shape and prepare your data before you can actually start with the more enjoyable part of building and…   \n",
       "365     During the seven-week Insight Data Engineering Fellows Program recent grads and experienced software engineers learn the latest open source technologies by building a data platform to handle large…   \n",
       "399  Today’s world of data science leverages data from various sources. Commonly, these sources are Hadoop File System, Enterprise Data Warehouse, Relational Database systems, Enterprise file systems, etc…   \n",
       "578                                                                                                         This video shows you how to construct queries to access the primary index through Cloudant's API.   \n",
       "692        One of the earliest documented catalogs was compiled at the great library of Alexandria in the third century BC, to help scholars manage, understand and access its vast collection of literature…   \n",
       "761  Today’s world of data science leverages data from various sources. Commonly, these sources are Hadoop File System, Enterprise Data Warehouse, Relational Database systems, Enterprise file systems, etc…   \n",
       "970                                                                                                                 This video shows you how to construct queries to access the primary index through the API   \n",
       "971         If you are like most data scientists, you are probably spending a lot of time to cleanse, shape and prepare your data before you can actually start with the more enjoyable part of building and…   \n",
       "\n",
       "                                                                                    doc_full_name  \\\n",
       "50                                                                   Graph-based machine learning   \n",
       "221                   How smart catalogs can turn the big data flood into an ocean of opportunity   \n",
       "232                                          Self-service data preparation with IBM Data Refinery   \n",
       "365                                                                  Graph-based machine learning   \n",
       "399  Using Apache Spark as a parallel processing framework for accessing REST based data services   \n",
       "578                                                                         Use the Primary Index   \n",
       "692                   How smart catalogs can turn the big data flood into an ocean of opportunity   \n",
       "761  Using Apache Spark as a parallel processing framework for accessing REST based data services   \n",
       "970                                                                         Use the Primary Index   \n",
       "971                                          Self-service data preparation with IBM Data Refinery   \n",
       "\n",
       "    doc_status  article_id  \n",
       "50        Live          50  \n",
       "221       Live         221  \n",
       "232       Live         232  \n",
       "365       Live          50  \n",
       "399       Live         398  \n",
       "578       Live         577  \n",
       "692       Live         221  \n",
       "761       Live         398  \n",
       "970       Live         577  \n",
       "971       Live         232  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find and explore duplicate articles\n",
    "df_content.loc[df_content['article_id'].duplicated(keep = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50                                                                                                                                                                              Community Detection at Scale\n",
       "365    During the seven-week Insight Data Engineering Fellows Program recent grads and experienced software engineers learn the latest open source technologies by building a data platform to handle large…\n",
       "Name: doc_description, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df_content[df_content['article_id'] == 50]['doc_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows that have the same article_id - only keep the first\n",
    "df_content_dpDup = df_content.drop_duplicates(['article_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Use the cells below to find:\n",
    "\n",
    "**a.** The number of unique articles that have an interaction with a user.  \n",
    "**b.** The number of unique articles in the dataset (whether they have any interactions or not).<br>\n",
    "**c.** The number of unique users in the dataset. (excluding null values)<br> \n",
    "**d.** The number of user-article interactions in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique articles that have at least one interaction is 714\n",
      "The number of unique articles on the IBM platform is 1051\n",
      "The number of unique users is 5149\n",
      "The number of user-article  is 45993\n"
     ]
    }
   ],
   "source": [
    "unique_articles = int(len(df['article_id'].unique()))\n",
    "print(\"The number of unique articles that have at least one interaction is {}\".format(unique_articles))\n",
    "total_articles = int(len(df_content_dpDup['article_id']))\n",
    "print(\"The number of unique articles on the IBM platform is {}\".format(total_articles))\n",
    "unique_users = len(df['email'].unique())# \n",
    "print(\"The number of unique users is {}\".format(unique_users))\n",
    "user_article_interactions = int(df.shape[0])\n",
    "print(\"The number of user-article  is {}\".format(user_article_interactions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Use the cells below to find the most viewed **article_id**, as well as how often it was viewed.  After talking to the company leaders, the `email_mapper` function was deemed a reasonable way to map users to ids.  There were a small number of null values, and it was find using other information that all of these null values likely belonged to a single user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "927"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['article_id']).count().sort_values(by = ['title'], ascending = False).iloc[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Float64Index([1429.0, 1330.0, 1431.0, 1427.0, 1364.0, 1314.0, 1293.0, 1170.0,\n",
       "              1162.0, 1304.0,\n",
       "              ...\n",
       "               662.0, 1200.0, 1092.0,  653.0, 1344.0, 1113.0, 1119.0,  984.0,\n",
       "              1127.0, 1266.0],\n",
       "             dtype='float64', name='article_id', length=714)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['article_id']).count().sort_values(by = ['title'], ascending = False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most viewed article id is: 1429.0\n",
      "The most viewed article was viewed 937 times\n"
     ]
    }
   ],
   "source": [
    "most_viewed_article_id = str(df.groupby(['article_id']).count().sort_values(by = ['title'], ascending = False).index[0]) # The most viewed article in the dataset as a string with one value following the decimal \n",
    "print(\"The most viewed article id is: {}\".format(most_viewed_article_id))\n",
    "max_views =int(df.groupby(['article_id']).count().sort_values(by = ['title'], ascending = False).iloc[0,0]) # The most viewed article in the dataset was viewed how many times?\n",
    "print(\"The most viewed article was viewed {:.0f} times\".format(max_views))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>using pixiedust for fast, flexible, and easier data analysis and experimentation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1314.0</td>\n",
       "      <td>healthcare python streaming application demo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1429.0</td>\n",
       "      <td>use deep learning for image classification</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338.0</td>\n",
       "      <td>ml optimization using cognitive assistant</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1276.0</td>\n",
       "      <td>deploy your python model as a restful api</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  \\\n",
       "0      1430.0   \n",
       "1      1314.0   \n",
       "2      1429.0   \n",
       "3      1338.0   \n",
       "4      1276.0   \n",
       "\n",
       "                                                                              title  \\\n",
       "0  using pixiedust for fast, flexible, and easier data analysis and experimentation   \n",
       "1                                      healthcare python streaming application demo   \n",
       "2                                        use deep learning for image classification   \n",
       "3                                         ml optimization using cognitive assistant   \n",
       "4                                         deploy your python model as a restful api   \n",
       "\n",
       "   user_id  \n",
       "0        1  \n",
       "1        2  \n",
       "2        3  \n",
       "3        4  \n",
       "4        5  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## No need to change the code here - this will be helpful for later parts of the notebook\n",
    "# Run this cell to map the user email to a user_id column and remove the email column\n",
    "\n",
    "def email_mapper():\n",
    "    coded_dict = dict()\n",
    "    cter = 1\n",
    "    email_encoded = []\n",
    "    \n",
    "    for val in df['email']:\n",
    "        if val not in coded_dict:\n",
    "            coded_dict[val] = cter\n",
    "            cter+=1\n",
    "        \n",
    "        email_encoded.append(coded_dict[val])\n",
    "    return email_encoded\n",
    "\n",
    "email_encoded = email_mapper()\n",
    "del df['email']\n",
    "df['user_id'] = email_encoded\n",
    "\n",
    "# show header\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oops! It looks like the value associated with: `The number of unique users in the dataset is ______` wasn't right. Try again.  It might just be the datatype.  All of the values should be ints except the article_id should be a string.  Let each row be considered a separate user-article interaction.  If a user interacts with an article 3 times, these are considered 3 separate interactions.\n"
     ]
    }
   ],
   "source": [
    "## If you stored all your results in the variable names above, \n",
    "## you shouldn't need to change anything in this cell\n",
    "\n",
    "sol_1_dict = {\n",
    "    '`50% of individuals have _____ or fewer interactions.`': median_val,\n",
    "    '`The total number of user-article interactions in the dataset is ______.`': user_article_interactions,\n",
    "    '`The maximum number of user-article interactions by any 1 user is ______.`': max_views_by_user,\n",
    "    '`The most viewed article in the dataset was viewed _____ times.`': max_views,\n",
    "    '`The article_id of the most viewed article is ______.`': most_viewed_article_id,\n",
    "    '`The number of unique articles that have at least 1 rating ______.`': unique_articles,\n",
    "    '`The number of unique users in the dataset is ______`': unique_users,\n",
    "    '`The number of unique articles on the IBM platform`': total_articles\n",
    "}\n",
    "\n",
    "# Test your dictionary against the solution\n",
    "t.sol_1_test(sol_1_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def sol_1_test(sol_1_dict):\\n    sol_1_dict_ = {\\n    \\'`50% of individuals have _____ or fewer interactions.`\\': 3,\\n    \\'`The total number of user-article interactions in the dataset is ______.`\\': 45993,\\n    \\'`The maximum number of user-article interactions by any 1 user is ______.`\\': 364,\\n    \\'`The most viewed article in the dataset was viewed _____ times.`\\': 937,\\n    \\'`The article_id of the most viewed article is ______.`\\': \\'1429.0\\',\\n    \\'`The number of unique articles that have at least 1 rating ______.`\\': 714,\\n    \\'`The number of unique users in the dataset is ______`\\': 5148,\\n    \\'`The number of unique articles on the IBM platform`\\': 1051,\\n    }\\n\\n    if sol_1_dict_ == sol_1_dict:\\n        print(\"It looks like you have everything right here! Nice job!\")\\n\\n    else:\\n        for k, v in sol_1_dict.items():\\n            if sol_1_dict_[k] != sol_1_dict[k]:\\n                print(\"Oops! It looks like the value associated with: {} wasn\\'t right. Try again.  It might just be the datatype.  All of the values should be ints except the article_id should be a string.  Let each row be considered a separate user-article interaction.  If a user interacts with an article 3 times, these are considered 3 separate interactions.\".format(k))\\n'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsource(t.sol_1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Rank\">Part II: Rank-Based Recommendations</a>\n",
    "\n",
    "Unlike in the earlier lessons, we don't actually have ratings for whether a user liked an article or not.  We only know that a user has interacted with an article.  In these cases, the popularity of an article can really only be based on how often an article was interacted with.\n",
    "\n",
    "`1.` Fill in the function below to return the **n** top articles ordered with most interactions as the top. Test your function using the tests below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['use deep learning for image classification',\n",
       "       'insights from new york car accident reports',\n",
       "       'visualize car data with brunel',\n",
       "       'use xgboost, scikit-learn & ibm watson machine learning apis',\n",
       "       'predicting churn with the spss random tree algorithm',\n",
       "       'healthcare python streaming application demo',\n",
       "       'finding optimal locations of new store using decision optimization',\n",
       "       'apache spark lab, part 1: basic concepts',\n",
       "       'analyze energy consumption in buildings',\n",
       "       'gosales transactions for logistic regression model'],\n",
       "      dtype='object', name='title')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['title']).count().sort_values(['user_id'], ascending = False).iloc[:10].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_articles(n, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    n - (int) the number of top articles to return\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    \n",
    "    OUTPUT:\n",
    "    top_articles - (list) A list of the top 'n' article titles \n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    top_articles = df.groupby(['title']).count().sort_values(['user_id'], ascending = False).iloc[:n].index\n",
    "    return top_articles # Return the top article titles from df (not df_content)\n",
    "\n",
    "def get_top_article_ids(n, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    n - (int) the number of top articles to return\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    \n",
    "    OUTPUT:\n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    top_articles = df.groupby(['article_id']).count().sort_values(['user_id'], ascending = False).iloc[:n].index\n",
    "  \n",
    "    return top_articles # Return the top article ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['use deep learning for image classification',\n",
      "       'insights from new york car accident reports',\n",
      "       'visualize car data with brunel',\n",
      "       'use xgboost, scikit-learn & ibm watson machine learning apis',\n",
      "       'predicting churn with the spss random tree algorithm',\n",
      "       'healthcare python streaming application demo',\n",
      "       'finding optimal locations of new store using decision optimization',\n",
      "       'apache spark lab, part 1: basic concepts',\n",
      "       'analyze energy consumption in buildings',\n",
      "       'gosales transactions for logistic regression model'],\n",
      "      dtype='object', name='title')\n",
      "Float64Index([1429.0, 1330.0, 1431.0, 1427.0, 1364.0, 1314.0, 1293.0, 1170.0,\n",
      "              1162.0, 1304.0],\n",
      "             dtype='float64', name='article_id')\n"
     ]
    }
   ],
   "source": [
    "print(get_top_articles(10))\n",
    "print(get_top_article_ids(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your top_5 looks like the solution list! Nice job.\n",
      "Your top_10 looks like the solution list! Nice job.\n",
      "Your top_20 looks like the solution list! Nice job.\n"
     ]
    }
   ],
   "source": [
    "# Test your function by returning the top 5, 10, and 20 articles\n",
    "top_5 = get_top_articles(5)\n",
    "top_10 = get_top_articles(10)\n",
    "top_20 = get_top_articles(20)\n",
    "\n",
    "# Test each of your three lists from above\n",
    "t.sol_2_test(get_top_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"User-User\">Part III: User-User Based Collaborative Filtering</a>\n",
    "\n",
    "\n",
    "`1.` Use the function below to reformat the **df** dataframe to be shaped with users as the rows and articles as the columns.  \n",
    "\n",
    "* Each **user** should only appear in each **row** once.\n",
    "\n",
    "\n",
    "* Each **article** should only show up in one **column**.  \n",
    "\n",
    "\n",
    "* **If a user has interacted with an article, then place a 1 where the user-row meets for that article-column**.  It does not matter how many times a user has interacted with the article, all entries where a user has interacted with an article should be a 1.  \n",
    "\n",
    "\n",
    "* **If a user has not interacted with an item, then place a zero where the user-row meets for that article-column**. \n",
    "\n",
    "Use the tests to make sure the basic structure of your matrix matches what is expected by the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_raw = df.groupby(['article_id', 'user_id']).count().unstack()\n",
    "matrix_raw[matrix_raw>0] = 1\n",
    "matrix_raw[matrix_raw.isna()] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the user-article matrix with 1's and 0's\n",
    "\n",
    "def create_user_item_matrix(df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - pandas dataframe with article_id, title, user_id columns\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_item - user item matrix \n",
    "    \n",
    "    Description:\n",
    "    Return a matrix with user ids as rows and article ids on the columns with 1 values where a user interacted with \n",
    "    an article and a 0 otherwise\n",
    "    '''\n",
    "    # Fill in the function here\n",
    "    user_item = df.groupby(['user_id', 'article_id'])['article_id'].count().unstack()\n",
    "    user_item[user_item>0] = 1\n",
    "    user_item[user_item.isna()] = 0\n",
    "    return user_item # return the user_item matrix \n",
    "\n",
    "user_item = create_user_item_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5149, 714)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>article_id</th>\n",
       "      <th>0.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>8.0</th>\n",
       "      <th>9.0</th>\n",
       "      <th>12.0</th>\n",
       "      <th>14.0</th>\n",
       "      <th>15.0</th>\n",
       "      <th>16.0</th>\n",
       "      <th>18.0</th>\n",
       "      <th>...</th>\n",
       "      <th>1434.0</th>\n",
       "      <th>1435.0</th>\n",
       "      <th>1436.0</th>\n",
       "      <th>1437.0</th>\n",
       "      <th>1439.0</th>\n",
       "      <th>1440.0</th>\n",
       "      <th>1441.0</th>\n",
       "      <th>1442.0</th>\n",
       "      <th>1443.0</th>\n",
       "      <th>1444.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5145</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5146</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5147</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5148</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5149</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5149 rows × 714 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "article_id  0.0     2.0     4.0     8.0     9.0     12.0    14.0    15.0    \\\n",
       "user_id                                                                      \n",
       "1              0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2              0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3              0.0     0.0     0.0     0.0     0.0     1.0     0.0     0.0   \n",
       "4              0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "5              0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...            ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "5145           0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "5146           0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "5147           0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "5148           0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "5149           0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "article_id  16.0    18.0    ...  1434.0  1435.0  1436.0  1437.0  1439.0  \\\n",
       "user_id                     ...                                           \n",
       "1              0.0     0.0  ...     0.0     0.0     1.0     0.0     1.0   \n",
       "2              0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "3              0.0     0.0  ...     0.0     0.0     1.0     0.0     0.0   \n",
       "4              0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "5              0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "...            ...     ...  ...     ...     ...     ...     ...     ...   \n",
       "5145           0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "5146           0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "5147           0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "5148           0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "5149           1.0     0.0  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "article_id  1440.0  1441.0  1442.0  1443.0  1444.0  \n",
       "user_id                                             \n",
       "1              0.0     0.0     0.0     0.0     0.0  \n",
       "2              0.0     0.0     0.0     0.0     0.0  \n",
       "3              0.0     0.0     0.0     0.0     0.0  \n",
       "4              0.0     0.0     0.0     0.0     0.0  \n",
       "5              0.0     0.0     0.0     0.0     0.0  \n",
       "...            ...     ...     ...     ...     ...  \n",
       "5145           0.0     0.0     0.0     0.0     0.0  \n",
       "5146           0.0     0.0     0.0     0.0     0.0  \n",
       "5147           0.0     0.0     0.0     0.0     0.0  \n",
       "5148           0.0     0.0     0.0     0.0     0.0  \n",
       "5149           0.0     0.0     0.0     0.0     0.0  \n",
       "\n",
       "[5149 rows x 714 columns]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have passed our quick tests!  Please proceed!\n"
     ]
    }
   ],
   "source": [
    "## Tests: You should just need to run this cell.  Don't change the code.\n",
    "assert user_item.shape[0] == 5149, \"Oops!  The number of users in the user-article matrix doesn't look right.\"\n",
    "assert user_item.shape[1] == 714, \"Oops!  The number of articles in the user-article matrix doesn't look right.\"\n",
    "assert user_item.sum(axis=1)[1] == 36, \"Oops!  The number of articles seen by user 1 doesn't look right.\"\n",
    "print(\"You have passed our quick tests!  Please proceed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Complete the function below which should take a user_id and provide an ordered list of the most similar users to that user (from most similar to least similar).  The returned result should not contain the provided user_id, as we know that each user is similar to him/herself. Because the results for each user here are binary, it (perhaps) makes sense to compute similarity as the dot product of two users. \n",
    "\n",
    "Use the tests to test your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "3933    35.0\n",
       "23      17.0\n",
       "3782    17.0\n",
       "203     15.0\n",
       "4459    15.0\n",
       "        ... \n",
       "2326     0.0\n",
       "2327     0.0\n",
       "2328     0.0\n",
       "2329     0.0\n",
       "5149     0.0\n",
       "Name: 1, Length: 5148, dtype: float64"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_prod_articles = user_item.dot(np.transpose(user_item))\n",
    "dot_prod_articles[1:10]\n",
    "user_id = 1\n",
    "closest_users = dot_prod_articles[user_id].sort_values(ascending = False)[1:]\n",
    "closest_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_users(user_id, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user_id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "    OUTPUT:\n",
    "    similar_users - (list) an ordered list where the closest users (largest dot product users)\n",
    "                    are listed first\n",
    "    \n",
    "    Description:\n",
    "    Computes the similarity of every pair of users based on the dot product\n",
    "    Returns an ordered\n",
    "    \n",
    "    '''\n",
    "    # compute similarity of each user to the provided user\n",
    "    dot_prod_articles = user_item.dot(np.transpose(user_item))\n",
    "    # sort by similarity\n",
    "    closest_users = dot_prod_articles[user_id].sort_values(ascending = False)\n",
    "    # create list of just the ids\n",
    "    most_similar_users = closest_users.index\n",
    "    # remove the own user's id\n",
    "    most_similar_users= most_similar_users[1:]\n",
    "    return most_similar_users # return a list of the users in order from most to least similar\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most similar users to user 1 are: Int64Index([3933, 23, 3782, 203, 4459, 3870, 131, 4201, 46, 5041], dtype='int64', name='user_id')\n",
      "The 5 most similar users to user 3933 are: Int64Index([3933, 23, 3782, 203, 4459], dtype='int64', name='user_id')\n",
      "The 3 most similar users to user 46 are: Int64Index([4201, 3782, 23], dtype='int64', name='user_id')\n"
     ]
    }
   ],
   "source": [
    "# Do a spot check of your function\n",
    "print(\"The 10 most similar users to user 1 are: {}\".format(find_similar_users(1)[:10]))\n",
    "print(\"The 5 most similar users to user 3933 are: {}\".format(find_similar_users(3933)[:5]))\n",
    "print(\"The 3 most similar users to user 46 are: {}\".format(find_similar_users(46)[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2.0',\n",
       " '12.0',\n",
       " '14.0',\n",
       " '16.0',\n",
       " '26.0',\n",
       " '28.0',\n",
       " '29.0',\n",
       " '33.0',\n",
       " '43.0',\n",
       " '50.0',\n",
       " '74.0',\n",
       " '76.0',\n",
       " '108.0',\n",
       " '109.0',\n",
       " '120.0',\n",
       " '124.0',\n",
       " '131.0',\n",
       " '164.0',\n",
       " '193.0',\n",
       " '194.0',\n",
       " '210.0',\n",
       " '213.0',\n",
       " '221.0',\n",
       " '223.0',\n",
       " '225.0',\n",
       " '236.0',\n",
       " '237.0',\n",
       " '241.0',\n",
       " '252.0',\n",
       " '253.0',\n",
       " '283.0',\n",
       " '295.0',\n",
       " '299.0',\n",
       " '302.0',\n",
       " '316.0',\n",
       " '336.0',\n",
       " '337.0',\n",
       " '339.0',\n",
       " '348.0',\n",
       " '359.0',\n",
       " '362.0',\n",
       " '367.0',\n",
       " '409.0',\n",
       " '422.0',\n",
       " '444.0',\n",
       " '477.0',\n",
       " '482.0',\n",
       " '510.0',\n",
       " '517.0',\n",
       " '524.0',\n",
       " '617.0',\n",
       " '634.0',\n",
       " '641.0',\n",
       " '656.0',\n",
       " '658.0',\n",
       " '665.0',\n",
       " '682.0',\n",
       " '693.0',\n",
       " '720.0',\n",
       " '721.0',\n",
       " '729.0',\n",
       " '744.0',\n",
       " '761.0',\n",
       " '800.0',\n",
       " '812.0',\n",
       " '821.0',\n",
       " '825.0',\n",
       " '833.0',\n",
       " '843.0',\n",
       " '910.0',\n",
       " '939.0',\n",
       " '943.0',\n",
       " '952.0',\n",
       " '957.0',\n",
       " '967.0',\n",
       " '969.0',\n",
       " '973.0',\n",
       " '981.0',\n",
       " '996.0',\n",
       " '1000.0',\n",
       " '1014.0',\n",
       " '1025.0',\n",
       " '1051.0',\n",
       " '1052.0',\n",
       " '1101.0',\n",
       " '1148.0',\n",
       " '1159.0',\n",
       " '1160.0',\n",
       " '1162.0',\n",
       " '1163.0',\n",
       " '1164.0',\n",
       " '1165.0',\n",
       " '1166.0',\n",
       " '1170.0',\n",
       " '1171.0',\n",
       " '1172.0',\n",
       " '1176.0',\n",
       " '1181.0',\n",
       " '1185.0',\n",
       " '1276.0',\n",
       " '1277.0',\n",
       " '1291.0',\n",
       " '1293.0',\n",
       " '1298.0',\n",
       " '1299.0',\n",
       " '1304.0',\n",
       " '1305.0',\n",
       " '1314.0',\n",
       " '1330.0',\n",
       " '1332.0',\n",
       " '1336.0',\n",
       " '1338.0',\n",
       " '1343.0',\n",
       " '1351.0',\n",
       " '1354.0',\n",
       " '1357.0',\n",
       " '1360.0',\n",
       " '1364.0',\n",
       " '1366.0',\n",
       " '1367.0',\n",
       " '1368.0',\n",
       " '1386.0',\n",
       " '1391.0',\n",
       " '1393.0',\n",
       " '1395.0',\n",
       " '1396.0',\n",
       " '1423.0',\n",
       " '1427.0',\n",
       " '1428.0',\n",
       " '1429.0',\n",
       " '1430.0',\n",
       " '1431.0',\n",
       " '1432.0',\n",
       " '1436.0',\n",
       " '1439.0']"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_user_articles(find_similar_users(1)[2])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Now that you have a function that provides the most similar users to each user, you will want to use these users to find articles you can recommend.  Complete the functions below to return the articles you would recommend to each user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_names(article_ids, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    article_ids - (list) a list of article ids\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_names - (list) a list of article names associated with the list of article ids \n",
    "                    (this is identified by the title column)\n",
    "    '''\n",
    "    # Your code here\n",
    "    article_names = list(df[df['article_id'].astype(str).isin(article_ids)]['title'].unique())\n",
    "    return article_names # Return the article names associated with list of article ids\n",
    "\n",
    "\n",
    "def get_user_articles(user_id, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_ids - (list) a list of the article ids seen by the user\n",
    "    article_names - (list) a list of article names associated with the list of article ids \n",
    "    \n",
    "    Description:\n",
    "    Provides a list of the article_ids and article titles that have been seen by a user\n",
    "    '''\n",
    "    # Your code here\n",
    "    article_ids = list(user_item.loc[user_id][user_item.loc[user_id] > 0].index.values.astype(str))\n",
    "    article_names = get_article_names(article_ids)\n",
    "    return article_ids, article_names # return the ids and names\n",
    "\n",
    "\n",
    "def user_user_recs(user_id, m=10):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user\n",
    "    \n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "    \n",
    "    Notes:\n",
    "    Users who are the same closeness are chosen arbitrarily as the 'next' user\n",
    "    \n",
    "    For the user where the number of recommended articles starts below m \n",
    "    and ends exceeding m, the last items are chosen arbitrarily\n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    #articles having been read by user_id\n",
    "    articles_read = get_user_articles(user_id)[0]\n",
    "    \n",
    "    recs = set()\n",
    "    similar_users = find_similar_users(user_id)\n",
    "    for idx_user in range(len(similar_users)):\n",
    "        similar_user = similar_users[idx_user]\n",
    "        similar_user_articles = get_user_articles(similar_user)[0]\n",
    "        new_articles = np.setdiff1d(similar_user_articles, articles_read)\n",
    "        recs.update(new_articles)\n",
    "        if len(recs) >= m:\n",
    "            break\n",
    "    recs = list(recs)[:m]              \n",
    "    return recs # return your recommendations for this user_id    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['visualize data with the matplotlib library',\n",
       " 'maximize oil company profits',\n",
       " 'use decision optimization to schedule league games',\n",
       " '5 practical use cases of social network analytics: going beyond facebook and twitter',\n",
       " 'the 3 kinds of context: machine learning and the art of the frame',\n",
       " 'pixiedust gets its first community-driven feature in 1.0.4',\n",
       " 'spark 2.1 and job monitoring available in dsx',\n",
       " 'data tidying in data science experience',\n",
       " 'recent trends in recommender systems',\n",
       " 'generalization in deep learning']"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_article_names(list(recs)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['visualize data with the matplotlib library',\n",
       " 'maximize oil company profits',\n",
       " 'use decision optimization to schedule league games',\n",
       " '5 practical use cases of social network analytics: going beyond facebook and twitter',\n",
       " 'the 3 kinds of context: machine learning and the art of the frame',\n",
       " 'pixiedust gets its first community-driven feature in 1.0.4',\n",
       " 'spark 2.1 and job monitoring available in dsx',\n",
       " 'data tidying in data science experience',\n",
       " 'recent trends in recommender systems',\n",
       " 'generalization in deep learning']"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Results\n",
    "get_article_names(user_user_recs(1, 10)) # Return 10 recommendations for user 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If this is all you see, you passed all of our tests!  Nice job!\n"
     ]
    }
   ],
   "source": [
    "# Test your functions here - No need to change this code - just run this cell\n",
    "assert set(get_article_names(['1024.0', '1176.0', '1305.0', '1314.0', '1422.0', '1427.0'])) == set(['using deep learning to reconstruct high-resolution audio', 'build a python app on the streaming analytics service', 'gosales transactions for naive bayes model', 'healthcare python streaming application demo', 'use r dataframes & ibm watson natural language understanding', 'use xgboost, scikit-learn & ibm watson machine learning apis']), \"Oops! Your the get_article_names function doesn't work quite how we expect.\"\n",
    "assert set(get_article_names(['1320.0', '232.0', '844.0'])) == set(['housing (2015): united states demographic measures','self-service data preparation with ibm data refinery','use the cloudant-spark connector in python notebook']), \"Oops! Your the get_article_names function doesn't work quite how we expect.\"\n",
    "assert set(get_user_articles(20)[0]) == set(['1320.0', '232.0', '844.0'])\n",
    "assert set(get_user_articles(20)[1]) == set(['housing (2015): united states demographic measures', 'self-service data preparation with ibm data refinery','use the cloudant-spark connector in python notebook'])\n",
    "assert set(get_user_articles(2)[0]) == set(['1024.0', '1176.0', '1305.0', '1314.0', '1422.0', '1427.0'])\n",
    "assert set(get_user_articles(2)[1]) == set(['using deep learning to reconstruct high-resolution audio', 'build a python app on the streaming analytics service', 'gosales transactions for naive bayes model', 'healthcare python streaming application demo', 'use r dataframes & ibm watson natural language understanding', 'use xgboost, scikit-learn & ibm watson machine learning apis'])\n",
    "print(\"If this is all you see, you passed all of our tests!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Now we are going to improve the consistency of the **user_user_recs** function from above.  \n",
    "\n",
    "* Instead of arbitrarily choosing when we obtain users who are all the same closeness to a given user - choose the users that have the most total article interactions before choosing those with fewer article interactions.\n",
    "\n",
    "\n",
    "* Instead of arbitrarily choosing articles from the user where the number of recommended articles starts below m and ends exceeding m, choose articles with the articles with the most total interactions before choosing those with fewer total interactions. This ranking should be  what would be obtained from the **top_articles** function you wrote earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>user_id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>5140</th>\n",
       "      <th>5141</th>\n",
       "      <th>5142</th>\n",
       "      <th>5143</th>\n",
       "      <th>5144</th>\n",
       "      <th>5145</th>\n",
       "      <th>5146</th>\n",
       "      <th>5147</th>\n",
       "      <th>5148</th>\n",
       "      <th>5149</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5145</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5146</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5147</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5148</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5149</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5149 rows × 5149 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "user_id  1     2     3     4     5     6     7     8     9     10    ...  \\\n",
       "user_id                                                              ...   \n",
       "1        36.0   2.0   6.0   3.0   0.0   4.0   1.0   6.0   4.0   7.0  ...   \n",
       "2         2.0   6.0   1.0   3.0   0.0   2.0   0.0   1.0   2.0   4.0  ...   \n",
       "3         6.0   1.0  40.0   5.0   1.0   7.0   1.0   5.0   2.0   5.0  ...   \n",
       "4         3.0   3.0   5.0  26.0   3.0   8.0   0.0   8.0   1.0   4.0  ...   \n",
       "5         0.0   0.0   1.0   3.0   3.0   1.0   0.0   3.0   0.0   0.0  ...   \n",
       "...       ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "5145      1.0   1.0   2.0   2.0   0.0   0.0   0.0   0.0   0.0   1.0  ...   \n",
       "5146      0.0   0.0   0.0   1.0   0.0   1.0   0.0   1.0   0.0   0.0  ...   \n",
       "5147      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "5148      0.0   0.0   0.0   1.0   0.0   0.0   0.0   1.0   0.0   0.0  ...   \n",
       "5149      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0  ...   \n",
       "\n",
       "user_id  5140  5141  5142  5143  5144  5145  5146  5147  5148  5149  \n",
       "user_id                                                              \n",
       "1         7.0   0.0   0.0   4.0   0.0   1.0   0.0   0.0   0.0   0.0  \n",
       "2         2.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0  \n",
       "3         7.0   0.0   0.0   5.0   0.0   2.0   0.0   0.0   0.0   0.0  \n",
       "4         6.0   0.0   0.0   2.0   0.0   2.0   1.0   0.0   1.0   0.0  \n",
       "5         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...       ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "5145      1.0   0.0   0.0   0.0   0.0   5.0   0.0   0.0   0.0   0.0  \n",
       "5146      0.0   0.0   0.0   1.0   0.0   0.0   7.0   0.0   0.0   0.0  \n",
       "5147      0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0  \n",
       "5148      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0  \n",
       "5149      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0  \n",
       "\n",
       "[5149 rows x 5149 columns]"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item.dot(np.transpose(user_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35., 17., 17., ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = (user_item.dot(np.transpose(user_item))[1].sort_values(ascending = False)[1:])\n",
    "similarities.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "23      135.0\n",
       "3782    135.0\n",
       "49      101.0\n",
       "3697    100.0\n",
       "3764     97.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sorted_users(user_id, df=df, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int)\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "            1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "            \n",
    "    OUTPUT:\n",
    "    neighbors_df - (pandas dataframe) a dataframe with:\n",
    "                    neighbor_id - is a neighbor user_id\n",
    "                    similarity - measure of the similarity of each user to the provided user_id\n",
    "                    num_interactions - the number of articles viewed by the user - if a u\n",
    "                    \n",
    "    Other Details - sort the neighbors_df by the similarity and then by number of interactions where \n",
    "                    highest of each is higher in the dataframe\n",
    "     \n",
    "    '''\n",
    "    # Your code here\n",
    "    similarities = user_item.dot(np.transpose(user_item))[user_id].sort_values(ascending = False)[1:]\n",
    "    user_interactions = user_item.sum(axis = 1).sort_values(ascending = False)\n",
    "    similarities_frame = pd.DataFrame({'neighbor_id': similarities.index, 'similarity': similarities.values})\n",
    "    interactions_frame = pd.DataFrame({'neighbor_id': user_interactions.index, 'num_interactions': user_interactions.values})\n",
    "    \n",
    "    neighbors_df = pd.merge(similarities_frame, interactions_frame, on = \"neighbor_id\")\n",
    "    neighbors_df.sort_values(['similarity', 'num_interactions'], ascending = False)\n",
    "    neighbors_df['neighbor_id'] = neighbors_df['neighbor_id']\n",
    "    return neighbors_df # Return the dataframe specified in the doc_string\n",
    "\n",
    "\n",
    "def user_user_recs_part2(user_id, m=10, user_item = user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user by article id\n",
    "    rec_names - (list) a list of recommendations for the user by article title\n",
    "    \n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "    \n",
    "    Notes:\n",
    "    * Choose the users that have the most total article interactions \n",
    "    before choosing those with fewer article interactions.\n",
    "\n",
    "    * Choose articles with the articles with the most total interactions \n",
    "    before choosing those with fewer total interactions. \n",
    "   \n",
    "    '''\n",
    "    # Your code here\n",
    "    #data preparation\n",
    "    top_sorted_users = get_top_sorted_users(user_id)\n",
    "    user_articles_read = get_user_articles(user_id)[0]\n",
    "    article_interactions = user_item.sum(axis = 0).sort_values(ascending = False)\n",
    "    \n",
    "    recs = set()\n",
    "    for i in range(top_sorted_users.shape[0]):\n",
    "        similar_user = top_sorted_users.iloc[i]['neighbor_id']\n",
    "        similar_user_articles = get_user_articles(similar_user)[0]\n",
    "        temp_article_interactions = article_interactions[article_interactions.index.astype(str).isin(similar_user_articles)]\n",
    "        new_articles_list = np.setdiff1d(list(temp_article_interactions.index.astype(str)), user_articles_read)\n",
    "        for article in new_articles_list:\n",
    "            recs.update([article])\n",
    "            if len(recs) >= m:\n",
    "                break\n",
    "        if len(recs) >= m:\n",
    "            break\n",
    "            \n",
    "    rec_names = get_article_names(recs)\n",
    "    return recs, rec_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 recommendations for user 20 are the following article ids:\n",
      "{'1059.0', '1052.0', '1162.0', '1164.0', '1175.0', '1169.0', '1173.0', '1163.0', '1161.0', '1172.0'}\n",
      "\n",
      "The top 10 recommendations for user 20 are the following article names:\n",
      "['access db2 warehouse on cloud and db2 with python', 'analyze energy consumption in buildings', 'analyze open data sets with pandas dataframes', 'apache spark lab, part 3: machine learning', 'analyze open data sets with spark & pixiedust', 'airbnb data for analytics: amsterdam calendar', 'annual precipitation by country 1990-2009', 'births attended by skilled health staff (% of total) by country', 'breast cancer detection with xgboost, wml and scikit', 'analyze data, build a dashboard with spark and pixiedust']\n"
     ]
    }
   ],
   "source": [
    "# Quick spot check - don't change this code - just use it to test your functions\n",
    "rec_ids, rec_names = user_user_recs_part2(20, 10)\n",
    "print(\"The top 10 recommendations for user 20 are the following article ids:\")\n",
    "print(rec_ids)\n",
    "print()\n",
    "print(\"The top 10 recommendations for user 20 are the following article names:\")\n",
    "print(rec_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Use your functions from above to correctly fill in the solutions to the dictionary below.  Then test your dictionary against the solution.  Provide the code you need to answer each following the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neighbor_id</th>\n",
       "      <th>similarity</th>\n",
       "      <th>num_interactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3933</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>17.0</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3782</td>\n",
       "      <td>17.0</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>203</td>\n",
       "      <td>15.0</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4459</td>\n",
       "      <td>15.0</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5143</th>\n",
       "      <td>2326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5144</th>\n",
       "      <td>2327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5145</th>\n",
       "      <td>2328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5146</th>\n",
       "      <td>2329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5147</th>\n",
       "      <td>5149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5148 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      neighbor_id  similarity  num_interactions\n",
       "0            3933        35.0              35.0\n",
       "1              23        17.0             135.0\n",
       "2            3782        17.0             135.0\n",
       "3             203        15.0              96.0\n",
       "4            4459        15.0              96.0\n",
       "...           ...         ...               ...\n",
       "5143         2326         0.0               1.0\n",
       "5144         2327         0.0               1.0\n",
       "5145         2328         0.0               4.0\n",
       "5146         2329         0.0               1.0\n",
       "5147         5149         0.0               1.0\n",
       "\n",
       "[5148 rows x 3 columns]"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_sorted_users(1).iloc[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3933\n",
      "242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Tests with a dictionary of results\n",
    "\n",
    "user1_most_sim = int(get_top_sorted_users(1).iloc[0]['neighbor_id']) # Find the user that is most similar to user 1 \n",
    "print(user1_most_sim)\n",
    "user131_10th_sim = int(get_top_sorted_users(131).iloc[10]['neighbor_id']) # Find the 10th most similar user to user 131\n",
    "print(user131_10th_sim)\n",
    "type(3933)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This all looks good!  Nice job!\n"
     ]
    }
   ],
   "source": [
    "## Dictionary Test Here\n",
    "sol_5_dict = {\n",
    "    'The user that is most similar to user 1.': user1_most_sim, \n",
    "    'The user that is the 10th most similar to user 131': user131_10th_sim,\n",
    "}\n",
    "\n",
    "t.sol_5_test(sol_5_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`6.` If we were given a new user, which of the above functions would you be able to use to make recommendations?  Explain.  Can you think of a better way we might make recommendations?  Use the cell below to explain a better method for new users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Provide your response here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`7.` Using your existing functions, provide the top 10 recommended articles you would provide for the a new user below.  You can test your function against our thoughts to make sure we are all on the same page with how we might make a recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_user = '0.0'\n",
    "\n",
    "# What would your recommendations be for this new user '0.0'?  As a new user, they have no observed articles.\n",
    "# Provide a list of the top 10 article ids you would give to \n",
    "new_user_recs = # Your recommendations here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert set(new_user_recs) == set(['1314.0','1429.0','1293.0','1427.0','1162.0','1364.0','1304.0','1170.0','1431.0','1330.0']), \"Oops!  It makes sense that in this case we would want to recommend the most popular articles, because we don't know anything about these users.\"\n",
    "\n",
    "print(\"That's right!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Content-Recs\">Part IV: Content Based Recommendations (EXTRA - NOT REQUIRED)</a>\n",
    "\n",
    "Another method we might use to make recommendations is to perform a ranking of the highest ranked articles associated with some term.  You might consider content to be the **doc_body**, **doc_description**, or **doc_full_name**.  There isn't one way to create a content based recommendation, especially considering that each of these columns hold content related information.  \n",
    "\n",
    "`1.` Use the function body below to create a content based recommender.  Since there isn't one right answer for this recommendation tactic, no test functions are provided.  Feel free to change the function inputs if you decide you want to try a method that requires more input values.  The input values are currently set with one idea in mind that you may use to make content based recommendations.  One additional idea is that you might want to choose the most popular recommendations that meet your 'content criteria', but again, there is a lot of flexibility in how you might make these recommendations.\n",
    "\n",
    "### This part is NOT REQUIRED to pass this project.  However, you may choose to take this on as an extra way to show off your skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_content_recs():\n",
    "    '''\n",
    "    INPUT:\n",
    "    \n",
    "    OUTPUT:\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Now that you have put together your content-based recommendation system, use the cell below to write a summary explaining how your content based recommender works.  Do you see any possible improvements that could be made to your function?  Is there anything novel about your content based recommender?\n",
    "\n",
    "### This part is NOT REQUIRED to pass this project.  However, you may choose to take this on as an extra way to show off your skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write an explanation of your content based recommendation system here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Use your content-recommendation system to make recommendations for the below scenarios based on the comments.  Again no tests are provided here, because there isn't one right answer that could be used to find these content based recommendations.\n",
    "\n",
    "### This part is NOT REQUIRED to pass this project.  However, you may choose to take this on as an extra way to show off your skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make recommendations for a brand new user\n",
    "\n",
    "\n",
    "# make recommendations for a user who only has interacted with article id '1427.0'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Matrix-Fact\">Part V: Matrix Factorization</a>\n",
    "\n",
    "In this part of the notebook, you will build use matrix factorization to make article recommendations to the users on the IBM Watson Studio platform.\n",
    "\n",
    "`1.` You should have already created a **user_item** matrix above in **question 1** of **Part III** above.  This first question here will just require that you run the cells to get things set up for the rest of **Part V** of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the matrix here\n",
    "user_item_matrix = pd.read_pickle('user_item_matrix.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# quick look at the matrix\n",
    "user_item_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` In this situation, you can use Singular Value Decomposition from [numpy](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.svd.html) on the user-item matrix.  Use the cell to perform SVD, and explain why this is different than in the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform SVD on the User-Item Matrix Here\n",
    "\n",
    "u, s, vt = # use the built in to get the three matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Provide your response here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Now for the tricky part, how do we choose the number of latent features to use?  Running the below cell, you can see that as the number of latent features increases, we obtain a lower error rate on making predictions for the 1 and 0 values in the user-item matrix.  Run the cell below to get an idea of how the accuracy improves as we increase the number of latent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_latent_feats = np.arange(10,700+10,20)\n",
    "sum_errs = []\n",
    "\n",
    "for k in num_latent_feats:\n",
    "    # restructure with k latent features\n",
    "    s_new, u_new, vt_new = np.diag(s[:k]), u[:, :k], vt[:k, :]\n",
    "    \n",
    "    # take dot product\n",
    "    user_item_est = np.around(np.dot(np.dot(u_new, s_new), vt_new))\n",
    "    \n",
    "    # compute error for each prediction to actual value\n",
    "    diffs = np.subtract(user_item_matrix, user_item_est)\n",
    "    \n",
    "    # total errors and keep track of them\n",
    "    err = np.sum(np.sum(np.abs(diffs)))\n",
    "    sum_errs.append(err)\n",
    "    \n",
    "    \n",
    "plt.plot(num_latent_feats, 1 - np.array(sum_errs)/df.shape[0]);\n",
    "plt.xlabel('Number of Latent Features');\n",
    "plt.ylabel('Accuracy');\n",
    "plt.title('Accuracy vs. Number of Latent Features');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` From the above, we can't really be sure how many features to use, because simply having a better way to predict the 1's and 0's of the matrix doesn't exactly give us an indication of if we are able to make good recommendations.  Instead, we might split our dataset into a training and test set of data, as shown in the cell below.  \n",
    "\n",
    "Use the code from question 3 to understand the impact on accuracy of the training and test sets of data with different numbers of latent features. Using the split below: \n",
    "\n",
    "* How many users can we make predictions for in the test set?  \n",
    "* How many users are we not able to make predictions for because of the cold start problem?\n",
    "* How many articles can we make predictions for in the test set?  \n",
    "* How many articles are we not able to make predictions for because of the cold start problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df.head(40000)\n",
    "df_test = df.tail(5993)\n",
    "\n",
    "def create_test_and_train_user_item(df_train, df_test):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df_train - training dataframe\n",
    "    df_test - test dataframe\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_item_train - a user-item matrix of the training dataframe \n",
    "                      (unique users for each row and unique articles for each column)\n",
    "    user_item_test - a user-item matrix of the testing dataframe \n",
    "                    (unique users for each row and unique articles for each column)\n",
    "    test_idx - all of the test user ids\n",
    "    test_arts - all of the test article ids\n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    return user_item_train, user_item_test, test_idx, test_arts\n",
    "\n",
    "user_item_train, user_item_test, test_idx, test_arts = create_test_and_train_user_item(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace the values in the dictionary below\n",
    "a = 662 \n",
    "b = 574 \n",
    "c = 20 \n",
    "d = 0 \n",
    "\n",
    "\n",
    "sol_4_dict = {\n",
    "    'How many users can we make predictions for in the test set?': # letter here, \n",
    "    'How many users in the test set are we not able to make predictions for because of the cold start problem?': # letter here, \n",
    "    'How many articles can we make predictions for in the test set?': # letter here,\n",
    "    'How many articles in the test set are we not able to make predictions for because of the cold start problem?': # letter here\n",
    "}\n",
    "\n",
    "t.sol_4_test(sol_4_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Now use the **user_item_train** dataset from above to find **U**, **S**, and **V** transpose using SVD. Then find the subset of rows in the **user_item_test** dataset that you can predict using this matrix decomposition with different numbers of latent features to see how many features makes sense to keep based on the accuracy on the test data. This will require combining what was done in questions `2` - `4`.\n",
    "\n",
    "Use the cells below to explore how well SVD works towards making predictions for recommendations on the test data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit SVD on the user_item_train matrix\n",
    "u_train, s_train, vt_train = # fit svd similar to above then use the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use these cells to see how well you can use the training \n",
    "# decomposition to predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`6.` Use the cell below to comment on the results you found in the previous question. Given the circumstances of your results, discuss what you might do to determine if the recommendations you make with any of the above recommendation systems are an improvement to how users currently find articles? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your response here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='conclusions'></a>\n",
    "### Extras\n",
    "Using your workbook, you could now save your recommendations for each user, develop a class to make new predictions and update your results, and make a flask app to deploy your results.  These tasks are beyond what is required for this project.  However, from what you learned in the lessons, you certainly capable of taking these tasks on to improve upon your work here!\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "> Congratulations!  You have reached the end of the Recommendations with IBM project! \n",
    "\n",
    "> **Tip**: Once you are satisfied with your work here, check over your report to make sure that it satisfies all the areas of the rubric (found on the project submission page at the end of the lesson). You should also probably remove all of the \"Tips\" like this one so that the presentation is as polished as possible.\n",
    "\n",
    "\n",
    "## Directions to Submit\n",
    "\n",
    "> Before you submit your project, you need to create a .html or .pdf version of this notebook in the workspace here. To do that, run the code cell below. If it worked correctly, you should get a return code of 0, and you should see the generated .html file in the workspace directory (click on the orange Jupyter icon in the upper left).\n",
    "\n",
    "> Alternatively, you can download this report as .html via the **File** > **Download as** submenu, and then manually upload it into the workspace directory by clicking on the orange Jupyter icon in the upper left, then using the Upload button.\n",
    "\n",
    "> Once you've done this, you can submit your project by clicking on the \"Submit Project\" button in the lower right here. This will create and submit a zip file with this .ipynb doc and the .html or .pdf version you created. Congratulations! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'Recommendations_with_IBM.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
